{"jc2cc8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"created_utc":["2020-10-16","06:26:50"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/","id":"jc2cc8","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/","selftext":"\/u\/Stuck_In_the_Matrix recently tweeted that the beta api is now available. The big feature is the multithreaded ingest that will allow it to keep to near real time rather than falling hours behind when reddit gets lots of comments.\n\nThere's also lots of backend technical improvements and other planned features. There are docs available for this [here](https:\/\/beta.pushshift.io\/redoc). An example request would be\n\nhttps:\/\/beta.pushshift.io\/search\/reddit\/comments?q=remindme&amp;size=10\n\nA couple of the filters have changed, `limit` to `size` and `ids` to `id` for example, so be sure to check the docs.\n\nThere isn't much data yet, only a few days, and it's a beta so things could change at any time, but it's an exciting step forward.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Pushshift beta ingest now available","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/","comments":{"g8z24n2":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"This is great! Do you plan to write in depth covering the tech stack, major changes in this version and the technical limitations driving them. \n\nLearning from this massive scale of a project would be immensely helpful to the developer community. Would really like to see a blog\/article covering Pushshift ingestion, processing &amp; serving layers.","created_utc":["2020-10-16","07:41:10"],"id":"g8z24n2","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g8z24n2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g91k0ik":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Seems to be an hour behind already.  :\/\n\nE: oh, maybe not.  The main pushshift.io page isn't using this yet, apparently.","created_utc":["2020-10-17","00:13:42"],"id":"g91k0ik","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g91k0ik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g92n8hh":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"I'm guessing this is not yet finalized since there are two endpoints for reddit comments,\n\n* Search Reddit Comments - (Elastic DB)\n* Search Reddit Db Comments - ?\n\nWill these be consolidated into one client-facing API? Or, are there advantages to querying one over the other?","created_utc":["2020-10-17","07:09:10"],"id":"g92n8hh","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g92n8hh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g92p90k":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Setting size too large no longer gracefully responds,\n\n* old way: https:\/\/api.pushshift.io\/reddit\/comment\/search\/?size=999\n * returns 100 items\n* new way: https:\/\/beta.pushshift.io\/search\/reddit\/comments?size=251\n * `\"msg\": \"ensure this value is less than or equal to 250\"`\n\nSo for the beta, if you write code to request 250 items and then Pushshift later lowers this to 200, your script will break.","created_utc":["2020-10-17","07:34:19"],"id":"g92p90k","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g92p90k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gid847y":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Beta search for comments now just returns an internal server error.","created_utc":["2021-01-07","02:26:18"],"id":"gid847y","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/gid847y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jc04zn":{"author":"thatyouare_iamthat","author_fullname":"t2_pkphj","author_premium":false,"created_utc":["2020-10-16","03:58:24"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jc04zn\/ctrlclick_on_links_in_redditsearchio_srp_changes\/","id":"jc04zn","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jc04zn\/ctrlclick_on_links_in_redditsearchio_srp_changes\/","selftext":"Ctrl+click on links in redditsearch.io Search Result page changes focus from the readditsearch.io tab to the newly opened tab.   \nThis is on firefox browser, and this behaviour is not seen on other sites.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Ctrl+click on links in redditsearch.io SRP changes focus from the readditsearch.io tab to the newly opened tab","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jc04zn\/ctrlclick_on_links_in_redditsearchio_srp_changes\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jb8ozw":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"created_utc":["2020-10-14","23:21:04"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/","id":"jb8ozw","num_comments":13,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/","selftext":"I'm working on a homebrewed web-client explorer of the pushshift data. I'm looking at hard disks, and I've unzipped a few files on my PC and am nervous that a 10TB HDD isn't going to cut it.  \n\nDoes anybody know where I can find the near exact amount of data?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How much data is there in all of the unzipped submissions and comments?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/","comments":{"g8u4vmx":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It's probably 3-4 terrabytes at most.\n\nBut extracting is cheap, all my scripts that use it just stream the compressed file and extract as I go. Even if you're exporting to some form of database, there's probably a way to compress it there.","created_utc":["2020-10-15","00:25:07"],"id":"g8u4vmx","link_id":"t3_jb8ozw","parent_id":"t3_jb8ozw","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8u4vmx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8u8o2c":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Is there any way to extract from those compressed files in chunks?","created_utc":["2020-10-15","00:59:02"],"id":"g8u8o2c","link_id":"t3_jb8ozw","parent_id":"t1_g8u4vmx","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8u8o2c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ua5ty":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"In theory yes, you could skip forward in the archive and directly pull out a certain day\/hour. But it might take a bit to find, since there's no way to know where exactly a certain timespan is other than to jump around till you find it. It is chronological, but the different days could have different volumes of comments.\n\nPlus that's not something I've ever done before.","created_utc":["2020-10-15","01:12:39"],"id":"g8ua5ty","link_id":"t3_jb8ozw","parent_id":"t1_g8u8o2c","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ua5ty\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ulmvd":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"That's an interesting idea.  I hadn't thought of that.  I was more wondering what the best way was to iterate through those huge files in chunks of say 100,000 posts at a time.  I'm just not familiar with how to pull items out of a compressed file in chunks via python.","created_utc":["2020-10-15","02:59:46"],"id":"g8ulmvd","link_id":"t3_jb8ozw","parent_id":"t1_g8ua5ty","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ulmvd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uqb9y":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"\nCan you direct me to how to do it? I\u2019m using a variation of this python file I showed here https:\/\/reddit.com\/r\/pushshift\/comments\/j9qx5f\/filter_monthly_comments_data_into_individual\/\n\n, but still unable to process the file.\n\nI have a laptop with 16GB RAM, which I believe is choking at the decompression step itself. Is there a way to stream the decompressed file to my python program?","created_utc":["2020-10-15","03:44:44"],"id":"g8uqb9y","link_id":"t3_jb8ozw","parent_id":"t1_g8u4vmx","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uqb9y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uqqxe":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-10-15","03:48:55"],"id":"g8uqqxe","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uqqxe\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ur10q":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Try this,\n\nhttps:\/\/github.com\/reveddit\/ragger\/blob\/242cd3461c7aadcfdb6f6b5a60a24a44178f4051\/pushshift_file_reader_writer.py#L21\n\nIt can handle `.xz`, `.bz`, `.gz` and `.zst`\n\ncc \/u\/swapripper \/u\/MakeYourMarks","created_utc":["2020-10-15","03:51:33"],"id":"g8ur10q","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ur10q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ur6c0":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I'll give this a run tonight, thanks!","created_utc":["2020-10-15","03:52:58"],"id":"g8ur6c0","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ur6c0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uux2a":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-10-15","04:29:22"],"id":"g8uux2a","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uux2a\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uvjct":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The pushshift author has an example [here](https:\/\/github.com\/pushshift\/zreader). It doesn't look like that example you posted does any decompression at all, it's expecting already decompressed files.","created_utc":["2020-10-15","04:35:29"],"id":"g8uvjct","link_id":"t3_jb8ozw","parent_id":"t1_g8uqb9y","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uvjct\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8v6s8o":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"Thanks!!","created_utc":["2020-10-15","06:31:54"],"id":"g8v6s8o","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8v6s8o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8v6vz9":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"Thank you. I\u2019ll give it a try.","created_utc":["2020-10-15","06:33:06"],"id":"g8v6vz9","link_id":"t3_jb8ozw","parent_id":"t1_g8uvjct","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8v6vz9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8xlc79":{"author":"immibis","author_fullname":"t2_dj2ua","author_premium":false,"banned_at_utc":null,"body":"Decompress each one and recompress them in chunks of 100000 posts","created_utc":["2020-10-15","23:09:04"],"id":"g8xlc79","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8xlc79\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"j7b48x":{"author":"dozzinale","author_fullname":"t2_nkco2","author_premium":false,"created_utc":["2020-10-08","14:09:08"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j7b48x\/are_the_monthly_archive_for_2020_comments\/","id":"j7b48x","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/j7b48x\/are_the_monthly_archive_for_2020_comments\/","selftext":"Hello there, I'm working on a subset of pushshift data from march and april 2020 (the .zst files), and in particular I'm using submission data. For each submission in my dataset, I'd like retrieving all of the related comments.\n\nHowever, I can't find the datasets for the comments (they are not available in the [related page](https:\/\/files.pushshift.io\/reddit\/comments\/)). Are they available somewhere? I'd like to avoid using the API because I have 2M+ submissions in my dataset.\n\nThank you!","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Are the monthly archive for 2020 comments available?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j7b48x\/are_the_monthly_archive_for_2020_comments\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"j4wfve":{"author":"Visible_Horror5977","author_fullname":"t2_7j1pn2gl","author_premium":false,"created_utc":["2020-10-04","12:52:02"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j4wfve\/facing_critical_issue_while_using_pushshift_maybe\/","id":"j4wfve","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/j4wfve\/facing_critical_issue_while_using_pushshift_maybe\/","selftext":"Today I was exploring pushshift but I am facing a tough issue\n\n[https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=6uey5x](https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=6uey5x)\n\nThis is for figuring out a submission\n\n[https:\/\/api.pushshift.io\/reddit\/submission\/comment\\_ids\/6uey5x](https:\/\/api.pushshift.io\/reddit\/submission\/comment_ids\/6uey5x)\n\nThis is for figuring out all comment id related to the above submission\n\nThis is fine.But for my example it is simply not working(no idea why?)\n\n[https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=4lmm61](https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=4lmm61)\n\n(This is working)\n\n[https:\/\/api.pushshift.io\/reddit\/submission\/comment\\_ids\/4lmm61](https:\/\/api.pushshift.io\/reddit\/submission\/comment_ids\/4lmm61)\n\nBut alas this is not working(It is not showing anything but it has 14000+ comments). Maybe pushshift fails for large number of comment?\n\n&amp;#x200B;\n\nIf I am making a stupid mistake then I apologize in advance","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Facing critical issue while using pushshift (maybe it may be a bug)?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j4wfve\/facing_critical_issue_while_using_pushshift_maybe\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"j4o5p9":{"author":"Serious_Ad_2644","author_fullname":"t2_43cqf6we","author_premium":false,"created_utc":["2020-10-04","01:32:43"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j4o5p9\/sse_stream_sending_only_keepalive_events\/","id":"j4o5p9","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/j4o5p9\/sse_stream_sending_only_keepalive_events\/","selftext":"Hello,\n\nI have an assignment which includes reading the Pushshift SSE stream. Have had [http:\/\/stream.pushshift.io\/](http:\/\/stream.pushshift.io\/) open in browser for a long time, yet haven't received any other event than 'keepalive'. Is there anything else I need to set up, or is the stream having issues lately?\n\nBecause of that, I could not find the json structure for comment and submission event \"data\" fields. Is the structure same as in [api.pushshift.io](https:\/\/api.pushshift.io) ?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"SSE Stream sending only keepalive events","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j4o5p9\/sse_stream_sending_only_keepalive_events\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"j1x4uw":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"created_utc":["2020-09-29","14:06:38"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/","id":"j1x4uw","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/","selftext":"So, I worked with some of the large monthly Pushshift files a long time ago, and am returning to them for a new project.  I've downloaded the file RC\\_2018-09.xz.  I've unzipped it and tried to load it into Pandas using chunksize to load manageable chunks (the whole file obviously won't load into memory).\n\nNormally I would do this:  \n\n\n    file_path = 'E:\/Data\/reddit_aug-sep2018\/unzipped\/RC_2018-09'\n        \n    reader = pd.read_json(file_path, lines=True, chunksize=100)\n    \n    df = next(reader)\n    print(df.head().T)\n\nReader should be a generator that returns the next 100 posts when called.  But instead, on the line where reader is defined, pandas starts to read the file and doesn't stop until it crashes with a memory error.    Am I insane or just missing something?  I'm sure in the past the archive files unzipped into line delimited json files, but... maybe not?\n\nIf this isn't the right approach, how can I read these files in chunks?\n\nAny help appreciated.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Odd Problem with Pushshift Archive Files","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/","comments":{"g72xqgp":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Increase your chunk size. That\u2019s too few chunks, which itself can generate a memory error.","created_utc":["2020-09-29","20:07:10"],"id":"g72xqgp","link_id":"t3_j1x4uw","parent_id":"t3_j1x4uw","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g72xqgp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g72yqhx":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Thanks, I would have never thought of that!","created_utc":["2020-09-29","20:14:26"],"id":"g72yqhx","link_id":"t3_j1x4uw","parent_id":"t1_g72xqgp","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g72yqhx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g73c5tf":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Chunksize is probably in bytes, not lines. So 100 bytes is likely way less than a single line.\n\nThere are a few really big comments out there, I usually set my chunk size to like 16 megabytes, which is `16,777,216` bytes.","created_utc":["2020-09-29","21:56:56"],"id":"g73c5tf","link_id":"t3_j1x4uw","parent_id":"t3_j1x4uw","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g73c5tf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g73lih0":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"This totally makes sense.   I was definitely thinking of it in terms of lines.  Thanks.","created_utc":["2020-09-29","23:04:54"],"id":"g73lih0","link_id":"t3_j1x4uw","parent_id":"t1_g73c5tf","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g73lih0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ixhjbi":{"author":"StrongSell","author_fullname":"t2_1kjbsz0b","author_premium":false,"created_utc":["2020-09-22","08:36:35"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ixhjbi\/how_do_i_know_how_much_to_back_off\/","id":"ixhjbi","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ixhjbi\/how_do_i_know_how_much_to_back_off\/","selftext":"C:\\\\Users\\\\\\_\\_\\_\\_\\\\Anaconda3\\\\lib\\\\site-packages\\\\psaw\\\\[PushshiftAPI.py:192](https:\/\/PushshiftAPI.py:192): UserWarning: Got non 200 code 429\n\n  warnings.warn(\"Got non 200 code %s\" % response.status\\_code)\n\nC:\\\\Users\\\\\\_\\_\\_\\_\\\\Anaconda3\\\\lib\\\\site-packages\\\\psaw\\\\[PushshiftAPI.py:180](https:\/\/PushshiftAPI.py:180): UserWarning: Unable to connect to [pushshift.io](https:\/\/pushshift.io). Retrying after backoff.\n\n  warnings.warn(\"Unable to connect to [pushshift.io](https:\/\/pushshift.io). Retrying after backoff.\")","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How do I know how much to back off?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ixhjbi\/how_do_i_know_how_much_to_back_off\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"iwegfs":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"created_utc":["2020-09-20","17:07:12"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/","id":"iwegfs","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/","selftext":"https:\/\/imgur.com\/a\/sn3Cinc\nhttps:\/\/github.com\/Fitzy1293\/redditsfinder\n\nThe newish pip package rich made this fun to do.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Made my pushshift program have very handy formatting, let me know what you think of it","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/","comments":{"g5zy37b":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"Here's a video of what it does \n\nhttps:\/\/streamable.com\/d6laxg","created_utc":["2020-09-20","19:50:00"],"id":"g5zy37b","link_id":"t3_iwegfs","parent_id":"t3_iwegfs","permalink":"\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/g5zy37b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g600uuk":{"author":"f_k_a_g_n","author_fullname":"t2_5wrff1n","author_premium":false,"banned_at_utc":null,"body":"Looks good! Thanks for sharing.","created_utc":["2020-09-20","20:13:31"],"id":"g600uuk","link_id":"t3_iwegfs","parent_id":"t1_g5zy37b","permalink":"\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/g600uuk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"Bj8NBMPWyUFax_jeJl3OHyugrQq2e_5lSOq3XM4LVaY","resolutions":[{"height":60,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=512ccdf06dabf32920860864693651ee619ab469","width":108},{"height":121,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f215cbda3f1902572f876aab844b17204e3db1f9","width":216},{"height":180,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=698751a42458a72ae3f3824fb6cf10bba8652e3c","width":320},{"height":360,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=a2c20cf901d2fe10cfb0eff9396b23f7bee0eafc","width":640},{"height":540,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=76438753b4eade5e659cc5c86e67b0ef85168eb6","width":960},{"height":607,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8d7c2a52cc86888e99513448d47eb1bc25135867","width":1080}],"source":{"height":1080,"url":"https:\/\/external-preview.redd.it\/ICHuoeBTHDLTUUTH1Swchjp2AyvZTw48Q0tszRNOeRI.jpg?auto=webp&amp;s=cdd1785f98675de11c28541e4561c9d82fb6d13c","width":1920},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"iw81i2":{"author":"QLZX","author_fullname":"t2_5knmwlnx","author_premium":false,"created_utc":["2020-09-20","08:27:05"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/","id":"iw81i2","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/","selftext":"I'm trying to get every comment made in a 24 hour period  \n\n\nIt doesn't matter if I do this for the last 24 hours' comments, the comments made between 24 and 48 hours ago, or the last 48 hours' comments, I only get 25 comments","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Can you only search for 25 comments at a time?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/","comments":{"g5ybip9":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"No one can help you without seeing your code.","created_utc":["2020-09-20","12:02:21"],"id":"g5ybip9","link_id":"t3_iw81i2","parent_id":"t3_iw81i2","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g5ybip9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g5znutt":{"author":"QLZX","author_fullname":"t2_5knmwlnx","author_premium":false,"banned_at_utc":null,"body":"This was more of a general question. I don\u2019t wanna spend time trying to fix it if there\u2019s a cap to how many results you can get","created_utc":["2020-09-20","18:32:14"],"id":"g5znutt","link_id":"t3_iw81i2","parent_id":"t1_g5ybip9","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g5znutt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g616eb5":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"The default is 25. You have to include a `size` argument in your calls. The current size limit is 100.","created_utc":["2020-09-21","00:29:20"],"id":"g616eb5","link_id":"t3_iw81i2","parent_id":"t1_g5znutt","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g616eb5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g616lt0":{"author":"QLZX","author_fullname":"t2_5knmwlnx","author_premium":false,"banned_at_utc":null,"body":"Oh, that\u2019s an issue for getting all comments from the last day\n\nThank you, though","created_utc":["2020-09-21","00:30:33"],"id":"g616lt0","link_id":"t3_iw81i2","parent_id":"t1_g616eb5","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g616lt0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g6172fs":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"You'll have to make multiple time-specific calls to get all comments from within the last day (depending on the sub) using `before` with the last timestamp from the previous batch of data. There's another way to do it without a timestamp but I don't remember the method off the top of my head, sorry.","created_utc":["2020-09-21","00:32:57"],"id":"g6172fs","link_id":"t3_iw81i2","parent_id":"t1_g616lt0","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g6172fs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jywudk":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"created_utc":["2020-11-22","16:53:23"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jywudk\/when_a_subreddit_is_deprivatized_does_pushshift\/","id":"jywudk","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jywudk\/when_a_subreddit_is_deprivatized_does_pushshift\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"When a subreddit is deprivatized, does Pushshift do anything about the backlog of available comments? Or will only comments made after the deprivatization be ingested?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jywudk\/when_a_subreddit_is_deprivatized_does_pushshift\/","comments":{"gd875f2":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Only the submissions\/comments while it's public.\n\nManual intervention is required to get it to request the back content.","created_utc":["2020-11-22","17:42:44"],"id":"gd875f2","link_id":"t3_jywudk","parent_id":"t3_jywudk","permalink":"\/r\/pushshift\/comments\/jywudk\/when_a_subreddit_is_deprivatized_does_pushshift\/gd875f2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jxs3zw":{"author":"koboldpenisgatherer","author_fullname":"t2_80eswnnl","author_premium":false,"created_utc":["2020-11-20","18:24:30"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/","id":"jxs3zw","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/","selftext":"I'm trying to archive a sub for personal use since its at the top of the eventual ban list (piracy) and I've been using reddit html archiver since January or so of this year on Debian with 0 issues. Recently whenever I type in to fetch links from the first of September to the 15th it gets stuck writing a comment and never gets past it. Doesn't seem to matter the sub neither, tried oblivion out of curiosity and got stuck on the same date.\n\n2020-09-01. \n\nHas there been a change in how date data is encoded or something? I'm a total novice at code I know just enough to lift and modify simple things. I've reduced the rate to 10 and tried doing just a single day (from 2020-09-01 to 2020-09-02 for example) but it still gets stuck and will be at that comment for days. I'm not sure if it's from pushshift's side of things or the lil python tool, so I figured I'd start here first.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Having difficulty downloading data from 2020 September onward using reddit html archiver","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/","comments":{"gd6t859":{"author":"d3rr","author_fullname":"t2_lf73j","author_premium":false,"banned_at_utc":null,"body":"It looks like a pushshift issue, I can reproduce this with conspiracy data:\n\n    python .\/fetch_links.py conspiracy 2020-9-1 2020-9-1\n\nGives\n\n    ...\n     2020-08-31 16:23:28 fetch link ik14eb comments 15\/15\n    2020-08-31 16:25:41 fetch link ik15w0 comments 43\/43\n    2020-08-31 16:26:12 fetch link ik167v comments 8\/8\n    got 10 links, wrote 10 and 292 comments\n    \/home\/ME\/.local\/lib\/python3.8\/site-packages\/psaw\/PushshiftAPI.py:192: UserWarning: Got non 200 code 500\n      warnings.warn(\"Got non 200 code %s\" % response.status_code)\n    \/home\/ME\/.local\/lib\/python3.8\/site-packages\/psaw\/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n      warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n\n\nThe script could handle this 500 error more gracefully, it seems like it never retries. As a workaround, skip that day.","created_utc":["2020-11-22","10:36:58"],"id":"gd6t859","link_id":"t3_jxs3zw","parent_id":"t3_jxs3zw","permalink":"\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/gd6t859\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gd6t94q":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-11-22","10:37:09"],"id":"gd6t94q","link_id":"t3_jxs3zw","parent_id":"t1_gd6t859","permalink":"\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/gd6t94q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jwj00o":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"created_utc":["2020-11-18","18:34:31"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/","id":"jwj00o","num_comments":17,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/","selftext":"Only just dropped an opt out request, but curious if anyone heard back from the admins recently?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Data deletion\/opt out thread","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/","comments":{"gcriud1":{"author":"thetrombonist","author_fullname":"t2_6nilj","author_premium":false,"banned_at_utc":null,"body":"lol nope\n\nI\u2019ve requested multiple times","created_utc":["2020-11-19","00:00:58"],"id":"gcriud1","link_id":"t3_jwj00o","parent_id":"t3_jwj00o","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcriud1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcs96bd":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Did you check yours recently to see?","created_utc":["2020-11-19","03:39:59"],"id":"gcs96bd","link_id":"t3_jwj00o","parent_id":"t1_gcriud1","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcs96bd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcsfhcc":{"author":"thetrombonist","author_fullname":"t2_6nilj","author_premium":false,"banned_at_utc":null,"body":"[Yep!](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?author=thetrombonist)","created_utc":["2020-11-19","04:36:43"],"id":"gcsfhcc","link_id":"t3_jwj00o","parent_id":"t1_gcs96bd","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcsfhcc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcu052w":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Anyone try contacting the admins? May have forgotten.","created_utc":["2020-11-19","16:51:28"],"id":"gcu052w","link_id":"t3_jwj00o","parent_id":"t1_gcsfhcc","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcu052w\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcvwg4z":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"I got a response from \/u\/Stuck_In_the_Matrix this evening, he's planning on running a batch deletion this weekend.","created_utc":["2020-11-20","01:29:05"],"id":"gcvwg4z","link_id":"t3_jwj00o","parent_id":"t1_gcu052w","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcvwg4z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcvwlz6":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Awesome! Thanks.","created_utc":["2020-11-20","01:30:20"],"id":"gcvwlz6","link_id":"t3_jwj00o","parent_id":"t1_gcvwg4z","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcvwlz6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw2xtb":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"What list do you need to be on for that to happen?","created_utc":["2020-11-20","02:25:21"],"id":"gcw2xtb","link_id":"t3_jwj00o","parent_id":"t1_gcvwg4z","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw2xtb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw3e4h":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"The [deletion request megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/)","created_utc":["2020-11-20","02:29:25"],"id":"gcw3e4h","link_id":"t3_jwj00o","parent_id":"t1_gcw2xtb","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw3e4h\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw3gsv":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"Thank you. Is there a way for me to pm someone my old account name?","created_utc":["2020-11-20","02:30:06"],"id":"gcw3gsv","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw3gsv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw49gk":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"you want \/u\/abrownn but he has been afk for a couple days so it may be a while before you get a response.","created_utc":["2020-11-20","02:37:23"],"id":"gcw49gk","link_id":"t3_jwj00o","parent_id":"t1_gcw3gsv","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw49gk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw4kge":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"Thank you, how likely is it that u\/stuck_in_the_matrix would be able to go through the entire list this weekend? I would probably need to send proof of ownership of the old account or something. Not something I can do on a megathread unfortunately.","created_utc":["2020-11-20","02:40:12"],"id":"gcw4kge","link_id":"t3_jwj00o","parent_id":"t1_gcw49gk","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw4kge\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw5209":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"assuming something doesn't go wrong between now and then he should be able to do the entire list but SITM usually gets the list from abrownn so we're also counting on him getting back by this weekend.","created_utc":["2020-11-20","02:44:43"],"id":"gcw5209","link_id":"t3_jwj00o","parent_id":"t1_gcw4kge","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw5209\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcwgzwa":{"author":"delet_this_throwaway","author_fullname":"t2_8n2bxnfu","author_premium":false,"banned_at_utc":null,"body":"Are DM requests going to be addressed in the batch deletion as well? I've reached out to the mod team and don't want to give the account info until I hear back from one of them.","created_utc":["2020-11-20","04:33:47"],"id":"gcwgzwa","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcwgzwa\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcyeqeu":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"abrownn should include the requests he's gotten via dm in the batch.\nHe is still handling the queue but i've not heard anything from him in a few days.","created_utc":["2020-11-20","18:45:44"],"id":"gcyeqeu","link_id":"t3_jwj00o","parent_id":"t1_gcwgzwa","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcyeqeu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf15idu":{"author":"OkRepresentative3046","author_fullname":"t2_94k9zs5t","author_premium":false,"banned_at_utc":null,"body":"can I still request to have my data deleted?","created_utc":["2020-12-08","08:51:26"],"id":"gf15idu","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf15idu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf28e6d":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Sure! request via the [Data Deletion Request Megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/) or DM or Reddit-Chat abrownn","created_utc":["2020-12-08","17:41:46"],"id":"gf28e6d","link_id":"t3_jwj00o","parent_id":"t1_gf15idu","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf28e6d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf31b5x":{"author":"OkRepresentative3046","author_fullname":"t2_94k9zs5t","author_premium":false,"banned_at_utc":null,"body":"thank you!","created_utc":["2020-12-08","21:24:44"],"id":"gf31b5x","link_id":"t3_jwj00o","parent_id":"t1_gf28e6d","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf31b5x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":1605718879.0,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jw4p4f":{"author":"Willing-Sleep3780","author_fullname":"t2_8xlx15j6","author_premium":false,"created_utc":["2020-11-18","01:54:27"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jw4p4f\/is_there_a_way_to_get_the_html_of_a_comment_from\/","id":"jw4p4f","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jw4p4f\/is_there_a_way_to_get_the_html_of_a_comment_from\/","selftext":"In the reddit API there is a property for the comments that has the html of the comment called body\\_html. Is there a similar thing in pushshift? Thanks","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is there a way to get the HTML of a comment from pushshift?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jw4p4f\/is_there_a_way_to_get_the_html_of_a_comment_from\/","comments":{"gcoemu0":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Best you're going to get is the markdown with included regex. You'll have to run the body value through a markdown to html converter and sanitizer. This npm package [marked](https:\/\/www.npmjs.com\/package\/marked) will help a lot. You'll also want to check out DOMPurify if you'd like to sanitize the output html.","created_utc":["2020-11-18","04:10:40"],"id":"gcoemu0","link_id":"t3_jw4p4f","parent_id":"t3_jw4p4f","permalink":"\/r\/pushshift\/comments\/jw4p4f\/is_there_a_way_to_get_the_html_of_a_comment_from\/gcoemu0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jtzo6c":{"author":"holdingroad","author_fullname":"t2_8dfhv0vb","author_premium":false,"created_utc":["2020-11-14","11:54:55"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/","id":"jtzo6c","num_comments":3,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/","selftext":"After what time user's comment get archived? Or that can not be predicted?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"After what time, comments get archived?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/","comments":{"gc9csha":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"ASAP which is typically within a few seconds.\n\nThe API has been having problems with large volumes of spam overwhelming it resulting in it getting multiple hours behind but this seems to have been resolved in the beta.","created_utc":["2020-11-14","17:39:41"],"id":"gc9csha","link_id":"t3_jtzo6c","parent_id":"t3_jtzo6c","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9csha\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc9nxz3":{"author":"holdingroad","author_fullname":"t2_8dfhv0vb","author_premium":false,"banned_at_utc":null,"body":"Writing blank text, so i can edit so we will test this (18:27)","created_utc":["2020-11-14","19:27:41"],"id":"gc9nxz3","link_id":"t3_jtzo6c","parent_id":"t1_gc9csha","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9nxz3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc9qisb":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"reveddit has a page that shows the current delay https:\/\/www.reveddit.com\/info\/\n\nYou can also calculate it using the time difference between \"created_utc\" and \"retrieved_on\" (or local system UTC time which will have slightly more variance but wont break if ingest stops working entirely) for the most recent comment or submission.","created_utc":["2020-11-14","19:51:28"],"id":"gc9qisb","link_id":"t3_jtzo6c","parent_id":"t1_gc9nxz3","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9qisb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jt3w2p":{"author":"buff_pls","author_fullname":"t2_26kogxm1","author_premium":false,"created_utc":["2020-11-12","23:40:34"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/","id":"jt3w2p","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/","selftext":"I'm trying to grab 5000 posts between certain dates but for some reason it ends at 500?\nI'm not too sure how I would separate this into batches either if necessary, and would appreciate any pointers.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Limit of 500 posts on PSAW?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/","comments":{"gc38s34":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I actually just responded to an identical post about this over in r\/redditdev \n\nhttps:\/\/www.reddit.com\/r\/redditdev\/comments\/jsrdpx\/psaw_returns_mismatching_number_of_posts\/","created_utc":["2020-11-12","23:41:46"],"id":"gc38s34","link_id":"t3_jt3w2p","parent_id":"t3_jt3w2p","permalink":"\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/gc38s34\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc3f1fk":{"author":"buff_pls","author_fullname":"t2_26kogxm1","author_premium":false,"banned_at_utc":null,"body":"Great, thanks!","created_utc":["2020-11-13","00:34:25"],"id":"gc3f1fk","link_id":"t3_jt3w2p","parent_id":"t1_gc38s34","permalink":"\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/gc3f1fk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jsl235":{"author":"tangelodev","author_fullname":"t2_7l9fac9h","author_premium":false,"created_utc":["2020-11-12","03:14:51"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jsl235\/getting_a_count_of_comments_by_author\/","id":"jsl235","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jsl235\/getting_a_count_of_comments_by_author\/","selftext":"I was using the aggs parameter with limit=0 before but that doesn't work anymore. Any ideas? Thanks","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Getting a count of comments by author","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jsl235\/getting_a_count_of_comments_by_author\/","comments":{"gbztm8s":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The aggs parameter [has been temporarily disabled](https:\/\/www.reddit.com\/r\/pushshift\/comments\/jm8yyt\/aggregations_have_been_temporarily_disabled_to\/) due to it negatively impacting platform stability.","created_utc":["2020-11-12","03:30:57"],"id":"gbztm8s","link_id":"t3_jsl235","parent_id":"t3_jsl235","permalink":"\/r\/pushshift\/comments\/jsl235\/getting_a_count_of_comments_by_author\/gbztm8s\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jskr1m":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"created_utc":["2020-11-12","02:57:43"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/","id":"jskr1m","num_comments":10,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/","selftext":"Hi,\n\nI have a program that scrapes Pushshift to find all comments and submissions a user has made. I have recently learned that the dumps are actually stored in offline files on Pushshift. I'm trying to figure out which files to download to find *all* of those comments and whatnot because of course it's inefficient and straining to re-fetch the same data from the internet in the future.\n\nfiles.pushshift.io seems a little un-self-explanatory so maybe someone could demystify it for me. Which files show all cumulative comments to date, and all cumulative submissions to date?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Where to download cumulative Reddit dumps (all comments &amp; submissions)?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/","comments":{"gbzq3py":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Comments are [here](https:\/\/files.pushshift.io\/reddit\/comments\/), submissions are [here](https:\/\/files.pushshift.io\/reddit\/submissions\/). Each file is one months worth. There's also three different compression schemes since pushshift has been running for years.\n\nThey are also somewhat out of date, the last 6 months of submissions and 10 months of comments still haven't been uploaded yet. So you'll need to use the API for those timespans.","created_utc":["2020-11-12","03:03:12"],"id":"gbzq3py","link_id":"t3_jskr1m","parent_id":"t3_jskr1m","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzq3py\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzqd5z":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"So you'd have to download every file? There's nothing cumulative?\n\nIt may be possible to concatenate all those files so you have one big file. That'd at least make the directory structure and coding easier to work with.","created_utc":["2020-11-12","03:05:21"],"id":"gbzqd5z","link_id":"t3_jskr1m","parent_id":"t1_gbzq3py","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzqd5z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzqwu5":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well the files are multiple gigabytes each. The whole thing put together, even compressed, would be like 500 gigabytes. Uncompressed it would be over a terabyte.\n\nThere isn't much point having that as one big file, it's too unwieldy to do anything with.","created_utc":["2020-11-12","03:09:49"],"id":"gbzqwu5","link_id":"t3_jskr1m","parent_id":"t1_gbzqd5z","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzqwu5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzs0z3":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Good point. I suppose what I can do is download each file, reduce it to just having the comments\/submisisons I'm interested in, and then save that.\n\nOr alternatively I could just make a dump of all those things with the API, and only update it by getting data dated from later on.","created_utc":["2020-11-12","03:18:38"],"id":"gbzs0z3","link_id":"t3_jskr1m","parent_id":"t1_gbzqwu5","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzs0z3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0abv5":{"author":"f_k_a_g_n","author_fullname":"t2_5wrff1n","author_premium":false,"banned_at_utc":null,"body":"There are over 8 billion reddit comments. I don't think downloading all the data dumps is going to be more efficient than just using the API based on what you described.\n\nData through 2019 is also available on BigQuery. There are some links in the[FAQ](https:\/\/www.reddit.com\/r\/pushshift\/comments\/bcxguf\/new_to_pushshift_read_this_faq\/)","created_utc":["2020-11-12","06:11:08"],"id":"gc0abv5","link_id":"t3_jskr1m","parent_id":"t3_jskr1m","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0abv5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0bcdv":{"author":"inspiredby","author_fullname":"t2_5kk2e","author_premium":false,"banned_at_utc":null,"body":"&gt; Good point. I suppose what I can do is download each file, reduce it to just having the comments\/submisisons I'm interested in, and then save that.\n\n&gt; Or alternatively I could just make a dump of all those things with the API, and only update it by getting data dated from later on.\n\nYSK the data in the compressed files is different from what's in the API. From the [FAQ](https:\/\/www.reddit.com\/r\/pushshift\/comments\/bcxguf\/new_to_pushshift_read_this_faq\/),\n\n&gt; The [Pushshift API](https:\/\/github.com\/pushshift\/api) serves a copy of reddit objects. Currently, data is copied into Pushshift at the time it is posted to reddit. Therefore, scores and other meta such as edits to a submission's `selftext` or a comment's `body` field may not reflect what is displayed by reddit.\n\n&gt; ...\n\n&gt; -\n\n&gt; The files in [files\/comments](https:\/\/files.pushshift.io\/reddit\/comments\/) and [files\/submissions](https:\/\/files.pushshift.io\/reddit\/submissions\/) each represent a copy of one month's worth of objects as they appeared on reddit at the time of the download. For example `RS_2018-08.xz` contains submissions made to reddit in August 2018 as they appeared on September 20th.\n\nScores and other meta may have between the time the data was grabbed for the API and when it was downloaded for the monthly file.","created_utc":["2020-11-12","06:22:08"],"id":"gc0bcdv","link_id":"t3_jskr1m","parent_id":"t1_gbzs0z3","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0bcdv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0n8iz":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Also, Watchful1, I've been wondering: what's the monthly cost of running \/u\/RemindMeBot? It's a great bot so I'm curious how much a bot like it would cost to run.","created_utc":["2020-11-12","08:21:39"],"id":"gc0n8iz","link_id":"t3_jskr1m","parent_id":"t1_gbzqwu5","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0n8iz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0t9s6":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You could easily run it on a $5 a month VPS.","created_utc":["2020-11-12","09:28:39"],"id":"gc0t9s6","link_id":"t3_jskr1m","parent_id":"t1_gc0n8iz","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0t9s6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcoio79":{"author":"new_in_montreal","author_fullname":"t2_8y7bo","author_premium":false,"banned_at_utc":null,"body":"Do we have any information on possible dump updates?","created_utc":["2020-11-18","04:48:30"],"id":"gcoio79","link_id":"t3_jskr1m","parent_id":"t1_gbzq3py","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gcoio79\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcp35ax":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Nope, nothing new.","created_utc":["2020-11-18","08:44:04"],"id":"gcp35ax","link_id":"t3_jskr1m","parent_id":"t1_gcoio79","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gcp35ax\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"js69o5":{"author":"sbs1799","author_fullname":"t2_769nh3nc","author_premium":false,"created_utc":["2020-11-11","13:07:35"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/","id":"js69o5","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/","selftext":"I was wondering if the files provided by Pushshift for a given date has all historical submissions. I see a compressed file for 2020: [https:\/\/files.pushshift.io\/reddit\/submissions\/RS\\_2020-04.zst](https:\/\/files.pushshift.io\/reddit\/submissions\/RS_2020-04.zst). Does this have all submissions made on Reddit since its beginning to April 2020?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How to get all submissions posted ever on Reddit?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/","comments":{"gbxhi9m":{"author":"Dark_Randor","author_fullname":"t2_101p0z","author_premium":true,"banned_at_utc":null,"body":"No, the compressed files only contain the content of that  month, so in your case April 2020. To my knowlege there is no complete file, only the monthly files (and they are already huge if decompressde, so I don\u00b4t think you acutally want a complete file). Theroretically you should be able to create one by attaching all the monthly JSONs to each other.","created_utc":["2020-11-11","14:42:49"],"id":"gbxhi9m","link_id":"t3_js69o5","parent_id":"t3_js69o5","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gbxhi9m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbytlmn":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"Couple heads-ups about patching together the old files \/u\/sbs1799: you'll have to stitch together a handful of compression and packaging schemas to get it all together. You're going to be missing about 7 months of recent data that you'll probably need to use the API for but I don't know that there's a good way to pull a *lot* of data through that.","created_utc":["2020-11-11","21:55:22"],"id":"gbytlmn","link_id":"t3_js69o5","parent_id":"t1_gbxhi9m","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gbytlmn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc4zh39":{"author":"sbs1799","author_fullname":"t2_769nh3nc","author_premium":false,"banned_at_utc":null,"body":"A  follow-up question: why is that size of files ([https:\/\/files.pushshift.io\/reddit\/comments\/](https:\/\/files.pushshift.io\/reddit\/comments\/)) increases with time. Should it not remain the roughly same for all periods?","created_utc":["2020-11-13","11:03:24"],"id":"gc4zh39","link_id":"t3_js69o5","parent_id":"t1_gbytlmn","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gc4zh39\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc5tvwl":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"I think what you're not accounting for is the growth of Reddit over time. You can get a rough idea of how many more people were active year over year just by the file sizes.","created_utc":["2020-11-13","17:58:40"],"id":"gc5tvwl","link_id":"t3_js69o5","parent_id":"t1_gc4zh39","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gc5tvwl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gll3ve9":{"author":"lanfranchi","author_fullname":"t2_9x03s","author_premium":false,"banned_at_utc":null,"body":"anyone know why the files ended in april 2020?","created_utc":["2021-02-01","12:57:57"],"id":"gll3ve9","link_id":"t3_js69o5","parent_id":"t3_js69o5","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gll3ve9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jrylb2":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"created_utc":["2020-11-11","03:38:11"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/","id":"jrylb2","num_comments":24,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/","selftext":"Donate here: https:\/\/www.patreon.com\/pushshift   \n\nCurrently, it costs at least $1,500\/month to run Pushshift. At the time I am writing this, Pushshift is only getting $300\/month (20%) on Patreon. Jason has been working really hard on this project for all of us and is running the project at a $1,200 deficit. Let's see if we can get that $300 month up to $500 a month before the end of November.  \n\nLet's open up our wallets a little bit and give Jason a helping hand. If you can't afford a few bucks a month, reach out to people you know who use Pushshift and ask them if they can please lend a helping hand.   \n\nhttps:\/\/www.patreon.com\/pushshift  \n\nMods who read this post, please consider adding the patreon to the sidebar, not just the FAQ. Jason is a humble guy, it's up to us to raise funds on behalf of his hard work. Let's see some initiative.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Funding Pushshift: Please help.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/","comments":{"gbw5qga":{"author":"huckingfoes","author_fullname":"t2_61kzg","author_premium":true,"banned_at_utc":null,"body":"I suggest posting this over at redditdev too; those folks I imagine would be greatly receptive to helping out this project.","created_utc":["2020-11-11","03:52:25"],"id":"gbw5qga","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbw5qga\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbw67az":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Sent mods a message asking permission. Great suggestion; thank you!","created_utc":["2020-11-11","03:56:46"],"id":"gbw67az","link_id":"t3_jrylb2","parent_id":"t1_gbw5qga","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbw67az\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwdk8j":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"No idea why you are getting downvoted. This is a great suggestion and you went so far as to ask permission which is the right move.","created_utc":["2020-11-11","05:05:54"],"id":"gbwdk8j","link_id":"t3_jrylb2","parent_id":"t1_gbw67az","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwdk8j\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwdq8y":{"author":"crazylegs888","author_fullname":"t2_5icns","author_premium":false,"banned_at_utc":null,"body":"I'll definitely donate before the end of the month.","created_utc":["2020-11-11","05:07:31"],"id":"gbwdq8y","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwdq8y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwlau6":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Awesome! Thank you.","created_utc":["2020-11-11","06:23:45"],"id":"gbwlau6","link_id":"t3_jrylb2","parent_id":"t1_gbwdq8y","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwlau6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwnhy2":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Update, I posted over there: https:\/\/old.reddit.com\/r\/redditdev\/comments\/js1mse\/funding_pushshift_please_help_if_you_can\/?  \n\nThank you for the suggestion and visibility.","created_utc":["2020-11-11","06:48:07"],"id":"gbwnhy2","link_id":"t3_jrylb2","parent_id":"t1_gbw5qga","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwnhy2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxjq68":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"Maybe dataisbeautiful too?","created_utc":["2020-11-11","15:11:30"],"id":"gbxjq68","link_id":"t3_jrylb2","parent_id":"t1_gbwnhy2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxjq68\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxjt7n":{"author":"DaveChild","author_fullname":"t2_35wpy","author_premium":false,"banned_at_utc":null,"body":"Thanks for this, I've become a Patreon as well. The only way something like this stays free to use is with support through efforts like Patreon.","created_utc":["2020-11-11","15:12:32"],"id":"gbxjt7n","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxjt7n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxvbr6":{"author":"whiplash_06","author_fullname":"t2_4lgr7ozq","author_premium":false,"banned_at_utc":null,"body":"Do you mind if I aggressively cross-post this across the ML and social science subreddits?","created_utc":["2020-11-11","17:10:17"],"id":"gbxvbr6","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxvbr6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyjts1":{"author":"bluzkluz","author_fullname":"t2_4eb1ygn5","author_premium":false,"banned_at_utc":null,"body":"Thanks Jason &amp; Co. This is a very useful resource. I contributed. But I suggest you start charging for heavy usage with a basic free plan. It would make this more reliable and could become somebody's full-time gig.","created_utc":["2020-11-11","20:34:46"],"id":"gbyjts1","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyjts1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbynchp":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Tagging \/u\/CharBram because I'm replying to their comment as well. It would be cool if people could get their rate limit increased by contributing a little extra. That being said, not sure if that melds with Jason's philosophy on open access.","created_utc":["2020-11-11","21:03:41"],"id":"gbynchp","link_id":"t3_jrylb2","parent_id":"t1_gbyjts1","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbynchp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbynni9":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"No thank you necessary. All I did was make a post. Thank you so much for your support. You are absolutely right that the only way to keep these amazing projects alive is with some help. Hats off.","created_utc":["2020-11-11","21:06:14"],"id":"gbynni9","link_id":"t3_jrylb2","parent_id":"t1_gbxjt7n","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbynni9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyo6fi":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I do not mind. In fact, that's exactly what I would love to see. Please, aggressively post!","created_utc":["2020-11-11","21:10:39"],"id":"gbyo6fi","link_id":"t3_jrylb2","parent_id":"t1_gbxvbr6","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyo6fi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyqckd":{"author":"bluzkluz","author_fullname":"t2_4eb1ygn5","author_premium":false,"banned_at_utc":null,"body":"I too am a big believer in open access and there is no contradiction with charging for it. Heavy users can get higher rate limits, reliability uptime, and\/or other benefits with some kind of subscription package. It would be a pity if this immensely useful project shuts down due to funding.","created_utc":["2020-11-11","21:28:41"],"id":"gbyqckd","link_id":"t3_jrylb2","parent_id":"t1_gbynchp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyqckd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbz0t68":{"author":"CharBram","author_fullname":"t2_gzjeb","author_premium":false,"banned_at_utc":null,"body":"Exactly. In its current form it\u2019s simply not sustainable. Costs will keep increasing and without a monetization model it\u2019s eventually going to shut down. What\u2019s he going to do when it\u2019s costing him 3k every month?\n\nAnd this is such a niche use case it\u2019s not great for most types of corporate sponsorship. You can\u2019t use ads because it\u2019s an API and also I doubt there\u2019s a massive amount of users. \n\nOnly option is to charge money for it. \n\nIf no one is willing to pay then it obviously isn\u2019t as valuable as we thought.","created_utc":["2020-11-11","22:57:24"],"id":"gbz0t68","link_id":"t3_jrylb2","parent_id":"t1_gbyqckd","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbz0t68\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbz2vd8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"He's definitely said in the past that once he implements user tokens you'll be able to pay for a higher rate limit.","created_utc":["2020-11-11","23:34:50"],"id":"gbz2vd8","link_id":"t3_jrylb2","parent_id":"t1_gbynchp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbz2vd8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc1nk2g":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"Why would anyone donate when we can't even perform a simple search by author? Put that option back, and I'll gladly donate.","created_utc":["2020-11-12","16:10:28"],"id":"gc1nk2g","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc1nk2g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc20whp":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Searching by author was [added back to the API shortly after it was removed](https:\/\/www.reddit.com\/r\/pushshift\/comments\/eivj0a\/the_author_parameter_has_been_restored_for_the\/) but it remains unavailable from [redditsearch.io](https:\/\/redditsearch.io\/)","created_utc":["2020-11-12","18:06:39"],"id":"gc20whp","link_id":"t3_jrylb2","parent_id":"t1_gc1nk2g","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc20whp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc2qi1q":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"\/u\/Rapiolli you heard it from here.  \n&gt; Put that option back, and I'll gladly donate.  \n\nThis means you'll be donating, right?","created_utc":["2020-11-12","21:15:49"],"id":"gc2qi1q","link_id":"t3_jrylb2","parent_id":"t1_gc20whp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc2qi1q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc39fil":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"&gt; Searching by author remains unavailable from redditsearch.io\n \nI will, once it's fixed.","created_utc":["2020-11-12","23:47:08"],"id":"gc39fil","link_id":"t3_jrylb2","parent_id":"t1_gc2qi1q","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc39fil\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc39nq2":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"This is an open source project that uses pushshift and allows you to search by author: https:\/\/camas.github.io\/reddit-search\/","created_utc":["2020-11-12","23:49:01"],"id":"gc39nq2","link_id":"t3_jrylb2","parent_id":"t1_gc39fil","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc39nq2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc3b9b4":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-11-13","00:02:06"],"id":"gc3b9b4","link_id":"t3_jrylb2","parent_id":"t1_gc39nq2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc3b9b4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcaj67g":{"author":"Remount_Kings_Troop_","author_fullname":"t2_la8cw","author_premium":true,"banned_at_utc":null,"body":"I'm in!","created_utc":["2020-11-14","23:34:45"],"id":"gcaj67g","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gcaj67g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcakdnh":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Thank you! Wow, I can't believe how far we've come thanks to you guys! We're only $19\/month away from $500\/month. Huge difference. Hats off to you and everyone else who has pitched in. Thank you so much!","created_utc":["2020-11-14","23:43:49"],"id":"gcakdnh","link_id":"t3_jrylb2","parent_id":"t1_gcaj67g","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gcakdnh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"PcRN5QVcjOPGoLPElAtNlPq5w6_uhzDM1_LAfn-Vjdc","resolutions":[{"height":35,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=762342761fc726c72828c99e6812bc6e5b980246","width":108},{"height":70,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0e790f039207cd3e43a95cab8ef3ea52fbfee79","width":216},{"height":104,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=047c68a6649c6069b1fafe02ff187cccb78a3b57","width":320},{"height":208,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bbd59d3a63d91404b72199b05f4edbf93506e1d","width":640},{"height":313,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=528913229fcf06447c1b7e8730e278f108bc3db8","width":960}],"source":{"height":313,"url":"https:\/\/external-preview.redd.it\/Oj1t94t-UCwgf9fTVeZoAcBR_8kmvzlg3dvMlxWTRGM.jpg?auto=webp&amp;s=62ff44d8541a96078490724b2899a2566061e397","width":960},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jrn1cn":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"created_utc":["2020-11-10","17:34:55"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/","id":"jrn1cn","num_comments":11,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/","selftext":"I found a deleted post by a user, but it isn't retrieved when I search using pushshift. Why might something not get caught by pushshift? Was it deleted too quickly? Thanks","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Some things don't get caught in pushshift. Why?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/","comments":{"gbu3nm0":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The current PushShift ingest of Reddit can lag significantly behind real time, so it probably just hasn't been ingested yet.","created_utc":["2020-11-10","17:40:00"],"id":"gbu3nm0","link_id":"t3_jrn1cn","parent_id":"t3_jrn1cn","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu3nm0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu3sjp":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"this was a post from several months ago","created_utc":["2020-11-10","17:41:09"],"id":"gbu3sjp","link_id":"t3_jrn1cn","parent_id":"t1_gbu3nm0","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu3sjp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu56jm":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Variety of possible reasons but yes, deleted too quickly is the most likely reason.\n\npushshift aims to get every post\/comment within a few seconds of posting but currently regularly falls behind due to reddit's increasing volume, reddit API limitations and spam. (this is being worked on)\n\nSo how fast is too quickly can vary between a few seconds and several hours.\n\n\nThen there are unusual but not uncommon scenarios like;  \nIn the event something is caught by redddit's spam filter or automoderator pushshift will capture it as deleted even if it's later approved by a moderator unless pushshift was running far enough behind that a moderator was able to approve it before pushshift first saw it.\n\nPushshift also allows people to opt out\/remove their data.","created_utc":["2020-11-10","17:52:25"],"id":"gbu56jm","link_id":"t3_jrn1cn","parent_id":"t3_jrn1cn","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu56jm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6av4":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"When you say \"it isn't retrieved\" does that mean the content of the post was not returned (i.e. it shows as deleted or removed)? Or does it not show up at all?","created_utc":["2020-11-10","18:01:24"],"id":"gbu6av4","link_id":"t3_jrn1cn","parent_id":"t1_gbu3sjp","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6av4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6kdf":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"it shows as  \"\\[removed\\]\"","created_utc":["2020-11-10","18:03:33"],"id":"gbu6kdf","link_id":"t3_jrn1cn","parent_id":"t1_gbu6av4","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6kdf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6wy5":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"Ah in that case the post was technically successfully ingested by Pushshift. However the content of the submission had already been deleted by the user or removed by a moderator prior to getting ingested so it is unavailable. PushShift cannot archive content that was already deleted\/removed on Reddit at the time of ingest.","created_utc":["2020-11-10","18:06:23"],"id":"gbu6wy5","link_id":"t3_jrn1cn","parent_id":"t1_gbu6kdf","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6wy5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu7gi9":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"okay, that makes sense! How long does it take for PushShift to ingest something? thanks","created_utc":["2020-11-10","18:10:39"],"id":"gbu7gi9","link_id":"t3_jrn1cn","parent_id":"t1_gbu6wy5","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu7gi9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbubvjq":{"author":"deleteaway11","author_fullname":"t2_8scc6vpc","author_premium":false,"banned_at_utc":null,"body":"Do they respond back to their followers for such requests and how long does it typically take? I realize they are probably busy with work etc.  I also realize it\u2019s a thankless job to do this for so many users. They deserve respect for treating people\u2019s privacy how it should be which is fortunate.","created_utc":["2020-11-10","18:45:28"],"id":"gbubvjq","link_id":"t3_jrn1cn","parent_id":"t1_gbu56jm","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbubvjq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbufuif":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"There is a [Data Deletion Request Megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/) run by abrownn he usually replies to every request, time varies as it tends to be done in batches.\n\nI've not tried to keep track of the usual times but off hand it seems to be several days, there is work being done to make the process more reliable and eventually self service.\n\n&gt;[Removal requests will be made easier by allowing users who still have an active Reddit account to simply log in via their Reddit account to prove ownership and then be given the ability to remove their data from the cluster. This will automate and speed up removal requests for users who are concerned about their privacy. This page will also allow a user to download all of their comments and posts if they choose to do so before removing their data.](https:\/\/www.reddit.com\/r\/pushshift\/comments\/jplcs1\/growing_pains_and_moving_forward_to_bigger_and\/#form-t3_jplcs1gjs:~:text=Removal%20requests%20will%20be%20made%20easier,do%20so%20before%20removing%20their%20data.)","created_utc":["2020-11-10","19:16:06"],"id":"gbufuif","link_id":"t3_jrn1cn","parent_id":"t1_gbubvjq","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbufuif\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbugmkw":{"author":"deleteaway11","author_fullname":"t2_8scc6vpc","author_premium":false,"banned_at_utc":null,"body":"Okay. I did see that thread. I also saw people emailing but guessing a response may take awhile? Not sure what others have experienced.","created_utc":["2020-11-10","19:22:05"],"id":"gbugmkw","link_id":"t3_jrn1cn","parent_id":"t1_gbufuif","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbugmkw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbunx37":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"From what i'm able to see from here comments in the megathread most reliably get replies usually within a day but the actual removal takes longer.\n\nSITM does the actual removals so it may be faster to contact him directly, the most reliable option being via his twitter @jasonbaumgartne","created_utc":["2020-11-10","20:18:19"],"id":"gbunx37","link_id":"t3_jrn1cn","parent_id":"t1_gbugmkw","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbunx37\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jrm3ew":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"created_utc":["2020-11-10","16:41:31"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrm3ew\/redditsearchio_question\/","id":"jrm3ew","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jrm3ew\/redditsearchio_question\/","selftext":"Does  redditsearch.io allow you to find posts by deleted user accounts? If so, how do I do it? Thanks","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"redditsearch.io Question","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jrm3ew\/redditsearchio_question\/","comments":{"gbtzjo5":{"author":"Subduction","author_fullname":"t2_4ahvv","author_premium":false,"banned_at_utc":null,"body":"And while we're on pushshift.io, if I search for comments since the beginning of my sub it tells me the total number, but if I search for posts it doesn't.\n\nIs there some way to get the total count of posts since the start?","created_utc":["2020-11-10","17:05:21"],"id":"gbtzjo5","link_id":"t3_jrm3ew","parent_id":"t3_jrm3ew","permalink":"\/r\/pushshift\/comments\/jrm3ew\/redditsearchio_question\/gbtzjo5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jqx234":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"created_utc":["2020-11-09","14:52:22"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/","id":"jqx234","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/","selftext":"Hi. Firstly, thanks for the [pushshift.io](https:\/\/pushshift.io). It helped me with fetching data from Reddit.\n\nNow, to the merits of the case. Why won't [pushshift.io](https:\/\/pushshift.io) change its API to the GraphQl? I saw some posts ago that the dev is working for better documentation and simplicity. There has been already made some progress in this case. Having read a few posts about the performance it is said that the number of requests has been increasing for some time. So, why didn't the dev change its API to the one without under- and over-fetch problem to increase API's performance?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Why REST, not Graphql?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/","comments":{"gbprq8d":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"How can you know if there is under\/over fetch without knowing what resources are being queried behind the scenes?","created_utc":["2020-11-09","15:17:28"],"id":"gbprq8d","link_id":"t3_jqx234","parent_id":"t3_jqx234","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbprq8d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpscr6":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"I need only id, title,  url, created\\_utc of the submission. Why do I need download: author,  domain,  full\\_link etc? There are a lot of data downloaded when I make a request. I would prefer choose what data I need in one query rather than download a bulk of data. \n\nI am talking about pushshift - my script over-fetching. I don't know what you are talking about. I am an amateur, I may not have understood something that you told correctly.","created_utc":["2020-11-09","15:24:56"],"id":"gbpscr6","link_id":"t3_jqx234","parent_id":"t1_gbprq8d","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpscr6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpt0gh":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"No worries.\n\nI'm not saying GraphQL is a bad idea, just that it might not fix performance issues, because it really depends how pushshift makes its queries to its datastores, if they can be broken up then yes, maybe when you only care about id, title, url etc then it can resolve  just that and save making some other expensive queries, but if it all comes from the same query then GraphQL isn't going to help pushshift (although you of course will see some benefit in the form of only receiving the fields you asked for... but pushshift still had to fetch them, it's just that GraphQL threw them away)","created_utc":["2020-11-09","15:32:34"],"id":"gbpt0gh","link_id":"t3_jqx234","parent_id":"t1_gbpscr6","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpt0gh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbptq0g":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"Ahh. So my query would go this way. Query -&gt; pushshift -&gt; pushshift\\_database  \nand response -&gt; pushshift -&gt; me? And you are talking about its hardware and software implementation performance.","created_utc":["2020-11-09","15:40:32"],"id":"gbptq0g","link_id":"t3_jqx234","parent_id":"t1_gbpt0gh","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbptq0g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpuetu":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"Pretty much, at the point where it's `pushshift -&gt; pushshift_database` I don't know how that part works, so I can't be sure if GraphQL can have benefit to performance.\n\nMaybe it could though?","created_utc":["2020-11-09","15:48:09"],"id":"gbpuetu","link_id":"t3_jqx234","parent_id":"t1_gbptq0g","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpuetu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbqv8d0":{"author":"forgothesemicolon","author_fullname":"t2_z9nhqn1","author_premium":false,"banned_at_utc":null,"body":"I believe you can use the `fields` parameter in order to restrict what data is returned. For example:\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/submission?size=5&amp;subreddit=pushshift&amp;fields=id,title,url,created_utc  \n\nThere I set fields to `id,title,url,created_utc` which means only the fields you mentioned are returned. I'm not sure what happens behind the scenes but that should stop your script over fetching at least. Unless I've misunderstood what your asking?","created_utc":["2020-11-09","21:03:20"],"id":"gbqv8d0","link_id":"t3_jqx234","parent_id":"t1_gbpscr6","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbqv8d0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbqvt96":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"Thanks. That's what I meant and didn't know about the fields. The problem is with public APIs which are overused by people like me and fetch a lot of data. People like me don't know about the existence of such params.","created_utc":["2020-11-09","21:08:01"],"id":"gbqvt96","link_id":"t3_jqx234","parent_id":"t1_gbqv8d0","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbqvt96\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jmokyd":{"author":"brogrramer","author_fullname":"t2_8kc269wz","author_premium":false,"created_utc":["2020-11-02","16:40:54"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jmokyd\/pushshift_output_for_author_aggregation_changed\/","id":"jmokyd","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jmokyd\/pushshift_output_for_author_aggregation_changed\/","selftext":"[removed]","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Pushshift output for author aggregation changed?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jmokyd\/pushshift_output_for_author_aggregation_changed\/","comments":{},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"oNiluQrWEx1JPsVsHd_rm3Nq4tzdWT7AnI4IpbTZvrA","resolutions":[{"height":108,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbaafc64e8bc5294f675836a9b0d40325beaa35f","width":108},{"height":216,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=34e975f5cccf43d120bdfe2de85c5db86ab33e87","width":216},{"height":320,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f2fe11c3a6150e043c08062c8fd76582aa86c79","width":320}],"source":{"height":346,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?auto=webp&amp;s=327bffbaa7bfda971fc78c62fb736fcf4c083aee","width":346},"variants":{}}]},"edited":null,"removed_by_category":"reddit","media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jlvnhn":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"created_utc":["2020-11-01","05:17:00"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/","id":"jlvnhn","num_comments":3,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/","selftext":"I am working with the Pushshift API to pull comments from Reddit. I am continually running into a JSONDecodeError and was hoping someone could shed some light onto possibly why?\n\nMy code was working two days ago and pulling successfully, but today, without making any changes, it is now throwing an error. The only thing that I can think of that changed is that I posted a comment on Reddit during that time.\n\nWhen I check the [URL](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?author=confusid1&amp;size=100) in my browser, it is returning JSON (most of the time -- I think it threw an error once or twice). Also, I am getting a Response 200 (although every now and then it's throwing a 502).\n\nCan anyone help me out as to the reasoning why this might be happening considering I didn't change my code at all?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"JSONDecodeError help?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/","comments":{"garfjz8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That usually happens when it returns an error instead of the result you're expecting. The cloudflare error page is html so when the json decoder tries to parse it, that's the error you get.\n\nNothing you can do other than catching the error, waiting and trying again.","created_utc":["2020-11-01","05:53:19"],"id":"garfjz8","link_id":"t3_jlvnhn","parent_id":"t3_jlvnhn","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/garfjz8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"garfojt":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Thanks for the reply. So by waiting and trying again, does that mean it should resolve itself in some period of time? Or would I have to implement a try\/except and work around the error?","created_utc":["2020-11-01","05:54:57"],"id":"garfojt","link_id":"t3_jlvnhn","parent_id":"t1_garfjz8","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/garfojt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gari2te":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It should resolve itself","created_utc":["2020-11-01","06:26:45"],"id":"gari2te","link_id":"t3_jlvnhn","parent_id":"t1_garfojt","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/gari2te\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jlmuvq":{"author":"elisewinn","author_fullname":"t2_wt8lr","author_premium":false,"created_utc":["2020-10-31","19:55:38"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jlmuvq\/502_errors\/","id":"jlmuvq","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/","selftext":"Hi guys,\n\nI am getting a lot of 'Server error: (502) Bad Gateway' messages while making calls to the comments &amp; submissions API. \n\nIs any server downtime happening right now?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"502 Errors","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jlmuvq\/502_errors\/","comments":{"gapuua5":{"author":"elisewinn","author_fullname":"t2_wt8lr","author_premium":false,"banned_at_utc":null,"body":"Ha, the good ol' 502 cloudfare screen confirms my diagnosis: [https:\/\/imgur.com\/a\/z7taLG4](https:\/\/imgur.com\/a\/z7taLG4)","created_utc":["2020-10-31","20:02:01"],"id":"gapuua5","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gapuua5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gapv4xw":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"I did too; but after backing off, I was still able to get results. Weird.","created_utc":["2020-10-31","20:04:30"],"id":"gapv4xw","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gapv4xw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaq03pr":{"author":"pandalolz","author_fullname":"t2_4xq41","author_premium":false,"banned_at_utc":null,"body":"It's still working for me, but it's very slow and I'm having to retry requests a lot.","created_utc":["2020-10-31","20:49:25"],"id":"gaq03pr","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gaq03pr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaq1jz9":{"author":"sneaky_dragon","author_fullname":"t2_5l2j2","author_premium":true,"banned_at_utc":null,"body":"I've been seeing the same for the past 24 hours as well. It works sometimes and doesn't work other times.","created_utc":["2020-10-31","21:02:46"],"id":"gaq1jz9","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gaq1jz9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gar4yxl":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"It\u2019s throttling","created_utc":["2020-11-01","03:47:32"],"id":"gar4yxl","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gar4yxl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gasqyz5":{"author":"nick4fake","author_fullname":"t2_n8xxw","author_premium":false,"banned_at_utc":null,"body":"I am getting 500, 502 and 504 erros","created_utc":["2020-11-01","17:15:28"],"id":"gasqyz5","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gasqyz5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gavqodf":{"author":"elisewinn","author_fullname":"t2_wt8lr","author_premium":false,"banned_at_utc":null,"body":"Me too. I have a strong need for consistency so I had to stop the requests. I'll try again today...","created_utc":["2020-11-02","11:09:24"],"id":"gavqodf","link_id":"t3_jlmuvq","parent_id":"t1_gasqyz5","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gavqodf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jiioav":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"created_utc":["2020-10-26","19:17:22"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/","id":"jiioav","num_comments":10,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/","selftext":"Is it on some sort of schedule like it won't have submissions from today until tomorrow, or next week, or a month from now?\n\nThe schedule tab on [pushshift.io](https:\/\/pushshift.io) has something on there from 2018 so I wasn't sure how this works exactly.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How far behind is the Pushshift Reddit data?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/","comments":{"ga6teyy":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you talking about the file dumps or the API? That schedule page definitely isn't reliable.","created_utc":["2020-10-26","20:01:55"],"id":"ga6teyy","link_id":"t3_jiioav","parent_id":"t3_jiioav","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6teyy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6tqfm":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"the api","created_utc":["2020-10-26","20:04:25"],"id":"ga6tqfm","link_id":"t3_jiioav","parent_id":"t1_ga6teyy","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6tqfm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6tw4z":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The API is usually pretty up to date. Right now it's about 30 minutes behind real time, but that varies depending on the time of day.","created_utc":["2020-10-26","20:05:41"],"id":"ga6tw4z","link_id":"t3_jiioav","parent_id":"t1_ga6tqfm","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6tw4z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6u6zs":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"The page with the monthly archive files is quite far behind (Apr 2020 for submissions, Dec 2019 for comments) but the actual Pushshift API tends to be somewhere between near real-time and several hours behind.","created_utc":["2020-10-26","20:08:08"],"id":"ga6u6zs","link_id":"t3_jiioav","parent_id":"t3_jiioav","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6u6zs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6uldu":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"oh wow that's cool, thank you very much","created_utc":["2020-10-26","20:11:22"],"id":"ga6uldu","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6uldu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6uo5k":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"thank you, I was asking about the api, so that is very good news.","created_utc":["2020-10-26","20:11:59"],"id":"ga6uo5k","link_id":"t3_jiioav","parent_id":"t1_ga6u6zs","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6uo5k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6v4ib":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"It can sometimes fall behind by quite a few hours, but generally it's pretty up to date.    I believe the ingest scripts were recently upgraded (or will be) so that should also help.","created_utc":["2020-10-26","20:15:36"],"id":"ga6v4ib","link_id":"t3_jiioav","parent_id":"t1_ga6uo5k","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6v4ib\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6x32e":{"author":"subredditsummarybot","author_fullname":"t2_ydo5x7u","author_premium":false,"banned_at_utc":null,"body":"Do you know how far behind the re-ingest is with the updated scores?","created_utc":["2020-10-26","20:31:19"],"id":"ga6x32e","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6x32e\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6yg74":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Nope, that would be way harder to track consistently.","created_utc":["2020-10-26","20:42:12"],"id":"ga6yg74","link_id":"t3_jiioav","parent_id":"t1_ga6x32e","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6yg74\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaaf61i":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"The [new (beta) ingest](https:\/\/beta.pushshift.io\/search\/reddit\/comments?size=10) appears to be only a few seconds behind.","created_utc":["2020-10-27","18:56:15"],"id":"gaaf61i","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/gaaf61i\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jhawh5":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"created_utc":["2020-10-24","18:33:19"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/","id":"jhawh5","num_comments":11,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/","selftext":"So I want to find the top TEXT posts of all time from a specific subreddit. I tried the api, sort by score desc, but submission_type doesn\u2019t seem to be a filter option. Can someone point me to a methodology to run my search?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Trying to filter api results by submission_type","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/","comments":{"ga0q9yp":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"I think the parameter you\u2019re looking for is is_self. It accepts a Boolean. https:\/\/pushshift.io\/api-parameters\/","created_utc":["2020-10-25","08:40:51"],"id":"ga0q9yp","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0q9yp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0uwdu":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"I thought that was to eliminate cross posts","created_utc":["2020-10-25","09:16:24"],"id":"ga0uwdu","link_id":"t3_jhawh5","parent_id":"t1_ga0q9yp","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0uwdu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0v8cm":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe I\u2019m not understanding your question? I think is_self pulled posts that had a body to them. If is_self is marked as yes, our will only posts that are self.subreddit posts and not include posts that links to external sources whether a cross post or not. I think.","created_utc":["2020-10-25","09:19:32"],"id":"ga0v8cm","link_id":"t3_jhawh5","parent_id":"t1_ga0uwdu","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0v8cm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vb0n":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"What about photo posts with no body?","created_utc":["2020-10-25","09:20:16"],"id":"ga0vb0n","link_id":"t3_jhawh5","parent_id":"t1_ga0v8cm","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vb0n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vs6o":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"I think it will ignore pictures too. Take a look at [this](https:\/\/imgur.com\/a\/njZbGsp). See how the top post says (i.redd.it) and the bottom post says (self.puzzles)? I think the is_self parameter will only pull the self.[subreddit] posts, which typically don't include pictures to my knowledge.","created_utc":["2020-10-25","09:24:21"],"id":"ga0vs6o","link_id":"t3_jhawh5","parent_id":"t1_ga0vb0n","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vs6o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vx2c":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe just give it a shot and see if it accomplishes what you want? Also, if you have an example of a post that is a picture but is a self.post, shoot a link here and someone can take a look.","created_utc":["2020-10-25","09:25:28"],"id":"ga0vx2c","link_id":"t3_jhawh5","parent_id":"t1_ga0vs6o","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vx2c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0wgsu":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"However, upon further investigation, if someone submits a text post with only a title and no body (which I guess is allowed in some subreddits like \/r\/AskReddit), it will pull that as a self_post even though there is no body.\n\nIn that case, you could probably use the selftext parameter to indicate you only want non empty selftext posts. That *might* not be something you can do with the API, but you could build it into a function.","created_utc":["2020-10-25","09:30:23"],"id":"ga0wgsu","link_id":"t3_jhawh5","parent_id":"t1_ga0vb0n","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0wgsu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga1qll0":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"If that\u2019s what this property does then it needs to be more clearly documented. The current description for is_self makes no sense to someone looking at the api doc for the first time.","created_utc":["2020-10-25","14:04:56"],"id":"ga1qll0","link_id":"t3_jhawh5","parent_id":"t1_ga0wgsu","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga1qll0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga811no":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-10-27","02:16:37"],"id":"ga811no","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga811no\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga815eh":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"[Well fuck](https:\/\/github.com\/pushshift\/api\/issues\/61)","created_utc":["2020-10-27","02:17:35"],"id":"ga815eh","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga815eh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga8kiwi":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe it's because the post was also deleted by the author? I don't know how to get around that though...","created_utc":["2020-10-27","05:11:15"],"id":"ga8kiwi","link_id":"t3_jhawh5","parent_id":"t1_ga815eh","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga8kiwi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jg6x0n":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"created_utc":["2020-10-22","22:42:10"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/","id":"jg6x0n","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/","selftext":"I'm sending 1 request per second and getting 429 errors, however I still get these errors when I send 1 request every 5 seconds (checking if files have been updated with wget's '-N' option).\n\nWhat is the correct request limit to use? The version of wget I'm using doesn't support throttling requests back when 429 errors are encountered, so I'm forced to pick a wait time.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"What is files.pushshift.io's request limit","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/","comments":{"g9olwfp":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"For the file downloads? I'm pretty sure it's 5 concurrent connections rather than requests per second. So if you're still downloading other files you have to wait for them to complete.\n\nAlso if you're downloading lots of files, consider [donating](https:\/\/pushshift.io\/donations\/) to stuck_in_the_matrix, he pays for the bandwidth out of pocket.","created_utc":["2020-10-22","23:02:26"],"id":"g9olwfp","link_id":"t3_jg6x0n","parent_id":"t3_jg6x0n","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9olwfp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9omalz":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Well the problem is I'm getting 429 errors even though I'm not downloading files. I'm running wget as follows:\n\n    wget -m -np -R \"index.html,*.js,*.css\" --wait 2 --random-wait https:\/\/files.pushshift.io\/reddit\/\n\nEven though I already have a full copy and no downloading is being done (yet), I'm still getting tons of \"429 Too Many Requests\" errors. It seems likely there's some per second request limit, because I don't hit it immediately.\n\nAlso, I'm already donating, that's the first thing I did when I started mirroring his files. That's the least I can do.","created_utc":["2020-10-22","23:05:16"],"id":"g9omalz","link_id":"t3_jg6x0n","parent_id":"t1_g9olwfp","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9omalz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9oongy":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Ah, you're trying to get the index of files. If I was doing this, I would definitely slow way down from once a second. You could easily wait a minute or more between requests.\n\nIs there a specific reason you need to check that fast?","created_utc":["2020-10-22","23:21:27"],"id":"g9oongy","link_id":"t3_jg6x0n","parent_id":"t1_g9omalz","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9oongy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9opql1":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Well I'm using wget in mirror mode - it doesn't give much flexibility on how often requests are done. If I slow it down to a minute between each request, it'll take 2 days to run wget in mirror mode (it does a request for every file with the \"If-Modified-Since\" header). Is there any custom tooling to mirror the files that'd work better? FTP or rsync access would be ideal, but I doubt that exists.","created_utc":["2020-10-22","23:29:03"],"id":"g9opql1","link_id":"t3_jg6x0n","parent_id":"t1_g9oongy","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9opql1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9oreuz":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Ah, I wasn't familiar with that.\n\nI can't think of any better way other than to write something custom yourself. You could try DM'ing stuck_in_the_matrix on twitter and asking about FTP access. It really seems like something that would be easy for him to set up.","created_utc":["2020-10-22","23:41:12"],"id":"g9oreuz","link_id":"t3_jg6x0n","parent_id":"t1_g9opql1","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9oreuz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9ortlg":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Why on twitter specifically? Does he prefer that over reddit PMs?","created_utc":["2020-10-22","23:44:06"],"id":"g9ortlg","link_id":"t3_jg6x0n","parent_id":"t1_g9oreuz","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9ortlg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9os6ew":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"He definitely checks his twitter dms more often than his reddit ones.","created_utc":["2020-10-22","23:46:42"],"id":"g9os6ew","link_id":"t3_jg6x0n","parent_id":"t1_g9ortlg","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9os6ew\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ikh5cn":{"author":"Unrealist99","author_fullname":"t2_2r8u5pzo","author_premium":true,"created_utc":["2020-09-01","12:26:36"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/","id":"ikh5cn","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/","selftext":"I went over to the r\/removeddit subreddit and turns out it's dead? So I came here. Hope I'm not breaking any rules.\n\n* Why does removeddit not show any comments or posts that are under one hour old? For an old post as an example, removeddit only shows Comments which are more than an hour old and not the new ones. I've read removeddit uses pushshift which logs new comments\/submissions every 1-2 seconds. So why the 1 hour gap for removeddit?\n\n* How long must a comment be present (for it to be shown in the removeddit) before being deleted? Is there a minimum timeframe like atleast 2-3 mins? Some of the comments are shown as [Deleted] even on removeddit. So I'm assuming there's a minimum timeframe for any comment.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Doubts about removeddit.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/","comments":{"g3lg5jj":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"&gt;Why does removeddit not show any comments or posts that are under one hour old?\n\npushshift was probably behind again, it does that lately due to high volumes of spam on reddit exceeding it's ingest capacity.\n\n&gt;How long must a comment be present\n\nLong enough for pushshift to see it which is typically only a couple seconds but as above when it gets behind it could be several hours.","created_utc":["2020-09-01","17:57:20"],"id":"g3lg5jj","link_id":"t3_ikh5cn","parent_id":"t3_ikh5cn","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3lg5jj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3orgrm":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Hi, you can try [reveddit](https:\/\/www.reveddit.com). It's a fork of removeddit that's been completely rewritten, and it will also tell you when Pushshift is behind,\n\nhttps:\/\/www.reveddit.com\/info\/","created_utc":["2020-09-02","08:45:04"],"id":"g3orgrm","link_id":"t3_ikh5cn","parent_id":"t3_ikh5cn","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3orgrm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3tjdxv":{"author":"Unrealist99","author_fullname":"t2_2r8u5pzo","author_premium":true,"banned_at_utc":null,"body":"Thanks! I'll check it out.","created_utc":["2020-09-03","10:35:49"],"id":"g3tjdxv","link_id":"t3_ikh5cn","parent_id":"t1_g3orgrm","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3tjdxv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3tjg2f":{"author":"Unrealist99","author_fullname":"t2_2r8u5pzo","author_premium":true,"banned_at_utc":null,"body":"So it's variable for any comment. Thought there would be a minimum fixed timeframe but that's not the case then.","created_utc":["2020-09-03","10:36:52"],"id":"g3tjg2f","link_id":"t3_ikh5cn","parent_id":"t1_g3lg5jj","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3tjg2f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":1598952695.0,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ik3x1w":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"created_utc":["2020-08-31","21:45:52"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/","id":"ik3x1w","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/","selftext":"You see a lot of posts asking for data in Pushshift to be taken out. I want to do just the opposite; get data placed into Pushshift.\n\nA while ago, I made a subreddit that was initially private and I didn't publish it until *after* I had completed the posts on it. Because the posts were made before the subreddit became public, Pushshift didn't ingest them.\n\nIs there anyway I can manually or otherwise get Pushshift to take the posts in? The reason I want this is because I have created a personal tool that scrapes Pushshift (and PRAW) for all my Reddit messgaes, posts and comments and archives them (basically a backup of my Reddit usage). Is there anyway I can get this data in Pushshift so it will be able to fetch it for future usage?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is there anyway I can cause Pushshift to ingest data it previously missed?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/","comments":{"g3iikdf":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":true,"banned_at_utc":null,"body":"Not by the API, you can request SITM to do it but he seems pretty far behind in general at the moment.\n\nLike IIUC the two big things right now are the ingest rewrite so it's not bogged down by spam anymore and removals which seem to be increasing in volume and there isn't an automated way to handle them setup yet.","created_utc":["2020-08-31","23:45:07"],"id":"g3iikdf","link_id":"t3_ik3x1w","parent_id":"t3_ik3x1w","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3iikdf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3ink91":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"I guess I can modify my script so it manually gets the PRAW data for my subreddit instead.","created_utc":["2020-09-01","00:22:15"],"id":"g3ink91","link_id":"t3_ik3x1w","parent_id":"t1_g3iikdf","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3ink91\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3iovwd":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":true,"banned_at_utc":null,"body":"SITM has done it in the past with quarantined subs and stuff but he was lot more active on the sub at the time so YMMV.\n\nIt can be done I just think it's unlikely until after the ingest rewrite is complete which was expected to be complete months ago and i've not seen anything about lately.","created_utc":["2020-09-01","00:31:45"],"id":"g3iovwd","link_id":"t3_ik3x1w","parent_id":"t1_g3ink91","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3iovwd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3joeo2":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Pretty sure the ingest rewrite is done, he just hasn't updated the main ingest to use it yet.","created_utc":["2020-09-01","05:37:19"],"id":"g3joeo2","link_id":"t3_ik3x1w","parent_id":"t1_g3iikdf","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3joeo2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"iitwp3":{"author":"TrAWei09","author_fullname":"t2_59rpqnzb","author_premium":false,"created_utc":["2020-08-29","18:07:06"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/","id":"iitwp3","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/","selftext":"Hi!\n\nI am trying to get all comments from (a lot of submssions from) a subreddits. \n\nTo do this, I've written two methods, one where I fetch a lot of submissions from a subreddit and save locally. It looks like this:\n\n\tdef fetchPosts(subreddit):\n\t\tdata = list(api.search_submissions(after=0,\n\t\t\t\t\t\t\t\t\t\t\tsubreddit=subreddit,\n\t\t\t\t\t\t\t\t\t\t\tlimit=3000000))\n\t\tpickle_out = open('{}.pickle'.format(subreddit),\"wb\")\n\t\tpickle.dump(data, pickle_out)\n\t\tpickle_out.close()\n\nThis one works fine I believe because if you run this on r\/pewdiepiesubmissions the pickle file is 730MB large. \n\nMy problem is with fetching all comments from the submssions. For that, I use this method:\n\n\n\tdef writeCommentsToFile(subreddit):\n    with open('{}.pickle'.format(subreddit), \"rb\") as f: \n      data = pickle.load(f);\n    outputFile = open('{}.txt'.format(subreddit), \"w\", encoding=\"utf-8\")\n    for submission in data:\n        submission.comments.replace_more(limit=None)\n        for comment in submission.comments.list():\n            outputFile.write(comment.body)\n    outputFile.close()\n\nAnd the extracted comments from r\/pewdiepiesubmissions are 4MB large. I used https:\/\/praw.readthedocs.io\/en\/latest\/tutorials\/comments.html as reference. \n\nAny reason why the text files are so small?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Getting all comments from submission, low file size?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/","comments":{"g39kfw4":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"How long does the second script take to complete? It should take literally hours to finish.\n\nWithout seeing any output, I'm guessing it's hitting an error from the reddit api at some point and crashing.","created_utc":["2020-08-29","21:17:35"],"id":"g39kfw4","link_id":"t3_iitwp3","parent_id":"t3_iitwp3","permalink":"\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/g39kfw4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g39m808":{"author":"TrAWei09","author_fullname":"t2_59rpqnzb","author_premium":false,"banned_at_utc":null,"body":"It took a while to finish. Maybe it was just a crapshoot - Im trying the same for r\/coolguides for example. The .pickle file is 100MB large, the .txt file is reaching 70Mb and still running. Maybe the attempt with r\/pewdiepiesubmissions was just a fail?","created_utc":["2020-08-29","21:32:54"],"id":"g39m808","link_id":"t3_iitwp3","parent_id":"t1_g39kfw4","permalink":"\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/g39m808\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"iihqfn":{"author":"mct1","author_fullname":"t2_4o6k6","author_premium":false,"created_utc":["2020-08-29","02:11:32"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/","id":"iihqfn","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/","selftext":"I noticed that files.pushshift.io\/reddit\/comments is missing comments for all of 2020. Have these not been ingested, or are these only available through the API but not the file archive?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Have comments been ingested for 2020?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/","comments":{"g36vaey":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"They just haven't been ingested yet. The process that ingests and creates the files is separate from the one that puts them in the database that backs the API.\n\n\/u\/Stuck_In_the_Matrix is working on catching everything up, but no timeline yet.","created_utc":["2020-08-29","02:52:02"],"id":"g36vaey","link_id":"t3_iihqfn","parent_id":"t3_iihqfn","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g36vaey\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37913u":{"author":"mct1","author_fullname":"t2_4o6k6","author_premium":false,"banned_at_utc":null,"body":"I figured it was something like that. Danke, good sir.","created_utc":["2020-08-29","05:07:37"],"id":"g37913u","link_id":"t3_iihqfn","parent_id":"t1_g36vaey","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g37913u\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g380pfq":{"author":"Sonoff","author_fullname":"t2_bkv9dju","author_premium":false,"banned_at_utc":null,"body":"I am sure that releasing these dumps would be a relief for the API, I am currently calling the API so many times to get 2020 comments whereas it would be one query on BigQuery... just saying ! u\/Stuck_In_the_Matrix","created_utc":["2020-08-29","11:00:12"],"id":"g380pfq","link_id":"t3_iihqfn","parent_id":"t3_iihqfn","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g380pfq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3jx02z":{"author":"douglasg14b","author_fullname":"t2_535da","author_premium":false,"banned_at_utc":null,"body":"Wait, so all the comments exist in the database, but not in exported files?\n\nI would think exporting would just be streaming a query to a JSON transform?\n\nSure it would be intensive, but even a modest\/slow ETL script can process a few hundred thousand records a second. Decent ones can do a few million records\/s.\n\nIs there a problem of some sort going on? I was just about to start a large analytical project based on reddit to prove\/disprove a few thoeries, and post it publicly, but with 8 months of recent data missing that's pretty much pointless.","created_utc":["2020-09-01","07:02:36"],"id":"g3jx02z","link_id":"t3_iihqfn","parent_id":"t1_g36vaey","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3jx02z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3jytmk":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"There's a number of reasons that \/u\/Stuck_In_the_Matrix would want to reingest the data rather than just pulling it from the database. The biggest one is that things are often updated\/deleted in the first few days after being created, so the database doesn't have the most up to date version. Reingesting gives a more accurate copy of the data.","created_utc":["2020-09-01","07:23:11"],"id":"g3jytmk","link_id":"t3_iihqfn","parent_id":"t1_g3jx02z","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3jytmk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3k60u6":{"author":"douglasg14b","author_fullname":"t2_535da","author_premium":false,"banned_at_utc":null,"body":"Ah, gotcha.","created_utc":["2020-09-01","08:57:51"],"id":"g3k60u6","link_id":"t3_iihqfn","parent_id":"t1_g3jytmk","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3k60u6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ii789u":{"author":"DailyGoofy","author_fullname":"t2_7vtx29y3","author_premium":false,"created_utc":["2020-08-28","16:41:45"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/","id":"ii789u","num_comments":8,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/","selftext":"So I wanted to crawl a few subreddits, but I cannot make it work like I used to. \n\nThis is my script:\n\n    start_time = 1483228800\n    batch_size = 100\n    request_fields = \"fields=author,created_utc,full_link,num_comments,score,selftext,title,\"\n    subreddits =[\"Bitcoin\", \"btc\", \"BitcoinMarkets\"] \n    \n    \n    for i in subreddits:\n        for o in range(5000):\n            try:\n                response = requests.get(\"https:\/\/api.pushshift.io\/reddit\/search\/submission\/?subreddit={i}&amp;after={after}&amp;size={size}&amp;{fields}\".format(i=i, after=start_time, size=batch_size, fields=request_fields))\n                data = json.loads(response.text)\n                data = pd.DataFrame(data['data'])\n                start_time = data.iloc[-1][\"created_utc\"]\n                data.to_csv('{i}_{o}.csv'.format(i=i, o=o))\n            except:\n                print(\"Finished crawling \/r\/{i}!\".format(i=i))\n                break\n    \n\nIt returns a few posts since the start\\_time but definetely not all. What am I doing wrong? Is this my fault that it is not working?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Pushshift does not work like it used to. What am I doing wrong?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/","comments":{"g34v08q":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"`{i}_{o}`","created_utc":["2020-08-28","17:01:08"],"id":"g34v08q","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34v08q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34v9m7":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"I'm not sure what's up with your code, but just wanted to say that you can make your life immeasurably easier by using the PSAW package to scrape Pushshift with python.","created_utc":["2020-08-28","17:03:20"],"id":"g34v9m7","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34v9m7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34ybzv":{"author":"DailyGoofy","author_fullname":"t2_7vtx29y3","author_premium":false,"banned_at_utc":null,"body":"it just creates the csv files: bitcoin\\_0.csv, bitcoin\\_1.csv, ...","created_utc":["2020-08-28","17:28:35"],"id":"g34ybzv","link_id":"t3_ii789u","parent_id":"t1_g34v08q","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34ybzv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35345c":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Apart from potentially creating 15k files, what wrong is that you don't respect rate limits and throwing away exceptions","created_utc":["2020-08-28","18:07:01"],"id":"g35345c","link_id":"t3_ii789u","parent_id":"t1_g34ybzv","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g35345c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g353rz0":{"author":"DailyGoofy","author_fullname":"t2_7vtx29y3","author_premium":false,"banned_at_utc":null,"body":"ah okay, do i need to use something like time.sleep(1) to make sure that not too many requests are send per minute?","created_utc":["2020-08-28","18:12:31"],"id":"g353rz0","link_id":"t3_ii789u","parent_id":"t1_g35345c","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g353rz0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g354n6g":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"better yet, use PSAW it will handle things like this for you","created_utc":["2020-08-28","18:19:43"],"id":"g354n6g","link_id":"t3_ii789u","parent_id":"t1_g353rz0","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g354n6g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35m2rs":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Seconded VR_DEVELOPER. The PSAW library automatically handles rate limiting, back offs, and retries.","created_utc":["2020-08-28","20:37:54"],"id":"g35m2rs","link_id":"t3_ii789u","parent_id":"t1_g353rz0","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g35m2rs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37lcdi":{"author":"Administrative-Two41","author_fullname":"t2_71u2jyl3","author_premium":false,"banned_at_utc":null,"body":"Your try-except area is pretty dangerous. You're catching any and all errors as a 'Finished Crawling' behaviour. If, like someone else says, you hit a rate-limit exception, you will crash out, but not know why. You'll want something like this:\n\n     for subreddit in subreddits:\n        for page in pages:\n            try:\n                response = requests.get(```your url```)\n            except Exception as err:\n                if &lt;specific error that is exit condition&gt; in str(err):\n                    print(\"Done\")\n                    break\n                else:\n                    print('Failure on page %d of subreddit %s\" % (page, subreddit)\n                    print('With error: %s) % (err))\n    \n    You can then use something like this to get around sleep conditions:\n    try:\n        data = requests.get(```theurl```)\n    except Exception as error:\n        if str(END_CONDITON) in error:\n            print(\"Done\")\n            break\n        else if str(RATE_LIMIT_ERROR) in error:\n            sleep(DELAY_INTERVAL)\n        else:\n            print(error)","created_utc":["2020-08-29","07:19:21"],"id":"g37lcdi","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g37lcdi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ii69a9":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"created_utc":["2020-08-28","15:37:35"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/","id":"ii69a9","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/","selftext":"In a query of the last 90 days with a score &gt;= 30, I've notice that there are no returns for the past month. Let's zoom in to the last 10 hours:\n\n&lt;https:\/\/api.pushshift.io\/reddit\/submission\/search\/?limit=100&amp;subreddit=AmItheAsshole&amp;after=10h&gt;\n\nThey all have a score of 1! That's weird. Of course, the elapsed time between posting and ingestion on that data is small (under an hour), but let's look at this submissions:\n\n&lt;https:\/\/api.pushshift.io\/reddit\/submission\/search?ids=ii00ur&gt;\n\nIt's managed to accumulate 137 comments but no upvotes? That doesn't make sense.\n\nI've also just realized I'm assuming that once a submission is ingested, that's it. It's a snapshot. Does Pushshift recrawl ingested submissions and change the initial score or num_comments?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Why does score bottom out -- but not num_comments? (Does Pushshift return to posts?)","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/","comments":{"g34vmgv":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Pushshift collects posts pretty close to real time so the score that shows up is the score that existed very soon after the post was made.  Therefore most scores will be very low and do not refelect the actual score of a post over time.\n\nIf you need an accurate score I recommend combining Pushshift with PRAW, which will in fact go get you the most up to date data, but tends to be a little slower.  Example here:\n\n[https:\/\/github.com\/dmarx\/psaw](https:\/\/github.com\/dmarx\/psaw)","created_utc":["2020-08-28","17:06:24"],"id":"g34vmgv","link_id":"t3_ii69a9","parent_id":"t3_ii69a9","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g34vmgv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3586rq":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Hi @Vivd_Walrus, as just [discussed here](https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/), elapsed time between created_utc and retrieved_utc ranges from 0--24 hours over the course of weeks. But that's not my concern here.\n\nI simply don't understand how Pushshift can report 137 comments at time of ingestion and a score of 1. (That's not the only case, just one.)","created_utc":["2020-08-28","18:48:30"],"id":"g3586rq","link_id":"t3_ii69a9","parent_id":"t1_g34vmgv","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g3586rq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35d0xi":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well pushshift ingests the comments too. Maybe it's updating the comment count on the post itself.","created_utc":["2020-08-28","19:27:12"],"id":"g35d0xi","link_id":"t3_ii69a9","parent_id":"t1_g3586rq","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35d0xi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35gp14":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-08-28","19:55:53"],"id":"g35gp14","link_id":"t3_ii69a9","parent_id":"t1_g35d0xi","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35gp14\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35gtlw":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"That would make sense. \n\n1. PS sees a submission, ingests it along with author, title, body_text, score, num_comments, etc.\n2. As comments come in, it store those and update their parent's num_comments but does not revisit the initial score or if the author or score have changed.\n\nI wonder if it is true? I've been surprised that num_counts on push_shift is so close to actual counts on Reddit.","created_utc":["2020-08-28","19:56:52"],"id":"g35gtlw","link_id":"t3_ii69a9","parent_id":"t1_g35d0xi","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35gtlw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35lwik":{"author":"theoddjosh","author_fullname":"t2_3r7pv","author_premium":false,"banned_at_utc":null,"body":"Can you explain more about how combining Pushshift with PRAW can get more updated data?\n\nI've been using Pushshift to retrieve some submission post selftext data, but oftentimes I see submissions as [removed] (because Pushshift took it's snapshot very quickly, and those particular posts were caught in the Automod filter and removed). The posts were later approved but since pushshift took its snapshot already the selftext data was never updated.","created_utc":["2020-08-28","20:36:33"],"id":"g35lwik","link_id":"t3_ii69a9","parent_id":"t1_g34vmgv","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35lwik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g36pz4k":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"You'd use the ID found from pushshift and query reddit itself. See [my code here](https:\/\/github.com\/reagle\/reddit\/blob\/master\/reddit-query.py).","created_utc":["2020-08-29","02:02:33"],"id":"g36pz4k","link_id":"t3_ii69a9","parent_id":"t1_g35lwik","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g36pz4k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ihw6r2":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"created_utc":["2020-08-28","02:35:37"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/","id":"ihw6r2","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/","selftext":"I calculate the time passed between something being posted on Reddit and Pushshift's ingestion via `elapsed_hours = round((r[\"retrieved_on\"] - r[\"created_utc\"]) \/ 3600)`. There's an odd pattern though. As you can see in the screen shot, the time it takes Pushshift to ingest posts is steadily decreasing and then it jumps out to 43 hours and then another slow, regular decline.\n\nhttps:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=2474&amp;format=png&amp;auto=webp&amp;s=90e6e35f07f1f413b6488f12f3a752965e3f68d5","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Why is there weird pattern in when pushshift ingests posts?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/","comments":{"g32xooj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That was two months ago right? It's certainly possible for pushshift to collect data, but something goes wrong with the database and it's never inserted. So it could fall way behind, then catch up instantly.\n\nSomething like that happened just a few days ago, [here's a graph](https:\/\/i.imgur.com\/RoHgEpq.png). You can see the normal daily lag bumps and then a big spike in the middle where it fell behind.","created_utc":["2020-08-28","03:03:06"],"id":"g32xooj","link_id":"t3_ihw6r2","parent_id":"t3_ihw6r2","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g32xooj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34hx9l":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Yes, from months ago, and thank you for the graph, which confirms that pattern isn't unusual.","created_utc":["2020-08-28","14:39:11"],"id":"g34hx9l","link_id":"t3_ihw6r2","parent_id":"t1_g32xooj","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g34hx9l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37fwa1":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Sweet chart! Is there any way to share a live view?","created_utc":["2020-08-29","06:18:30"],"id":"g37fwa1","link_id":"t3_ihw6r2","parent_id":"t1_g32xooj","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37fwa1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37grwq":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Unfortunately not. It's a private grafana server I use to monitor my various bots.","created_utc":["2020-08-29","06:27:50"],"id":"g37grwq","link_id":"t3_ihw6r2","parent_id":"t1_g37fwa1","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37grwq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37xj3d":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Cloudflare can cache private servers, or certain routes (including ones that return json), with very little setup. The server would only get one hit every 2 hours. And if you still want a live view for yourself you can make that separate.","created_utc":["2020-08-29","10:07:31"],"id":"g37xj3d","link_id":"t3_ihw6r2","parent_id":"t1_g37grwq","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37xj3d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":{"nwkk0hlsrmj51":{"e":"Image","id":"nwkk0hlsrmj51","m":"image\/png","p":[{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b6924a9f78b084fdc73aa2dfbeefea73394a330b","x":108,"y":59},{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=aa80dddf06304de8b1037b083a046d46b69ff42d","x":216,"y":119},{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f20fc7aefca622128d59d99abd9ef85ea3890d5e","x":320,"y":177},{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d743968fa5da3ddeeed82f93feaee07e08cb0a5a","x":640,"y":355},{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad708b463d2996e2d5df3bace67cf1a39a95e6d1","x":960,"y":533},{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=aacc55e329ee7fe878b8b868c09baa0294a1e78a","x":1080,"y":599}],"s":{"u":"https:\/\/preview.redd.it\/nwkk0hlsrmj51.png?width=2474&amp;format=png&amp;auto=webp&amp;s=90e6e35f07f1f413b6488f12f3a752965e3f68d5","x":2474,"y":1374},"status":"valid"}},"thumbnail_height":77.0,"thumbnail_width":140.0,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ihv2qw":{"author":"Agentyscr","author_fullname":"t2_7ug4exka","author_premium":false,"created_utc":["2020-08-28","01:29:30"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/","id":"ihv2qw","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/","selftext":"In april I scraped some subreddits and built a dataset. It was about was about 800MB of data. Today I wanted to update my dataset with the new submissions since april. But it returned just a few submissions per subreddit.\n\nSo I ran my script from April again. It took a few seconds and returned 2MB of data instead of 800MB. After a short inspection I figured that Pushshift returns only 100 results per request now. But still, my script is fine and should return the entire dataset. \n\nDid something else happen why I can't get the data I need? Was data deleted or something? What happened?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Cannot get all submissions that i am looking for - Pushshift error or my error?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/","comments":{"g32qzt6":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I doubt anyone can help you without seeing your code.","created_utc":["2020-08-28","02:02:27"],"id":"g32qzt6","link_id":"t3_ihv2qw","parent_id":"t3_ihv2qw","permalink":"\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/g32qzt6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g33hvwf":{"author":"jmreagle","author_fullname":"t2_a6ftd","author_premium":false,"banned_at_utc":null,"body":"It could be that the limit went from 1000, 500, 100 per request over the past year. (Just learned this myself!)","created_utc":["2020-08-28","06:08:36"],"id":"g33hvwf","link_id":"t3_ihv2qw","parent_id":"t3_ihv2qw","permalink":"\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/g33hvwf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ihslxi":{"author":"TrAWei09","author_fullname":"t2_59rpqnzb","author_premium":false,"created_utc":["2020-08-27","23:14:19"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihslxi\/trying_to_scrape_all_comments_from_a_subreddit\/","id":"ihslxi","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ihslxi\/trying_to_scrape_all_comments_from_a_subreddit\/","selftext":"Hello,\n\nI am searching for ways to get all comments from a subreddit. \n\nMy best try so far was to do this via API-call:\n\nSay, I want to scrape comments from a subreddit, what I can do is send a GET-Request to this:\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/comment?subreddit=ASUBREDDIT\n\nObviously, this gets me the last ... posts. \n\nNow an idea is to use the before and size parameter like so:\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/comment?subreddit=ASUBREDDIT&amp;before=9999999999&amp;size=100000000000\n\nthis still does not give me all comments. Is there any way to write API calls so I can get all comments or do I have to use psaw for this?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Trying to scrape all comments from a subreddit using pushshift api","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihslxi\/trying_to_scrape_all_comments_from_a_subreddit\/","comments":{"g32bn4t":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"It's helpful to start with some clarification of terms. There are separate endpoints for `comment` and `submission`. Both of your calls are to the `comment` endpoint. \n\nA better approach would be to use the `submission` endpoint with iterating before\/after arguments to get all submissions from the subreddit, then gather comments using the appropriate endpoint and a `link_id` argument","created_utc":["2020-08-27","23:55:07"],"id":"g32bn4t","link_id":"t3_ihslxi","parent_id":"t3_ihslxi","permalink":"\/r\/pushshift\/comments\/ihslxi\/trying_to_scrape_all_comments_from_a_subreddit\/g32bn4t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ihdw0z":{"author":"LukeAbby","author_fullname":"t2_c4pwnc","author_premium":false,"created_utc":["2020-08-27","06:42:38"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihdw0z\/golang_pushshift_library\/","id":"ihdw0z","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ihdw0z\/golang_pushshift_library\/","selftext":"Does a Pushshift library in Golang exist already? I'd be interested in creating one if one does not exist but I figure I should check for existence before jumping to do it. I did some googling but some smaller libraries could exist not high up to find.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Golang Pushshift Library","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihdw0z\/golang_pushshift_library\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ih66b8":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"created_utc":["2020-08-26","22:58:57"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/","id":"ih66b8","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/","selftext":"Based on the [docs](https:\/\/pushshift.io\/api-parameters\/) I thought I could get up to 500 results, but though I've set `size` to 150, I only get 100.\n\n&lt;https:\/\/api.pushshift.io\/reddit\/submission\/search\/?size=150&amp;subreddit=AmItheAsshole&amp;after=2018-04-01&gt;\n\nAlso, is there a `limit` paramater as well, and if so, is it different than size?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Difference between size and limit, and are they capped at 100?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/","comments":{"g2y8t69":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I've always used `limit`, but I imagine that `size` is just an alias.\n\nThe max limit was changed to 100 sometime in the last few months.","created_utc":["2020-08-26","23:49:22"],"id":"g2y8t69","link_id":"t3_ih66b8","parent_id":"t3_ih66b8","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2y8t69\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2yjrff":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Ah. That was very confusing\/frustrating. The [docs on github should be  updated](https:\/\/github.com\/pushshift\/api). \n\nAny suggestions on how to get more than a 100? (I've been learning\/testing with 5-100 the past week and was finally ready to grab a couple hundred for analysis.) \nCan we pay to remove limits? (My project is a small one-off one.)\nPerhaps someone could share their Python function that does multiple pulls over a date range by breaking them up into smaller contiguous requests?","created_utc":["2020-08-27","01:16:18"],"id":"g2yjrff","link_id":"t3_ih66b8","parent_id":"t1_g2y8t69","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2yjrff\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2ymx66":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You just take the created_utc timestamp of the last object and use it with the before parameter.\n\nI have a script [here](https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py) that downloads all of a users posts\/comments which has an example.","created_utc":["2020-08-27","01:42:59"],"id":"g2ymx66","link_id":"t3_ih66b8","parent_id":"t1_g2yjrff","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2ymx66\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g31iv6i":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Thanks. I added `collect_pushshift_results()` to [my own code](https:\/\/github.com\/reagle\/reddit\/blob\/master\/reddit-query.py#L158) and it seems to be working.","created_utc":["2020-08-27","20:21:24"],"id":"g31iv6i","link_id":"t3_ih66b8","parent_id":"t1_g2ymx66","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g31iv6i\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"giskxwm":{"author":"QishengLi","author_fullname":"t2_117l3k","author_premium":false,"banned_at_utc":null,"body":"May I ask, what is the `limit` parameter? I didn't find it in the documentation...","created_utc":["2021-01-10","22:06:43"],"id":"giskxwm","link_id":"t3_ih66b8","parent_id":"t3_ih66b8","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/giskxwm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"giw4137":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"As watchful1 stated `limit` appears to be an alias for `size` so both function exactly the same.","created_utc":["2021-01-11","17:42:07"],"id":"giw4137","link_id":"t3_ih66b8","parent_id":"t1_giskxwm","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/giw4137\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ievmmh":{"author":"bwz3r","author_fullname":"t2_3vyzbhqn","author_premium":false,"created_utc":["2020-08-23","05:40:18"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ievmmh\/i_wrote_this_tool_to_help_you_search_and_organize\/","id":"ievmmh","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ievmmh\/i_wrote_this_tool_to_help_you_search_and_organize\/","selftext":"I hope you like it! :)\n\n[https:\/\/github.com\/web-temps\/SnootyScrape](https:\/\/github.com\/web-temps\/SnootyScrape)","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"I wrote this tool to help you search and organize Pushshift data","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ievmmh\/i_wrote_this_tool_to_help_you_search_and_organize\/","comments":{},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"qAn5PXdabMUToE2E9ypB77Ayk-UEGEwUCN0Y4h7fxT8","resolutions":[{"height":108,"url":"https:\/\/external-preview.redd.it\/sjich_-hZ-f_f00QJNcJVuvXtaJCy-a7PQ3vCFz3iBY.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=86a5c0b06d32a1cc319f2fb4c9b1d37e8071cefd","width":108},{"height":216,"url":"https:\/\/external-preview.redd.it\/sjich_-hZ-f_f00QJNcJVuvXtaJCy-a7PQ3vCFz3iBY.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c2fb73e82af39314f1aa00acd81bae09873b1e74","width":216},{"height":320,"url":"https:\/\/external-preview.redd.it\/sjich_-hZ-f_f00QJNcJVuvXtaJCy-a7PQ3vCFz3iBY.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=7565a59999559a7b40ee0cc82a1d839fa6cb9a16","width":320}],"source":{"height":420,"url":"https:\/\/external-preview.redd.it\/sjich_-hZ-f_f00QJNcJVuvXtaJCy-a7PQ3vCFz3iBY.jpg?auto=webp&amp;s=0910e340f54e4c1bbe16cdd9f3b5f4a36fc9deb5","width":420},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ieq6p7":{"author":"Mdsp9070","author_fullname":"t2_t7qtw09","author_premium":false,"created_utc":["2020-08-22","23:50:41"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ieq6p7\/doubts_and_siggestions\/","id":"ieq6p7","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ieq6p7\/doubts_and_siggestions\/","selftext":"There's a easy way to get \"all\" comments and reply of a submission with pushshift without making dozens of requests?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Doubts and siggestions","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ieq6p7\/doubts_and_siggestions\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"icykt5":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"created_utc":["2020-08-20","02:02:07"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/","id":"icykt5","num_comments":18,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/","selftext":"I searched a guy's profile once and found like 5000 mentions of \"Monsanto\" over 5 years. Better that corporations can protect such people. \n\nI was felt *absurdly* empowered when I first realized I could actually analyze other users and instantly find anything *I* have said in the past. Why is that a bad thing?\n\nOh, lemme guess, \"witch-hunting\"? Why hasn't AHS been banned yet then? Or \/r\/politics with their singular focus? \n\n#PAID OFF. CLASSIC.\n \nWe'd need an open source program if we ever wanted to escape the corporate stranglehold.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"The only reason user search was removed was because powerful people paid to have it removed.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/","comments":{"g25lt08":{"author":"inspiredby","author_fullname":"t2_5kk2e","author_premium":false,"banned_at_utc":null,"body":"User search still exists in the Pushshift API.\n\nAlso reddit's API is open, it even has RSS if you want to subscribe to a certain user.","created_utc":["2020-08-20","02:07:08"],"id":"g25lt08","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25lt08\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25m7h1":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"[here](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?author=AKnightAlone&amp;fields=permalink,url,full_link,link,title,body,selftext,self_text&amp;size=100) are your last 100 comments , using pushshift .\n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.\n\ndo you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","02:10:42"],"id":"g25m7h1","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25m7h1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25m8t8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You know pushshift isn't run by reddit right? The owner of pushshift has nothing to do with whether subs are banned or not.\n\nIt's actually rather insulting that you would accuse someone who spends thousands of dollars of his own money each month providing this service of being paid off. If he didn't want to do it you would have no way at all of searching for comments.","created_utc":["2020-08-20","02:11:02"],"id":"g25m8t8","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25m8t8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25o4gm":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"Sorry, last 100 comments is accessible by scrolling through my recent pages. What is it? 25 accessible recent pages or so?","created_utc":["2020-08-20","02:27:41"],"id":"g25o4gm","link_id":"t3_icykt5","parent_id":"t1_g25m7h1","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25o4gm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25og92":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"I just told you I found someone with 5000 mentions of \"Monsanto\" in 5 years. Are you implying there's no incentive for shills to hide their efforts? Hell, I'd pay this guy *several* thousand if I was specifically Monsanto. Why is it surprising to imagine literally *anyone* with money wanting to obscure Reddit user scrutiny?","created_utc":["2020-08-20","02:30:38"],"id":"g25og92","link_id":"t3_icykt5","parent_id":"t1_g25m8t8","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25og92\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25onmk":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"Man you can use the create\\_utc of the last comment and ask pushshift for the 100 comments that happened before the 100th comment. You can repeat that with a loop and get all your data .\n\nAlso the 100 results used to be 1000 , and now for some reason (which maybe is temporal) is 100 .\n\nYou did not address the :\n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.  \n&gt;  \n&gt;do you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","02:32:29"],"id":"g25onmk","link_id":"t3_icykt5","parent_id":"t1_g25o4gm","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25onmk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25p5kw":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; do you have any evidence to support your claim or you are just trashing ?\n\nEvidence seems to have become the age old cry of the oppressor. Is \"capitalism\" evidence, or do you consider that a \"conspiracy theory.\" I'm going to guess you simultaneously ignore all the flaws and corrupt incentives of capitalism, *and* you strangely think there's nothing conspiratorial about entire immense parts of society, which possess extreme power, just coincidentally kind of successfully using that to their advantage in every way that's visible *as well* as plenty of ways that aren't legal(as they constantly get caught) or directly obvious on first glance.\n\n&gt;Man you can use the create_utc of the last comment and ask pushshift for the 100 comments that happened before the 100th comment. You can repeat that with a loop and get all your data .\n\nIs this programmer jargon? Okay, let's wait until someone does that again and makes it the equivalent of uBlock Origin so people don't have to keep jumping around to new exploiters.","created_utc":["2020-08-20","02:36:57"],"id":"g25p5kw","link_id":"t3_icykt5","parent_id":"t1_g25onmk","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25p5kw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25p6ak":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Monsanto really doesn't care about people talking about them on the internet. They certainly don't care enough to pay someone thousands of dollars. And on top of that, the owner of pushshift is a really stand up guy, there's zero chance he would accept money to hide content.\n\nThat's totally aside from the fact that user search wasn't removed. Anyone with a fraction of technical knowledge can still easily download a users entire comment history.\n\nYou sound like a conspiracy theorist.","created_utc":["2020-08-20","02:37:08"],"id":"g25p6ak","link_id":"t3_icykt5","parent_id":"t1_g25og92","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25p6ak\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25pued":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; Monsanto really doesn't care about people talking about them on the internet. They certainly don't care enough to pay someone thousands of dollars.\n\nYou're denying shills? You're denying that PR is *vastly* more important than literally anything a corporation does? \n\nA corporation could directly murder people, and if their PR people convinced everyone it didn't happen, then they wouldn't need to be sued. That's pretty much not even an analogy. That's literally what corporations do. I'm a hemophiliac. Tell me your stance on Bayer and HIV.\n\n&gt;You sound like a conspiracy theorist.\n\nThe \"theory\" part is a little condescending, don't you think?","created_utc":["2020-08-20","02:43:13"],"id":"g25pued","link_id":"t3_icykt5","parent_id":"t1_g25p6ak","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25pued\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25qug8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Seriously, just remove this post.","created_utc":["2020-08-20","02:52:12"],"id":"g25qug8","link_id":"t3_icykt5","parent_id":"t1_g25lt08","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25qug8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25rubt":{"author":"raunakdaga","author_fullname":"t2_obupsj3","author_premium":true,"banned_at_utc":null,"body":"Can this guy be banned mods?","created_utc":["2020-08-20","03:01:12"],"id":"g25rubt","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25rubt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25rw4y":{"author":"dtiftw","author_fullname":"t2_gl20o","author_premium":false,"banned_at_utc":null,"body":"&gt; Hell, I'd pay this guy several thousand if I was specifically Monsanto.\n\nWhy? Do you think that reddit is a target audience for agricultural technology for a company that no longer exists?","created_utc":["2020-08-20","03:01:40"],"id":"g25rw4y","link_id":"t3_icykt5","parent_id":"t1_g25og92","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25rw4y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25s0dr":{"author":"BayesianProtoss","author_fullname":"t2_ut8g0pw","author_premium":false,"banned_at_utc":null,"body":"You just don\u2019t know what you\u2019re doing lol. This dude seriously talking about \u201canalyzing\u201d posts and can\u2019t navigate a basic Json api lmfao","created_utc":["2020-08-20","03:02:43"],"id":"g25s0dr","link_id":"t3_icykt5","parent_id":"t1_g25p5kw","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25s0dr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25s1vp":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"Regarding your first paragraph I will just post [this](https:\/\/external-content.duckduckgo.com\/iu\/?u=http%3A%2F%2Fwww.taoofcolor.com%2Fwp-content%2Fuploads%2FiStock_000003088605Small.jpg&amp;f=1&amp;nofb=1) picture as an answer.\n\nRegarding your second paragraph : does [this](https:\/\/camas.github.io\/reddit-search\/) make you happy ?\n\nYou still have not addressed this : \n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.  \ndo you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","03:03:06"],"id":"g25s1vp","link_id":"t3_icykt5","parent_id":"t1_g25p5kw","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25s1vp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25u0pn":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-08-20","03:20:49"],"id":"g25u0pn","link_id":"t3_icykt5","parent_id":"t1_g25rubt","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25u0pn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25ubcf":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"Oh, goodness. I've already got you tagged as \"Shill Monsanto.\" Fucking eerie. This is straight up horrifying, actually.\n\nI ***specifically*** called you out *over a year ago* ***because*** of Pushshift search functions: https:\/\/www.reddit.com\/r\/TheseFuckingAccounts\/comments\/bclgr3\/monsanto_shill_accounts_found_hunting_for_new\/\n\nHow did you end up in this thread?","created_utc":["2020-08-20","03:23:27"],"id":"g25ubcf","link_id":"t3_icykt5","parent_id":"t1_g25rw4y","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25ubcf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25ux2a":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"There's a Monsanto defender *literally* in this thread I called out over a year ago specifically because I mentioned Monsanto in here. Is that not a little creepy to you?\n\nAnti-Sanders efforts on your part. Interesting. Runs in line with the pro-corporate idea I'm bringing up.","created_utc":["2020-08-20","03:28:52"],"id":"g25ux2a","link_id":"t3_icykt5","parent_id":"t1_g25s0dr","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25ux2a\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25w7rk":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; The only reason user search was removed was because powerful people paid to have it removed.\n&gt; do you have any evidence to support your claim or you are just trashing ?\n\nOver a year ago, I made this post: https:\/\/www.reddit.com\/r\/TheseFuckingAccounts\/comments\/bclgr3\/monsanto_shill_accounts_found_hunting_for_new\/\n\nIn this thread, I see this: \n\nApparently I can't upload to Imgur now, ***but*** the the same guy I called out within the hour was badgering me in this thread. Is that coincidental to you? I ***specifically*** called him out in that thread ***because*** I used Pushshift to look at words he used in the past, and *coincidentally*, immediately when I say \"Monsanto\" in this thread, he's here and telling me it's no longer a business. Even though I ALSO FUCKING MENTION BAYER, THEIR NEW OWNER, KILLED MY FELLOW MINORITY HEMOPHILIACS.","created_utc":["2020-08-20","03:40:31"],"id":"g25w7rk","link_id":"t3_icykt5","parent_id":"t1_g25s1vp","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25w7rk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"icg4px":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"created_utc":["2020-08-19","06:29:59"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/","id":"icg4px","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Added nice terminal output boxes for my pushshift program!","url":"https:\/\/i.redd.it\/s51r5mc9pvh51.gif","comments":{"g29t9vl":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"This looks interesting, can you share more information?","created_utc":["2020-08-21","02:52:51"],"id":"g29t9vl","link_id":"t3_icg4px","parent_id":"t3_icg4px","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g29t9vl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g29xd0y":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"The README.md\n\nhttps:\/\/github.com\/Fitzy1293\/redditsfinder\/blob\/master\/README.md\n\nIt outputs a specified users posted in subs sorted by how many times they posted. \n\nEach post and its most important attributes are read in groups of 100 from pushshift, avoiding rate-limiting, then manipulated to be readable pieces of json. That file is located .\/users\/'yourUser'\/all_posts.json","created_utc":["2020-08-21","03:30:52"],"id":"g29xd0y","link_id":"t3_icg4px","parent_id":"t1_g29t9vl","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g29xd0y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2a3wsr":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Very cool","created_utc":["2020-08-21","04:29:42"],"id":"g2a3wsr","link_id":"t3_icg4px","parent_id":"t1_g29xd0y","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g2a3wsr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2a71uc":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"Thanks","created_utc":["2020-08-21","04:58:29"],"id":"g2a71uc","link_id":"t3_icg4px","parent_id":"t1_g2a3wsr","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g2a71uc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"image","preview":{"enabled":true,"images":[{"id":"smUzDSfzrgThAjmBPWpKjWbcsltdozkqdqoiSM1fvkk","resolutions":[{"height":60,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=108&amp;crop=smart&amp;format=png8&amp;s=945a22a118dce4d5eb8531280c8dc972a8f537b4","width":108},{"height":121,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=216&amp;crop=smart&amp;format=png8&amp;s=98ec9048052a22beb77ee33dea2c739a22201b70","width":216},{"height":180,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=320&amp;crop=smart&amp;format=png8&amp;s=bca23899d4f369496d15ba33d90212cb36971560","width":320},{"height":360,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=640&amp;crop=smart&amp;format=png8&amp;s=31de84ee50b55aecd0d18df80b4cf677ca013224","width":640},{"height":540,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=960&amp;crop=smart&amp;format=png8&amp;s=e5f636cf088be75872f7502815984dbcc0faf0e8","width":960},{"height":607,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=1080&amp;crop=smart&amp;format=png8&amp;s=cd80b4d7a54737f5c99c8bb9bd4cdc980ee2fa6f","width":1080}],"source":{"height":720,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?format=png8&amp;s=e9430cd213b5b5ec599a049be450a16d04934038","width":1280},"variants":{"gif":{"resolutions":[{"height":60,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=108&amp;crop=smart&amp;s=8cc17b8959381f80998c78b274043a99d544b272","width":108},{"height":121,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=216&amp;crop=smart&amp;s=1df0608863a6337b1f4786c8b30e7c342536effe","width":216},{"height":180,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=320&amp;crop=smart&amp;s=2e6b43bed971c3e55dcc975f74a46c7237e35263","width":320},{"height":360,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=640&amp;crop=smart&amp;s=cc9f9d99323fe4942e350aa07612d5e8e93ecb1c","width":640},{"height":540,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=960&amp;crop=smart&amp;s=b6b1c98fe55a755d45b7140986c538e448ab81a3","width":960},{"height":607,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=1080&amp;crop=smart&amp;s=94fa8e8cccfe5e8abde13f44a39de71d6a319d3f","width":1080}],"source":{"height":720,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?s=499d5e5761a8911c5b60658289f9cd87dc053801","width":1280}},"mp4":{"resolutions":[{"height":60,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=108&amp;format=mp4&amp;s=db87e418a6129b0865336340bd614224eda7a875","width":108},{"height":121,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=216&amp;format=mp4&amp;s=e062756ae704a54842413bffef25ecebe709933b","width":216},{"height":180,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=320&amp;format=mp4&amp;s=b043a43b3a021181ac600babd70f86f6e23547a8","width":320},{"height":360,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=640&amp;format=mp4&amp;s=060e499caad9dabb4b58444b74c5a711f1c754ad","width":640},{"height":540,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=960&amp;format=mp4&amp;s=2c46bd9b342e311259715554ef26c862c366bc6f","width":960},{"height":607,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?width=1080&amp;format=mp4&amp;s=05337d47ad887b598ebd97f83f82bc505ad59c7a","width":1080}],"source":{"height":720,"url":"https:\/\/preview.redd.it\/s51r5mc9pvh51.gif?format=mp4&amp;s=b0ac21e9e683479f45fa42998f075e5ef6423238","width":1280}}}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":78.0,"thumbnail_width":140.0,"url_overridden_by_dest":"https:\/\/i.redd.it\/s51r5mc9pvh51.gif","crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"icfhvx":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"created_utc":["2020-08-19","05:46:51"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/","id":"icfhvx","num_comments":21,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/","selftext":"I currently run a script that polls the API between 2 dates for posts on a subreddit.  However, when I poll the API it can sometimes take 30s-5m for a response to return.  This adds up over-time when you're attempting to get all links from a large subreddit.  Anyone else having this problem?  I'm also having a hard-time finding proper guidelines for using the API guidelines.  E.G.  If i'm pulling a subreddit for 1000 posts at a time am I making 1 request or am I doing 10-20 smaller requests to get the full 1000.  Any advice?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Speeding up API calls?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/","comments":{"g229bxk":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"FYI, the API size limit was reduced to 100 results per query last month [due to abuse that impacted system latency](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hncg6q\/has_the_commentsubmission_size_limit_decreased\/fxgx0hz\/).","created_utc":["2020-08-19","05:55:11"],"id":"g229bxk","link_id":"t3_icfhvx","parent_id":"t3_icfhvx","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g229bxk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22af12":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Okay. So if I run a request at limit=1000, is it doing 100x10 for 10 requests?","created_utc":["2020-08-19","06:05:18"],"id":"g22af12","link_id":"t3_icfhvx","parent_id":"t1_g229bxk","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22af12\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22b04b":{"author":"thottius","author_fullname":"t2_hmbrjf6","author_premium":false,"banned_at_utc":null,"body":"If you\u2019re using push shift directly and not some wrapper I\u2019m pretty sure it\u2019ll just return 100 results\n\nIf that is the case, and you\u2019d still like to get 1000 comments, you\u2019ll have to make a work around for that. If you pm me I can link you to the code I\u2019ve made for that","created_utc":["2020-08-19","06:10:46"],"id":"g22b04b","link_id":"t3_icfhvx","parent_id":"t1_g22af12","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22b04b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22b6yg":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"That's 1 API call right?  Any idea on why it can take 30s-5m?","created_utc":["2020-08-19","06:12:36"],"id":"g22b6yg","link_id":"t3_icfhvx","parent_id":"t1_g22b04b","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22b6yg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22bc41":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"If you're performing the queries yourself with GET requests from api.pushshift.io, then it will only return 100 results. You would have to handle pagination using the `after` argument. If you're using [PSAW](https:\/\/github.com\/dmarx\/psaw), it should handle pagination for you with 10x requests with 100 results each.","created_utc":["2020-08-19","06:13:57"],"id":"g22bc41","link_id":"t3_icfhvx","parent_id":"t1_g22af12","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22bc41\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22c028":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I'm using a get request using the before and after arguments.  Before in some point the very distant past(say 1-2 years).  The before argument changes based on the created\\_utc so it moves towards that past date.  I'm just confused as to why it can take a few minutes to get 100 results for 1 api call.  I thought I was going over the limit.","created_utc":["2020-08-19","06:20:12"],"id":"g22c028","link_id":"t3_icfhvx","parent_id":"t1_g22bc41","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22c028\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22cq1g":{"author":"thottius","author_fullname":"t2_hmbrjf6","author_premium":false,"banned_at_utc":null,"body":"Yes that\u2019s one call. No reasons in particular come to mind, especially for a the simple query it sounds like you\u2019re making. Save aggregations, the calls I make usually take seconds. Could try loading the query in browser to check if it\u2019s something in your code. If that took the same time for me, I\u2019d start blaming my WiFi, but you may have already checked that.","created_utc":["2020-08-19","06:27:09"],"id":"g22cq1g","link_id":"t3_icfhvx","parent_id":"t1_g22b6yg","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22cq1g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22de7m":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I grabbed the time before the request and the time after and it took 63 seconds.  So i'm not sure.  It may be my wifi as I have issues with packet loss","created_utc":["2020-08-19","06:33:39"],"id":"g22de7m","link_id":"t3_icfhvx","parent_id":"t1_g22cq1g","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22de7m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22essq":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"I've only ever worked from the current to the past by adjusting the `after` argument. I wonder if there's some weird performance hit going from the past to the current.","created_utc":["2020-08-19","06:47:22"],"id":"g22essq","link_id":"t3_icfhvx","parent_id":"t1_g22c028","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22essq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24iqaw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you saying you send a get request and it takes 5 minutes to respond?","created_utc":["2020-08-19","20:55:51"],"id":"g24iqaw","link_id":"t3_icfhvx","parent_id":"t3_icfhvx","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24iqaw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24jb51":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I sent a get request and it took approx 63 seconds.  If i'm doing 10 calls for 1000 posts thats 10 minutes, if a subreddit has 10000 posts thats approx 1hr and a half.","created_utc":["2020-08-19","21:00:25"],"id":"g24jb51","link_id":"t3_icfhvx","parent_id":"t1_g24iqaw","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24jb51\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24l0jy":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That definitely doesn't sound right. Is there anything complicated about the request or are just filtering on `subreddit` and `before`? Are you requesting really old posts?\n\nThe API should just return an error if you're sending requests too fast.","created_utc":["2020-08-19","21:13:46"],"id":"g24l0jy","link_id":"t3_icfhvx","parent_id":"t1_g24jb51","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24l0jy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24lw3l":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Can you run the request with `&amp;metadata=true` and check the `execution_time_milliseconds` field?","created_utc":["2020-08-19","21:20:15"],"id":"g24lw3l","link_id":"t3_icfhvx","parent_id":"t1_g24jb51","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24lw3l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24nygu":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Heres the url,\n\n    after = 1547856000\n    \n    url = \"https:\/\/api.pushshift.io\/reddit\/submission\/search\/?subreddit={}&amp;sort=desc&amp;sort_type=created_utc&amp;size={}&amp;after={}&amp;before={}\".format(subreddit, size, after, befor)\n    \n    response = requests.get(url, headers=headers).json()\n\nThat's roughly Jan 2018 if I remember correctly.  So it will request posts from Jan 2018 onward until it reaches today.  Yeah, sometimes its super quick like 2 seconds and then it just hangs for 1 to 5 minutes.","created_utc":["2020-08-19","21:35:23"],"id":"g24nygu","link_id":"t3_icfhvx","parent_id":"t1_g24l0jy","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24nygu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24o9yr":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Any particular reason you're using both after and before? I usually just pick a start time, set the before OR after, look at the date of the most recent or oldest post depending on what direction I'm going and use that for the next request. It's definitely possible having both is tripping something up.\n\nEdit: I also don't think you need sort_type, that's the default.","created_utc":["2020-08-19","21:37:43"],"id":"g24o9yr","link_id":"t3_icfhvx","parent_id":"t1_g24nygu","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24o9yr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24u0gk":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"&gt;execution\\_time\\_milliseconds\n\nFor one request with size=100, \n\n    148.99\n\nThe average over 5 requests is,\n\n    201.91363636363633","created_utc":["2020-08-19","22:19:53"],"id":"g24u0gk","link_id":"t3_icfhvx","parent_id":"t1_g24lw3l","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24u0gk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24ucmz":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"&gt;Any particular reason you're using both after and before?\n\nIt was the first way I found when googling how to get all the posts in a subreddit.\n\nWhat is the preferable way?\n\n  \nEdit:\n\nI also provided in another comment, the average execution in milliseconds over 5 requests is 201.91.","created_utc":["2020-08-19","22:22:09"],"id":"g24ucmz","link_id":"t3_icfhvx","parent_id":"t1_g24o9yr","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24ucmz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24uxbb":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"[Here's the script I wrote](https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py) to download posts for a user. The url I use is just\n\n    https:\/\/api.pushshift.io\/reddit\/{}\/search?limit=1000&amp;sort=desc&amp;author={}&amp;before=\n\nIt's definitely possible that having both is causing issues with the way pushshift's databases work. Doesn't hurt to just try without it.","created_utc":["2020-08-19","22:26:06"],"id":"g24uxbb","link_id":"t3_icfhvx","parent_id":"t1_g24ucmz","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24uxbb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24w4k0":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Thanks!  I actually ran a request the below url,\n\n    https:\/\/api.pushshift.io\/reddit\/submission\/search\/?subreddit=music&amp;sort=desc&amp;size=1000&amp;before=1597861083&amp;metadata=true\n\nthe average execution time over 5 requests was 320.47666666666663. Perhaps, when PushShift API gets bogged down thats what can cause the long delays?","created_utc":["2020-08-19","22:35:16"],"id":"g24w4k0","link_id":"t3_icfhvx","parent_id":"t1_g24uxbb","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24w4k0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24wqc1":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"300 milliseconds is like a third of a second. For requests for data that hasn't been requested recently, so the server has to do the lookup rather than pulling it from a cache, that's pretty normal. It wouldn't explain a 30 second response time, much less a 5 minute one.","created_utc":["2020-08-19","22:39:55"],"id":"g24wqc1","link_id":"t3_icfhvx","parent_id":"t1_g24w4k0","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24wqc1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g252pb8":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I know when I tracked the time yesterday to receive a response it took 63 seconds.  Today, it took 1-2s which is very good(using time.time()).  Thanks for the code snippet and the info on the api!  I appreciate that brother","created_utc":["2020-08-19","23:26:58"],"id":"g252pb8","link_id":"t3_icfhvx","parent_id":"t1_g24wqc1","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g252pb8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ibt07s":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"created_utc":["2020-08-18","06:01:06"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/","id":"ibt07s","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/","selftext":"I'm trying to write a big export (all posts from r\/trees for two years) to a .csv using filewriter.writerow in Python, and I'm predictable getting a MemoryError every time. Does anyone know an simply Python workaround for this? Maybe something that doesn't store the entire dataset to RAM before writing it to the .csv?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Another novice question --- any easy workarounds for MemoryError?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/","comments":{"g1y8jjh":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Could you post your code?","created_utc":["2020-08-18","07:37:47"],"id":"g1y8jjh","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g1y8jjh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1z21di":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"pfff just buy more ram: https:\/\/blog.codinghorror.com\/hardware-is-cheap-programmers-are-expensive\/","created_utc":["2020-08-18","15:12:00"],"id":"g1z21di","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g1z21di\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g206en4":{"author":"HecrouxIdiot","author_fullname":"t2_29hmup0c","author_premium":false,"banned_at_utc":null,"body":"depends on the source, the source file type if its from the file dumps.For zstd, you have no other option (afaik) but to put whole file in memory. for others, you can extract data by chunks.\n\nSince your time frame is not that large, why not use the web api? you can reduce the time taken to scrap for low data rate using threads. I had a similar problem regarding zstd files specifically (the other extensions i figured out) so i just went with the web api, before the rate limit was decreased. \n\nEdit: r\/trees is not a low activity subreddit as i thought would be. Even so, there is a tradeoff w.r.t time and memory comparing web api vs file dump.The time aspect is enhanced by the recent rate limit of 100 records per request(last time I checked)","created_utc":["2020-08-18","20:03:54"],"id":"g206en4","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g206en4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25cacf":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"As requested, here's the code I'm using. A work in progress, obviously.\n\nfrom psaw import PushshiftAPIimport csvimport datetime as dt\n\napi = PushshiftAPI()\n\nstart\\_epoch=int(dt.datetime(2012, 11,9).timestamp())\n\nsubreddit = input('Which subreddit would you like to scrape? ')\n\ncorpus = list(api.search\\_submissions(after = start\\_epoch,subreddit = subreddit,limit = 5000))\n\nwith open('D:\/CAMER\/%s.csv' % subreddit, 'w', encoding='utf-8') as csvfile:fieldnames = \\['id', 'title', 'author', 'original content', 'selfpost', 'stickied', 'selftext','time created', 'locked', 'number of comments', 'NSFW', 'permalink', 'score', 'upvote ratio','url'\\]filewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)filewriter.writeheader()\n\nfor i in corpus:\n\nfilewriter.writerow({'id': i.id,'title': i.title,'author': i.author,'original content': i.is\\_original\\_content,'selfpost': i.is\\_self,'time created': i.created\\_utc,'stickied': i.stickied,'locked': i.locked,'NSFW': i.over\\_18,'selftext': i.selftext,#'comment forest': cache,'number of comments': i.num\\_comments,'score': i.score,'upvote ratio': i.upvote\\_ratio,'permalink': i.permalink,'url': i.url})\n\nprint ('We did it!')","created_utc":["2020-08-20","00:44:28"],"id":"g25cacf","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25cacf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25cbgm":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"posted :)","created_utc":["2020-08-20","00:44:43"],"id":"g25cbgm","link_id":"t3_ibt07s","parent_id":"t1_g1y8jjh","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25cbgm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25d2nl":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well 5000 objects shouldn't cause memory errors, but if you're trying to do more, just don't put them in a list. You can just do\n\n    for i in api.search_submissions(after = start_epoch,subreddit = subreddit,limit = 5000):","created_utc":["2020-08-20","00:50:49"],"id":"g25d2nl","link_id":"t3_ibt07s","parent_id":"t1_g25cacf","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25d2nl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ibf4mm":{"author":"yourpaljon","author_fullname":"t2_94kqlcr","author_premium":false,"created_utc":["2020-08-17","17:27:31"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ibf4mm\/search_for_links_that_isnt_part_of_a_certain\/","id":"ibf4mm","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ibf4mm\/search_for_links_that_isnt_part_of_a_certain\/","selftext":"Say I want to search for gifs, but anything except for imgur, is that possible?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Search for links that isn't part of a certain domain?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ibf4mm\/search_for_links_that_isnt_part_of_a_certain\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"iaxn86":{"author":"benji700","author_fullname":"t2_bvgas","author_premium":false,"created_utc":["2020-08-16","21:33:08"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/","id":"iaxn86","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/","selftext":"Is it possible to know what level a comment is when pulling data from pushshift, as in whether it is a response to the submission or a response to another comment in that submission?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Distinguishing responses to a comment vs responses to a submission","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/","comments":{"g1rclau":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Look at \"parent\\_id\" prefix. **t3_** is a link, **t1_** is a comment.","created_utc":["2020-08-16","21:41:12"],"id":"g1rclau","link_id":"t3_iaxn86","parent_id":"t3_iaxn86","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rclau\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rdnwq":{"author":"benji700","author_fullname":"t2_bvgas","author_premium":false,"banned_at_utc":null,"body":"Thanks so much!","created_utc":["2020-08-16","21:50:27"],"id":"g1rdnwq","link_id":"t3_iaxn86","parent_id":"t1_g1rclau","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rdnwq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rqgd7":{"author":"benji700","author_fullname":"t2_bvgas","author_premium":false,"banned_at_utc":null,"body":"How about t2? Are those submissions (original posts) that aren't links?","created_utc":["2020-08-16","23:40:40"],"id":"g1rqgd7","link_id":"t3_iaxn86","parent_id":"t1_g1rclau","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rqgd7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rsmvj":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"per this https:\/\/www.reddit.com\/dev\/api#fullnames it should be an account","created_utc":["2020-08-16","23:58:57"],"id":"g1rsmvj","link_id":"t3_iaxn86","parent_id":"t1_g1rqgd7","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rsmvj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rsn44":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The `t2_` prefix is for the 'author\\_fullname' field","created_utc":["2020-08-16","23:59:00"],"id":"g1rsn44","link_id":"t3_iaxn86","parent_id":"t1_g1rqgd7","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rsn44\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1s9ml9":{"author":"comradeswitch","author_fullname":"t2_118shh","author_premium":false,"banned_at_utc":null,"body":"The simplest way is to check the parent id against the link id. If they're the same, you have a root comment.","created_utc":["2020-08-17","02:36:43"],"id":"g1s9ml9","link_id":"t3_iaxn86","parent_id":"t3_iaxn86","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1s9ml9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ia0ztt":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"created_utc":["2020-08-15","07:14:44"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/","id":"ia0ztt","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/","selftext":"I'm running a PSAW shift to scrape all posts from a subreddit, and after a certain period of time I always get two warnings: \"Got non 200 code 429\" and \"Unable to connect to [pushshift.io](https:\/\/pushshift.io). Retrying after backoff.\"  \n\n\nDoes anyone know what my script is doing after these warnings? Really what I'm trying to understand is whether I should just leave it running (presuming the \"backoff\" just takes some time and then it will go back to work), or whether I need to debug.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"What happens when the script is \"retrying after backoff\"?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/","comments":{"g1iuvzu":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That's mostly normal. It just means you made too many requests and pushshift told you to slow down, so PSAW is slowing down. If you're getting it a lot, every few seconds maybe, something is wrong. But if it's only occasionally you're fine.","created_utc":["2020-08-15","07:18:21"],"id":"g1iuvzu","link_id":"t3_ia0ztt","parent_id":"t3_ia0ztt","permalink":"\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/g1iuvzu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1ivck0":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"Thank you!","created_utc":["2020-08-15","07:21:25"],"id":"g1ivck0","link_id":"t3_ia0ztt","parent_id":"t1_g1iuvzu","permalink":"\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/g1ivck0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"i9iipd":{"author":"Composer_Own","author_fullname":"t2_6q7qs4oz","author_premium":false,"created_utc":["2020-08-14","11:56:30"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/","id":"i9iipd","num_comments":3,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/","selftext":"Anyone aware of a big query data source, where I can find all Reddit submissions(posts) for a subreddit starting 2010, I am aware of fh-bigquery.reddit\\_posts which has data after 12-2015, and then there is pushshift.rt\\_reddit which barely has any data, I cannot use APIs pushshift and PRAW because I want large volumes of data, but they are my last option","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Bigquery data source for getting all posts in a subreddit","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/","comments":{"g1fg4zd":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"In a specific subreddit?","created_utc":["2020-08-14","13:47:47"],"id":"g1fg4zd","link_id":"t3_i9iipd","parent_id":"t3_i9iipd","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1fg4zd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1g3m1j":{"author":"Composer_Own","author_fullname":"t2_6q7qs4oz","author_premium":false,"banned_at_utc":null,"body":"Yes I have a list of 11 subreddits for which I need to find all posts and comments since 2010, fh-bigquery has tables after 12-2015 which I can query but how do I get information before that date, do we have any other dataset, considering the fact that pushshift.rt\\_reddit does not have data.","created_utc":["2020-08-14","17:52:48"],"id":"g1g3m1j","link_id":"t3_i9iipd","parent_id":"t1_g1fg4zd","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1g3m1j\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1sa8xo":{"author":"comradeswitch","author_fullname":"t2_118shh","author_premium":false,"banned_at_utc":null,"body":"The fh-bigquery \"fullcorpus\" table timestamped with Dec 2015 has all the data collected prior to that date for posts. You can check the date of the first post in there with \"select timestamp_seconds(min(created_utc)\" or the like. The v2 dataset has separate tables for 2010 and a few years after but I'm not sure what that is- there are posts in the first dataset that aren't in the second and vice versa for the same time period.","created_utc":["2020-08-17","02:42:55"],"id":"g1sa8xo","link_id":"t3_i9iipd","parent_id":"t1_g1g3m1j","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1sa8xo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"i96aa7":{"author":"Carbon_Rod","author_fullname":"t2_a5z8k","author_premium":false,"created_utc":["2020-08-13","22:17:32"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/","id":"i96aa7","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/","selftext":"Site comes up fine, but when I try to do an actual search, the \"Searching...\" thing never goes away. I'm on Chrome, and I don't have any adblockers or scriptblockers in use on that site.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is redditsearch.io down for everyone?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/","comments":{"g1d2b0z":{"author":"ObsidianDreamsRedux","author_fullname":"t2_czj3bdp","author_premium":false,"banned_at_utc":null,"body":"Seems fine to me. I searched for a few generic words and got results right away.  Firefox on Linux. And I do have pihole doing ad blocking for the network, and it was not a factor at all.","created_utc":["2020-08-13","22:42:29"],"id":"g1d2b0z","link_id":"t3_i96aa7","parent_id":"t3_i96aa7","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1d2b0z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dfs37":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Working for me.","created_utc":["2020-08-14","00:27:27"],"id":"g1dfs37","link_id":"t3_i96aa7","parent_id":"t3_i96aa7","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dfs37\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dhh4o":{"author":"Carbon_Rod","author_fullname":"t2_a5z8k","author_premium":false,"banned_at_utc":null,"body":"Think I figured it out; I had redditsearch.io in a pinned tab. However, it won't work unless you have the full https:\/\/www.redditsearch.io (figured that out trying to use Firefox instead). So, I unpinned and repinned with the https address and it's working again.","created_utc":["2020-08-14","00:40:47"],"id":"g1dhh4o","link_id":"t3_i96aa7","parent_id":"t1_g1d2b0z","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dhh4o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dhn00":{"author":"Carbon_Rod","author_fullname":"t2_a5z8k","author_premium":false,"banned_at_utc":null,"body":"Think my problem was using redditsearch.io, when it wanted the full https:\/\/www.redditsearch.io address (at least, it works when I do that, so problem solved).","created_utc":["2020-08-14","00:42:05"],"id":"g1dhn00","link_id":"t3_i96aa7","parent_id":"t1_g1dfs37","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dhn00\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"i8dlzs":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"created_utc":["2020-08-12","16:20:26"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/","id":"i8dlzs","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/","selftext":"I want to download all posts and comments from \/r\/aoe2\n\nI can use pushift app with date parameter but wonder if there is a a dump somewhere on the Internet that I can just download.\n\nPlease help with advice","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How to download all posts from a subreddit?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/","comments":{"g18c0xf":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"No, there are no per subreddit dumps. There are dumps of all posts\/comments, but put together it's nearly a terrabyte of data, and much more uncompressed. It would be much faster to just use the API.","created_utc":["2020-08-12","19:56:55"],"id":"g18c0xf","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g18c0xf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g18fppy":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Thanks. The combined dumps are beyond what my machine xan handle","created_utc":["2020-08-12","20:23:45"],"id":"g18fppy","link_id":"t3_i8dlzs","parent_id":"t1_g18c0xf","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g18fppy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g193djy":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-08-12","23:19:11"],"id":"g193djy","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g193djy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g193tsq":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"    import requests\n    import time\n\n    subreddit = 'aoe2'\n    maxThings = -1\n    printWait = 2\n    requestSize = 100\n\n\n    def requestJSON(url):\n        while True:\n            try:\n                r = requests.get(url)\n                if r.status_code != 200:\n                    print('error code', r.status_code)\n                    time.sleep(5)\n                    continue\n                else:\n                    break\n            except Exception as e:\n                print(e)\n                time.sleep(5)\n                continue\n        return r.json()\n\n    meta = requestJSON('https:\/\/api.pushshift.io\/meta')\n    limitPerMinute = meta['server_ratelimit_per_minute']\n    requestWait = 60 \/ limitPerMinute\n\n    print('server_ratelimit_per_minute', limitPerMinute)\n\n    things = ('submission', 'comment')\n\n    for thing in things:\n        i = 0\n\n        with open(subreddit + '_' + thing + '_' + str(int(time.time())) + '.txt', 'w') as f:\n            print('\\n[starting', thing + 's]')\n\n            if maxThings &lt; 0:\n\n                url = 'https:\/\/api.pushshift.io\/reddit\/search\/'\\\n                      + thing + '\/?subreddit='\\\n                      + subreddit\\\n                      + '&amp;metadata=true&amp;size=0'\n                \n                json = requestJSON(url)\n                \n                totalResults = json['metadata']['total_results']\n                print('total ' + thing + 's', 'in', subreddit,':', totalResults)\n            else:\n                totalResults = maxThings\n                print('downloading most recent', maxThings)\n\n\n            created_utc = ''\n\n            startTime = time.time()\n            timePrint = startTime\n            while True:\n                url = 'http:\/\/api.pushshift.io\/reddit\/search\/'\\\n                      + thing + '\/?subreddit=' + subreddit\\\n                      + '&amp;size=' + str(requestSize)\\\n                      + '&amp;before=' + str(created_utc)\n\n                json = requestJSON(url)\n\n                if len(json['data']) == 0:\n                    break\n\n                doneHere = False\n                for post in json['data']:\n                    created_utc = post[\"created_utc\"]\n                    f.write(str(post) + '\\n')\n                    i += 1\n                    if i &gt;= totalResults:\n                        doneHere = True\n                        break\n\n                if doneHere:\n                    break\n                \n                if time.time() - timePrint &gt; printWait:\n                    timePrint = time.time()\n                    percent = i \/ totalResults * 100\n                    \n                    timePassed = time.time() - startTime\n                    timeLeft = timePassed \/ i * totalResults\n                    \n                    print('{:.2f}'.format(percent) + '%', '|',\n                          time.strftime(\"%H:%M:%S\", time.gmtime(timePassed)),\n                          '| approx time left',\n                          time.strftime(\"%H:%M:%S\", time.gmtime(timeLeft)))\n\n\n                time.sleep(requestWait)","created_utc":["2020-08-12","23:22:39"],"id":"g193tsq","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g193tsq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g196e3x":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Many thanks. I'll try this out\n\nIf you have a github and work on social media analysis, I would love to follow your work","created_utc":["2020-08-12","23:42:19"],"id":"g196e3x","link_id":"t3_i8dlzs","parent_id":"t1_g193tsq","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g196e3x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g197c0e":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Don't really have anything worth githubbing","created_utc":["2020-08-12","23:49:41"],"id":"g197c0e","link_id":"t3_i8dlzs","parent_id":"t1_g196e3x","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g197c0e\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g19ivbz":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Okay, thanks nonetheless","created_utc":["2020-08-13","01:21:04"],"id":"g19ivbz","link_id":"t3_i8dlzs","parent_id":"t1_g197c0e","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g19ivbz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"i5a2s7":{"author":"Xzanium2","author_fullname":"t2_719l7c1v","author_premium":false,"created_utc":["2020-08-07","11:24:02"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/","id":"i5a2s7","num_comments":8,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/","selftext":"Using \n\n    api.search_comments()\n\ngives comments from r\/all. I want comments from a specific subreddit.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How do you find the comments for a specific subreddit?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/","comments":{"g0nmiiq":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Say you wanted to get the comments of \/r\/pushshift.\n\n`api.search_comments(subreddit=\"Pushshift\")`\n\nhttps:\/\/github.com\/dmarx\/psaw","created_utc":["2020-08-07","11:27:36"],"id":"g0nmiiq","link_id":"t3_i5a2s7","parent_id":"t3_i5a2s7","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g0nmiiq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1c63i1":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"stop adding everyone who comments in r\/INTJ you stupid dumb ass shitty faggot retard no one wants to join your pathetic low-quality sub you fucking moron","created_utc":["2020-08-13","18:43:57"],"id":"g1c63i1","link_id":"t3_i5a2s7","parent_id":"t3_i5a2s7","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1c63i1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cdb0n":{"author":"Xzanium2","author_fullname":"t2_719l7c1v","author_premium":false,"banned_at_utc":null,"body":"Excuse me how is this related to r\/pushshift?","created_utc":["2020-08-13","19:38:09"],"id":"g1cdb0n","link_id":"t3_i5a2s7","parent_id":"t1_g1c63i1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cdb0n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cdh42":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"its what you're using to annoy the fuck out of everyone who comments on r\/INTJ, dont act fucking stupid","created_utc":["2020-08-13","19:39:21"],"id":"g1cdh42","link_id":"t3_i5a2s7","parent_id":"t1_g1cdb0n","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cdh42\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cvwy1":{"author":"ScamWatchReporter","author_fullname":"t2_5w21c3a3","author_premium":false,"banned_at_utc":null,"body":"Report his account, the bot account, and the sub account to rEDDIT.com\/report if you have been added as authorized user to a sub you didn't subscribe to by him and his bots. He's had several accounts banned for it already","created_utc":["2020-08-13","21:54:49"],"id":"g1cvwy1","link_id":"t3_i5a2s7","parent_id":"t1_g1cdh42","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cvwy1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cwpzn":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"I hadn't considered that so thanks so much for bringing that to my attention! I'm about to submit this report right now since this guy and his shitty bot has been annoying the hell out of me for over a week.","created_utc":["2020-08-13","22:00:53"],"id":"g1cwpzn","link_id":"t3_i5a2s7","parent_id":"t1_g1cvwy1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cwpzn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1iomnv":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"Please don't use language like that.","created_utc":["2020-08-15","06:33:53"],"id":"g1iomnv","link_id":"t3_i5a2s7","parent_id":"t1_g1c63i1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1iomnv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1itdy5":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"shut the fuck up Karen I wasn't talking to your bichass","created_utc":["2020-08-15","07:07:45"],"id":"g1itdy5","link_id":"t3_i5a2s7","parent_id":"t1_g1iomnv","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1itdy5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"km63g6":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"created_utc":["2020-12-29","05:31:19"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/","id":"km63g6","num_comments":9,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/","selftext":"I've got a current copy of the pushshift submissions and comment data. Would there be interest in a torrent of the data?\n\nI figured at some point I would also write a tool to ingest the data in to sqlite or another DB. Both would help relieve strain on the pushshift infrastructure. It would also allow people to perform more involved analysis.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is there interest in a torrent?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/","comments":{"ghcx2yg":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I'm planning to put up a torrent of the files. Stuck_In_the_Matrix is recompressing all the older ones which is useful for a couple reasons, so I'm waiting for that to be done rather than put together something that will be out of date in a few months.\n\nThe data is already in a database, that's what the API is backed by. The main limitation is the huge amount of traffic the API gets, so unless you can either provide a lots of bandwidth and processing power, it probably won't make a big difference.","created_utc":["2020-12-29","05:40:34"],"id":"ghcx2yg","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghcx2yg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghczwu4":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"My idea was to develop a pipeline to allow people to download the submissions and comments via bittorrent, and move that data into a local DB for analysis (vs the pushshift API) . This way people can do their work without any burden on the pushshift infra. I wouldn't host anything but the torrents.","created_utc":["2020-12-29","06:07:05"],"id":"ghczwu4","link_id":"t3_km63g6","parent_id":"t1_ghcx2yg","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghczwu4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghe9f4t":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"How would you split it up?","created_utc":["2020-12-29","16:51:35"],"id":"ghe9f4t","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghe9f4t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gheg2ud":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"For reference: https:\/\/archive.org\/details\/reddit-comments-7z","created_utc":["2020-12-29","17:54:00"],"id":"gheg2ud","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/gheg2ud\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghevdi5":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"Absolutely! While pushshift is great, it is evident that it's a part-time venture. Some sort of distributed mirror taking a slightly different approach would be amazing for reliability and would definitely take load off the pushshift servers as heavy users (the ones likely to be causing issues) are more capable of working locally.","created_utc":["2020-12-29","20:02:49"],"id":"ghevdi5","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghevdi5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghf1tq2":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"Yup, similar idea. That was last updated in 2017.","created_utc":["2020-12-29","20:55:26"],"id":"ghf1tq2","link_id":"t3_km63g6","parent_id":"t1_gheg2ud","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghf1tq2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghf1yo6":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"I figure I would put it all in one torrent. Most clients let the user pick their files.","created_utc":["2020-12-29","20:56:33"],"id":"ghf1yo6","link_id":"t3_km63g6","parent_id":"t1_ghe9f4t","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghf1yo6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghfbc40":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"FYI the PushShift dataset is available on BigQuery: [https:\/\/pushshift.io\/using-bigquery-with-reddit-data\/](https:\/\/pushshift.io\/using-bigquery-with-reddit-data\/)","created_utc":["2020-12-29","22:13:26"],"id":"ghfbc40","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghfbc40\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghfyb08":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"Good point, that is sufficient for a lot of people. Based on some posts I've seen here, it appears the BigQuery data goes from 2015 through 2019, so it might not be comprehensive enough for some purposes.\n\nFor me, I prefer having it stored locally so that queries don't go over the network and I don't need to think about a quota, bandwidth, or network latency.","created_utc":["2020-12-30","01:27:15"],"id":"ghfyb08","link_id":"t3_km63g6","parent_id":"t1_ghfbc40","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghfyb08\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kku7y9":{"author":"PickleRickFanning","author_fullname":"t2_9a3bixh3","author_premium":false,"created_utc":["2020-12-27","03:17:18"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/","id":"kku7y9","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Why does push shift only work on some subs?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/","comments":{"gh4knfr":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Why do you think that? It works on all subs other than private ones and maybe some quarantined ones.","created_utc":["2020-12-27","04:00:48"],"id":"gh4knfr","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/gh4knfr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh4ro1x":{"author":"ATownHoldItDown","author_fullname":"t2_8v1x6","author_premium":false,"banned_at_utc":null,"body":"I have a feeling you're using redditsearch.io, and your query is limited to 'today'.\n\nIf you search a sub that was banned a year ago, it won't show anything from today. Likewise, if you search for posts from two years ago for a sub that is 6 months old, it won't show anything.","created_utc":["2020-12-27","05:11:46"],"id":"gh4ro1x","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/gh4ro1x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghkwf05":{"author":"AIArtisan","author_fullname":"t2_1230r07v","author_premium":false,"banned_at_utc":null,"body":"ive had no issues finding data from all subs I have tried. whats your query?","created_utc":["2020-12-31","07:29:06"],"id":"ghkwf05","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/ghkwf05\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghlnao8":{"author":"PickleRickFanning","author_fullname":"t2_9a3bixh3","author_premium":false,"banned_at_utc":null,"body":"I am using an app (removedit) that pulls from push shift but on most of the bigger subs, it can't recover removed or deleted comments\/posts. On smaller subs this isn't usually a problem. Granted, I am using an app that may not be querying the api in the right way but it does work sometimes and others not. \n\nIs this because of settings possibly set by mods in the default subs (for example), the app not working correctly or something else entirely?","created_utc":["2020-12-31","14:36:51"],"id":"ghlnao8","link_id":"t3_kku7y9","parent_id":"t1_ghkwf05","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/ghlnao8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kkdlr5":{"author":"TheMedernShairluck","author_fullname":"t2_5kp5wq5q","author_premium":false,"created_utc":["2020-12-26","07:50:57"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/","id":"kkdlr5","num_comments":3,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/","selftext":"Hello everyone!\n\nI really like redditsearch.io and find it very useful. However, I only recently noticed that the max search results is 100. Is there anyway to increase the limit (e.g. 200 or 500), or is there any plan on doing so?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Increase the number of search results on redditsearch.io?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/","comments":{"gh1yyt8":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Kinda sorta? 100 results is an API limitation, it used to be higher but its been reduced to 100 for performance reasons.\nThe way around this is to paginate through the results.\n\nThere's a third party front end at https:\/\/camas.github.io\/reddit-search\/ that gives a more button that will request the next 100 results. It's not automatic but it's still a large step up from doing the pagination manually.","created_utc":["2020-12-26","08:45:34"],"id":"gh1yyt8","link_id":"t3_kkdlr5","parent_id":"t3_kkdlr5","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh1yyt8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh23nyy":{"author":"TheMedernShairluck","author_fullname":"t2_5kp5wq5q","author_premium":false,"banned_at_utc":null,"body":"Thx! I'm having a look at it now.\n\nIt's kinda odd: it gives the top 100 results, but when I press 'more', gives 99% of the same top 100 results. I'll play around with it a bit and see if I can do better.","created_utc":["2020-12-26","10:12:20"],"id":"gh23nyy","link_id":"t3_kkdlr5","parent_id":"t1_gh1yyt8","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh23nyy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh3b4ey":{"author":"cyrilio","author_fullname":"t2_62dmi","author_premium":true,"banned_at_utc":null,"body":"This is a good question. Looks like that other comment has great link for it","created_utc":["2020-12-26","20:36:30"],"id":"gh3b4ey","link_id":"t3_kkdlr5","parent_id":"t3_kkdlr5","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh3b4ey\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kk8ge4":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"created_utc":["2020-12-26","01:47:22"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/","id":"kk8ge4","num_comments":7,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/","selftext":"I have a deleted account from back in the day that still shows everything in the pushshift but not the actual subreddits or anything. Can pushshift delete all that archived stuff as if it never existed on Reddit to begin with?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is it possible for pushshift to delete old posts and images from a deleted account?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/","comments":{"gh127pp":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"There's literally a thread pinned to the top of the subreddit explaining how to do this","created_utc":["2020-12-26","01:56:11"],"id":"gh127pp","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh127pp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh128tk":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"But it really works?","created_utc":["2020-12-26","01:56:32"],"id":"gh128tk","link_id":"t3_kk8ge4","parent_id":"t1_gh127pp","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh128tk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh12ezh":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Yes, though in fairness I've had a few drinks and didn't comprehend the part about it being a deleted account (even though it's literally right in the title, doh), so I may be an ass.  Not sure how they handle that.","created_utc":["2020-12-26","01:58:24"],"id":"gh12ezh","link_id":"t3_kk8ge4","parent_id":"t1_gh128tk","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh12ezh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh12ikx":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"The deleted account shows up in pushshift but it can\u2019t be viewed as if it were active. Doesn\u2019t show up in regular Reddit either","created_utc":["2020-12-26","01:59:28"],"id":"gh12ikx","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh12ikx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh18v5x":{"author":"_Xeet_","author_fullname":"t2_2yb5l98z","author_premium":false,"banned_at_utc":null,"body":"You need to send proof you owned the account but it does get removed if you send a request, read the pinned thread","created_utc":["2020-12-26","03:09:02"],"id":"gh18v5x","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh18v5x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh19reh":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"Sweet I didn\u2019t know this was possible to do.","created_utc":["2020-12-26","03:19:34"],"id":"gh19reh","link_id":"t3_kk8ge4","parent_id":"t1_gh18v5x","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh19reh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghflwhu":{"author":"Ok-Resident-5794","author_fullname":"t2_99i909oi","author_premium":false,"banned_at_utc":null,"body":"I will point out that this is only possible for it being searchable via the pushshift api. The comments are continually put into files which are then available for download to other people, who then share them with others. So by way of the comments actually being deleted? It's impossible and they will exist forever. But by way of making sure they can't be searched via this 1 API? Yea.\n\nExample: All the comments\/posts that have been saved are available via torrent. https:\/\/www.reddit.com\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/","created_utc":["2020-12-29","23:41:30"],"id":"ghflwhu","link_id":"t3_kk8ge4","parent_id":"t1_gh19reh","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/ghflwhu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kilscl":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"created_utc":["2020-12-23","06:10:18"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/","id":"kilscl","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/","selftext":"https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=created_utc&amp;frequency=hour&amp;size=0\n\nreturns:\n\n```{\n    \"data\": []\n}```","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"is aggs not working for everyone?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/","comments":{"ggrne63":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Aggs is disabled because usage of it was overwhelming the service.","created_utc":["2020-12-23","06:36:06"],"id":"ggrne63","link_id":"t3_kilscl","parent_id":"t3_kilscl","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggrne63\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggro4w4":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"Gotcha. \n\nIf you or anyone knows, will it be something that's offered in the new api? For money of course.","created_utc":["2020-12-23","06:44:06"],"id":"ggro4w4","link_id":"t3_kilscl","parent_id":"t1_ggrne63","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggro4w4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggro9ds":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"No real telling what will actually happen. The new api has been planned for literally two years now, but there's only progress when Stuck_In_the_Matrix has time.","created_utc":["2020-12-23","06:45:29"],"id":"ggro9ds","link_id":"t3_kilscl","parent_id":"t1_ggro4w4","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggro9ds\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggrookx":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"Hey u\/Stuck_in_the_Matrix, can I give you some money to use aggs for like just one hour?\n\nEdit: So I just read through the post from November and descided to rescind this request and give you some money anyway. Cheers, looking forward to seeing aggs back soon hopefully.","created_utc":["2020-12-23","06:50:02"],"id":"ggrookx","link_id":"t3_kilscl","parent_id":"t1_ggro9ds","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggrookx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kigfrx":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"created_utc":["2020-12-23","01:07:23"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/","id":"kigfrx","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/","selftext":"&amp;#x200B;\n\nHi everyone. I'm totally new to the API. Say I have a parent\\_id such as \"t1\\_d5z2b0c\" or \"t3\\_4vlbv9\". How would I use the API to get the body of the parent comment?\n\nAlso, what do the t1\\_ and t3\\_ prefixes signify? \n\nThanks!","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Noob Question: Getting Parent Body?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/","comments":{"ggqu82r":{"author":"git-commit-die","author_fullname":"t2_3rv8esy0","author_premium":true,"banned_at_utc":null,"body":"The t1_ and t3_ prefixes are [type prefixes](https:\/\/www.reddit.com\/dev\/api\/#fullnames), if a fullname *(e.g. the parent ID)* starts with `t1_` it's a comment and if it starts with `t3_` it's a link\/submission.\n\nSo if you have the parent ID t1_d5z2b0c, you could get the parent body by making a follow up request for only that ID *(d5z2b0c)*. You can also use the `filter` parameter to have it only return the body.\n\n`...\/reddit\/search\/comment\/?filter=body&amp;ids=d5z2b0c`\n\nTop-level comments will have a parent ID that corelates to a submission, but the process would pretty much be the same, just use the submission endpoint instead. Note that submission text bodies use `selftext` instead of `body`, and not all submissions have text bodies.\n\n`...\/reddit\/search\/submission\/?filter=selftext&amp;ids=4vlbv9`","created_utc":["2020-12-23","02:00:30"],"id":"ggqu82r","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggqu82r\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggtb5u4":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"The t prefixes are 'thing' types from the Reddit API. See the table titled 'type prefixes' here: https:\/\/www.reddit.com\/dev\/api\/\n\nYou can use and `ids` argument to specify the parent's id and `fields` to collect only body. You'll need to use the right endpoints for comments vs submissions, and adjust the `fields` for submissions (likely title + selftext)","created_utc":["2020-12-23","19:24:52"],"id":"ggtb5u4","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggtb5u4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggtj1vc":{"author":"PM_ME_UR_INSIGHTS","author_fullname":"t2_pk2aa","author_premium":false,"banned_at_utc":null,"body":"If you want a Dad Bod, just snack a lot and reduce your activity levels.","created_utc":["2020-12-23","20:33:01"],"id":"ggtj1vc","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggtj1vc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggu50ho":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"banned_at_utc":null,"body":"Hilarious!","created_utc":["2020-12-23","23:47:57"],"id":"ggu50ho","link_id":"t3_kigfrx","parent_id":"t1_ggtj1vc","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggu50ho\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggu50yb":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-12-23","23:48:04"],"id":"ggu50yb","link_id":"t3_kigfrx","parent_id":"t1_ggtb5u4","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggu50yb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kifh6s":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"created_utc":["2020-12-23","00:18:03"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kifh6s\/noob_question_getting_parent_or_child_comment\/","id":"kifh6s","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kifh6s\/noob_question_getting_parent_or_child_comment\/","selftext":"&amp;#x200B;\n\nHi everyone! Currently I have a list of comment identifiers that I'm interested in. I understand how to get the body of each comment using the API. However, I'm also interested in getting the immediate parent comment or child comment for every comment id in my list if such comments exist.\n\nHow would I approach this?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Noob Question: Getting Parent or Child Comment","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kifh6s\/noob_question_getting_parent_or_child_comment\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kg3cg5":{"author":"CaptainMelon","author_fullname":"t2_51smd","author_premium":false,"created_utc":["2020-12-19","09:13:11"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/","id":"kg3cg5","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/","selftext":"It would be very useful to be able to search on links posted, so for example if you manage [example.com](https:\/\/example.com), you can see what people say about you. For now you can only search the rendered text and not the links inside.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"[feature request] Allow to search on the links","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/","comments":{"ggcbmbw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you talking about searching link posts to your domain or just the domain included anywhere in a post? Pretty sure both of those are already possible.","created_utc":["2020-12-19","09:55:18"],"id":"ggcbmbw","link_id":"t3_kg3cg5","parent_id":"t3_kg3cg5","permalink":"\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/ggcbmbw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggcdhx1":{"author":"CaptainMelon","author_fullname":"t2_51smd","author_premium":false,"banned_at_utc":null,"body":"Both, searching for a link included in the comments, for now the api search on the text only. For example if I search for \"nosdeputes\" in the comments api, it doesn't return this comment linking to the website \"nosdeputes.fr\": [https:\/\/www.reddit.com\/r\/france\/comments\/keb25k\/fil\\_pour\\_demander\\_aux\\_modos\\_demp%C3%AAcher\\_la\/gg1movd\/](https:\/\/www.reddit.com\/r\/france\/comments\/keb25k\/fil_pour_demander_aux_modos_demp%C3%AAcher_la\/gg1movd\/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3)","created_utc":["2020-12-19","10:26:50"],"id":"ggcdhx1","link_id":"t3_kg3cg5","parent_id":"t1_ggcbmbw","permalink":"\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/ggcdhx1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kfp2fl":{"author":"Mpl0010","author_fullname":"t2_diryc","author_premium":false,"created_utc":["2020-12-18","19:07:12"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kfp2fl\/having_trouble_getting_data_today\/","id":"kfp2fl","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kfp2fl\/having_trouble_getting_data_today\/","selftext":"I've been playing around with Pushshift this week by pulling all of the comments of a specific thread. The past few days I was successful in pulling the data but starting yesterday, zero data will return with the endpoints I use.\n\n\n\nThis is an example of what I use: https:\/\/api.pushshift.io\/reddit\/comment\/search\/?fields=body,id&amp;link_id=kflmbi\n\nPreviously I was able to export a json file filled with all of the comments from thread. Now I'm not getting anything and I'm not sure what has changed. Any help would be awesome!\n\n\n\nEdit: It looks like it doesn't like any thread that was created within the last 5 or so hours.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Having trouble getting data today.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kfp2fl\/having_trouble_getting_data_today\/","comments":{"gga2tvu":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"The ingest often gets behind. Try again in a few hours, or consider the beta API: https:\/\/beta.pushshift.io\/redoc","created_utc":["2020-12-18","20:49:11"],"id":"gga2tvu","link_id":"t3_kfp2fl","parent_id":"t3_kfp2fl","permalink":"\/r\/pushshift\/comments\/kfp2fl\/having_trouble_getting_data_today\/gga2tvu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":1608313253.0,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kfa0gq":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"created_utc":["2020-12-18","02:24:56"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/","id":"kfa0gq","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/","selftext":"I've seemed to notice that when pulling from month to month you get more data then going from year to year. Has anyone else experienced this? Does going from day to day get more data?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Data Pulling","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/","comments":{"gg7dhp0":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Maybe it's just that recent months have seen bigger relative increases, then the relative increases of recent years.","created_utc":["2020-12-18","03:31:02"],"id":"gg7dhp0","link_id":"t3_kfa0gq","parent_id":"t3_kfa0gq","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gg7dhp0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg91711":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":true,"banned_at_utc":null,"body":"What\u2019s ur filter? If u pull 2019 they year, then you pull 1\/2019-12\/2019 separately and add them up, you get different post count?","created_utc":["2020-12-18","15:56:29"],"id":"gg91711","link_id":"t3_kfa0gq","parent_id":"t3_kfa0gq","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gg91711\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gge6x1b":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"yes exactly!","created_utc":["2020-12-19","22:00:58"],"id":"gge6x1b","link_id":"t3_kfa0gq","parent_id":"t1_gg91711","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gge6x1b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gge74zz":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"Ive been doing research for a data analytics company and have seen this happen in several groups and topics","created_utc":["2020-12-19","22:02:54"],"id":"gge74zz","link_id":"t3_kfa0gq","parent_id":"t1_gg7dhp0","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gge74zz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kdjie1":{"author":"meager_narration","author_fullname":"t2_8yakpzil","author_premium":false,"created_utc":["2020-12-15","12:42:16"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kdjie1\/how_to_helpget_involved_with_pushshift_project\/","id":"kdjie1","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kdjie1\/how_to_helpget_involved_with_pushshift_project\/","selftext":"Besides making donations, how can I help with the development of the Pushshift project?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How to help\/get involved with Pushshift project?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kdjie1\/how_to_helpget_involved_with_pushshift_project\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kddn9l":{"author":"fortniteBot3000","author_fullname":"t2_6861tb8e","author_premium":false,"created_utc":["2020-12-15","05:21:57"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/","id":"kddn9l","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/","selftext":"Everytime that I change a new reddit link and replace the r with a c, it just gives me an error and tells me the site can't be reached.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is there something wrong with Ceddit?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/","comments":{"gfw2s3y":{"author":"KKingler","author_fullname":"t2_ngpoa","author_premium":true,"banned_at_utc":null,"body":"Try removeddit","created_utc":["2020-12-15","06:34:33"],"id":"gfw2s3y","link_id":"t3_kddn9l","parent_id":"t3_kddn9l","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfw2s3y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfw6v1p":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Looks like the site is down: https:\/\/snew.notabug.io\/","created_utc":["2020-12-15","07:16:22"],"id":"gfw6v1p","link_id":"t3_kddn9l","parent_id":"t3_kddn9l","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfw6v1p\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfz4mp6":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-12-16","01:28:59"],"id":"gfz4mp6","link_id":"t3_kddn9l","parent_id":"t1_gfw2s3y","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfz4mp6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"go8omq8":{"author":"AwesomeTheKid","author_fullname":"t2_kt1du","author_premium":false,"banned_at_utc":null,"body":"Works. Thanks.","created_utc":["2021-02-21","18:52:02"],"id":"go8omq8","link_id":"t3_kddn9l","parent_id":"t1_gfw2s3y","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/go8omq8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kdd06y":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"created_utc":["2020-12-15","04:43:33"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kdd06y\/does_pushshiftpsaw_ever_add_things_out_of_order\/","id":"kdd06y","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kdd06y\/does_pushshiftpsaw_ever_add_things_out_of_order\/","selftext":"I have a program that effectively tries to make a dump of everything you've done with your Reddit account, and it uses PSAW to do this. I'm thinking of updating the program to be able to only look for comments that are made after the latest comment in a previous dump, to make it much faster to run. However, the issue that may be here is comments\/posts that were made before that latest comment, but were not added to PSAW after the dump (i.e. if Pushshift was temporarily down and the missed comments were added later).\n\nDoes anyone know if this happens, i.e. SITM will have the program ingest comments it missed during a downtime?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Does Pushshift\/PSAW ever add things out of order (i.e. backfill missing comments\/posts)?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kdd06y\/does_pushshiftpsaw_ever_add_things_out_of_order\/","comments":{"gfvzxom":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"This does happen, but only very occasionally.","created_utc":["2020-12-15","06:08:34"],"id":"gfvzxom","link_id":"t3_kdd06y","parent_id":"t3_kdd06y","permalink":"\/r\/pushshift\/comments\/kdd06y\/does_pushshiftpsaw_ever_add_things_out_of_order\/gfvzxom\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kbmezv":{"author":"RyanRowley007","author_fullname":"t2_7jxjdlzq","author_premium":false,"created_utc":["2020-12-12","11:35:54"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/","id":"kbmezv","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/","selftext":"Does anyone know if someone has previously written a script that saves user data locally (in any format JSON, csv, etc)?\n\nThanks","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Data download for user","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/","comments":{"gfkp2u4":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py","created_utc":["2020-12-12","21:47:59"],"id":"gfkp2u4","link_id":"t3_kbmezv","parent_id":"t3_kbmezv","permalink":"\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/gfkp2u4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfmxkhx":{"author":"RyanRowley007","author_fullname":"t2_7jxjdlzq","author_premium":false,"banned_at_utc":null,"body":"you legend, thank you","created_utc":["2020-12-13","05:35:49"],"id":"gfmxkhx","link_id":"t3_kbmezv","parent_id":"t1_gfkp2u4","permalink":"\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/gfmxkhx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kbap4d":{"author":"mattster42","author_fullname":"t2_7mdlw","author_premium":true,"created_utc":["2020-12-11","22:31:56"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/","id":"kbap4d","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/","selftext":"I'm using PSAW with the purpose of getting all posts and comments from a specific author in a specific subreddit up to a specific date, iterating through a list of authors. In order to capture the most current karma scores and metadata, I'm only using PSAW to get the post\/comment IDs.\n\nFor each author, I make two queries, one for posts and one for comments. Here they are:\n\n`authorList = list(api.search_submissions(subreddit=sub, filter=['id'], author=authorName, before=endStamp, sort='asc'))`\n\n`authorCommentList = list(api.search_comments(subreddit=sub, filter=['id'], author=authorName, before=endStamp, sort='asc'))`\n\nThe endStamp is February 19, 2019. When running the script, it looks like each query is taking roughly seven to ten seconds to return a response, even if the results are small (say, four posts and thirteen comments). I know PSAW is supposed to be faster than this, so I want to make sure I'm not structuring these queries in an inefficient manner that's unfairly taxing this resource.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Is this query inefficient\/resource taxing?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/","comments":{"gffzuad":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It's not you, it's just slow. Here's my query time graph for a really simple diagnostic query over the last 7 days in seconds the api took to respond\n\nhttps:\/\/i.imgur.com\/VvSAiql.png\n\nSome days it's faster but a lot of time it's just overwhelmed in general.","created_utc":["2020-12-11","23:08:58"],"id":"gffzuad","link_id":"t3_kbap4d","parent_id":"t3_kbap4d","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gffzuad\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg202m":{"author":"mattster42","author_fullname":"t2_7mdlw","author_premium":true,"banned_at_utc":null,"body":"Thanks, that's good to know. And a good reminder to donate.","created_utc":["2020-12-11","23:25:31"],"id":"gfg202m","link_id":"t3_kbap4d","parent_id":"t1_gffzuad","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfg202m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfvdby8":{"author":"transfo47","author_fullname":"t2_obf65","author_premium":false,"banned_at_utc":null,"body":"&gt;but a lot of time it's just overwhelmed in general.\n\nI feel that.","created_utc":["2020-12-15","02:46:08"],"id":"gfvdby8","link_id":"t3_kbap4d","parent_id":"t1_gffzuad","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfvdby8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfvdoaj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Oddly it's actually been better the last few days\n\nhttps:\/\/i.imgur.com\/K1zMmvF.png\n\nso maybe you just have to struggle along until life gets better :)","created_utc":["2020-12-15","02:49:20"],"id":"gfvdoaj","link_id":"t3_kbap4d","parent_id":"t1_gfvdby8","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfvdoaj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg756mh":{"author":"mattster42","author_fullname":"t2_7mdlw","author_premium":true,"banned_at_utc":null,"body":"I noticed it seemed to speed up those days, too. But it's been slow again the past day or so. Does your graph reflect this?","created_utc":["2020-12-18","02:15:08"],"id":"gg756mh","link_id":"t3_kbap4d","parent_id":"t1_gfvdoaj","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gg756mh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg785y0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Yeah, it's not as bad as it was a week ago, but it's definitely worse\n\nhttps:\/\/i.imgur.com\/NZbdyy3.png","created_utc":["2020-12-18","02:40:38"],"id":"gg785y0","link_id":"t3_kbap4d","parent_id":"t1_gg756mh","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gg785y0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kb9930":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"created_utc":["2020-12-11","21:18:03"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/","id":"kb9930","num_comments":17,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Caching the last download","url":"\/r\/learnpython\/comments\/kb3de8\/caching_the_last_download\/","comments":{"gffkekw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I think I would need a lot more info about what the bot is doing to make a recommendation. Just starting a day ago rather than at a certain date is easy.\n\n    from datetime import datetime, timedelta\n\n    one_day_ago = datetime.utcnow() - timedelta(days=1)\n\nBut saving a whole bunch of data so it can be reused when the script restarts depends a lot more on what exactly you're doing.","created_utc":["2020-12-11","21:46:26"],"id":"gffkekw","link_id":"t3_kb9930","parent_id":"t3_kb9930","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffkekw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffkm7b":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"I'll share my script in a PasteBin link if if would help.\n\nhttps:\/\/pastebin.com\/hcKDm2gM","created_utc":["2020-12-11","21:47:32"],"id":"gffkm7b","link_id":"t3_kb9930","parent_id":"t1_gffkekw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffkm7b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffz2mj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Unless I'm completely misremembering something, doing a `search_submissions` like that starts at the current date and goes backwards.\n\nAre you wanting to run that until it crashes, lets say it starts right now and gets back to a week ago, then when you start it again it starts at a week ago and keeps going? Or do you run it like once a day and want start at the current time and go back till yesterday when you hit posts you've already downloaded?\n\nAlso, what part is crashing? It might be simpler to figure out how to stop it from crashing than restarting it every time it does.","created_utc":["2020-12-11","23:04:55"],"id":"gffz2mj","link_id":"t3_kb9930","parent_id":"t1_gffkm7b","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffz2mj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffzojc":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"I think when it crashes, it's something to do with requests. It varies, so I can't really narrow it down. The most common thing is running out of attempts to get a page. What I want to be able to do is to set it running, and it will trawl through the histories of the specified subreddits, downloading images until it hits the beginning. Once it has hit the beginning for all subs, start tracking any new submissions.","created_utc":["2020-12-11","23:08:05"],"id":"gffzojc","link_id":"t3_kb9930","parent_id":"t1_gffz2mj","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffzojc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg20rk":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"But is it crashing trying to access the pushshift api, the reddit api or when it downloads the pictures? How many pictures on average does it download before crashing? And do you have any idea how many you're expecting if it did manage to download the entire history?\n\nIt's possible this is reddit telling you that you're downloading too many things too fast and you should slow down. In which case restarting it each time is just a stop gap and won't end up helping that much.\n\nCould you post the whole error message?","created_utc":["2020-12-11","23:25:40"],"id":"gfg20rk","link_id":"t3_kb9930","parent_id":"t1_gffzojc","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg20rk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg2f4d":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"If it actually manages to get the whole history, there will be several thousand images. Probably more than 20k. I have tried putting in a 10-second gap between each image download, but it still fails eventually. I think most of the time it gets around 2000 images but I did manage to get 15k once.","created_utc":["2020-12-11","23:29:04"],"id":"gfg2f4d","link_id":"t3_kb9930","parent_id":"t1_gfg20rk","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg2f4d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg57iw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Could you try it again and post the full error message when it crashes?","created_utc":["2020-12-11","23:52:44"],"id":"gfg57iw","link_id":"t3_kb9930","parent_id":"t1_gfg2f4d","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg57iw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfjc0kq":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"Here is the full error message:  \n[https:\/\/pastebin.com\/n3Xg5kvr](https:\/\/pastebin.com\/n3Xg5kvr)","created_utc":["2020-12-12","17:17:49"],"id":"gfjc0kq","link_id":"t3_kb9930","parent_id":"t1_gfg57iw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfjc0kq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm4vko":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The first one is an error downloading the image. You can fix that by putting a try\/except around the `r = requests.get(url)` line. The second is a bug in PSAW that was fixed about 6 months ago, so you need to update PSAW to the latest version.","created_utc":["2020-12-13","02:19:50"],"id":"gfm4vko","link_id":"t3_kb9930","parent_id":"t1_gfjc0kq","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm4vko\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm62ce":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"The latest version appears to be 0.0.12, and that's the version I have. I've not had PSAW for more than 6 months.","created_utc":["2020-12-13","02:25:36"],"id":"gfm62ce","link_id":"t3_kb9930","parent_id":"t1_gfm4vko","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm62ce\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm7j3o":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Huh, you're right. It looks like this fix was never released as a new version. Which is unfortunately since it's literally in the part of the code that catches pushshift errors and retries instead of just crashing.\n\nYou could edit your PSAW file manually, [here's the commit](https:\/\/github.com\/dmarx\/psaw\/commit\/b61ec6f73e5566525a9b32e466843fd11817556d) with the fix. So you would need to find where that code is stored and just make that change yourself.\n\nOr you could uninstall PSAW and install it directly from github, like this `pip install git+https:\/\/github.com\/dmarx\/psaw`.\n\nYou might also be able to just try again. Pushshift has been having lots of stability issues the last week, but for some reason it's much, much more stable today. If you fix that first issue by putting the try\/except around the image download it might just work.","created_utc":["2020-12-13","02:33:04"],"id":"gfm7j3o","link_id":"t3_kb9930","parent_id":"t1_gfm62ce","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm7j3o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfmal04":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"Ah, OK. I've made those changes and re-run the code, so I'll see if anything messes up or whether it's all good now. Thanks for the help.","created_utc":["2020-12-13","02:49:59"],"id":"gfmal04","link_id":"t3_kb9930","parent_id":"t1_gfm7j3o","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfmal04\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfp3ibw":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"If I put the `r = requests.get(url)` line inside the `try\/except` block, it tells me that `r` might have been referenced before assignment. Wouldn't I have to put a bunch of other stuff inside the `try\/except` block as well?","created_utc":["2020-12-13","16:09:13"],"id":"gfp3ibw","link_id":"t3_kb9930","parent_id":"t1_gfm7j3o","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfp3ibw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfq92lc":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You need to `return` in the `except` part. If it can't download the image, you need to skip it rather than keep going in the function.","created_utc":["2020-12-13","21:13:03"],"id":"gfq92lc","link_id":"t3_kb9930","parent_id":"t1_gfp3ibw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfq92lc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfqa4zg":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"And if I do that, it will work properly?","created_utc":["2020-12-13","21:18:52"],"id":"gfqa4zg","link_id":"t3_kb9930","parent_id":"t1_gfq92lc","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfqa4zg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfqae0f":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I don't know, try it and see","created_utc":["2020-12-13","21:20:12"],"id":"gfqae0f","link_id":"t3_kb9930","parent_id":"t1_gfqa4zg","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfqae0f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfswjit":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"It kind of works, but the files seem to stop downloading at around 100-200 and I'm not sure why","created_utc":["2020-12-14","13:34:37"],"id":"gfswjit","link_id":"t3_kb9930","parent_id":"t1_gfqae0f","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfswjit\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":"\/r\/learnpython\/comments\/kb3de8\/caching_the_last_download\/","crosspost_parent":"t3_kb3de8","crosspost_parent_list":[{"all_awardings":[],"allow_live_comments":false,"approved_at_utc":null,"approved_by":null,"archived":false,"author":"Arag0ld","author_flair_background_color":null,"author_flair_css_class":null,"author_flair_richtext":[],"author_flair_template_id":null,"author_flair_text":null,"author_flair_text_color":null,"author_flair_type":"text","author_fullname":"t2_2z32g6fu","author_patreon_flair":false,"author_premium":false,"awarders":[],"banned_at_utc":null,"banned_by":null,"can_gild":true,"can_mod_post":false,"category":null,"clicked":false,"content_categories":null,"contest_mode":false,"created":1607724281.0,"created_utc":1607695481.0,"discussion_type":null,"distinguished":null,"domain":"self.learnpython","downs":0,"edited":false,"gilded":0,"gildings":{},"hidden":false,"hide_score":false,"id":"kb3de8","is_crosspostable":true,"is_meta":false,"is_original_content":false,"is_reddit_media_domain":false,"is_robot_indexable":true,"is_self":true,"is_video":false,"likes":null,"link_flair_background_color":"","link_flair_css_class":null,"link_flair_richtext":[],"link_flair_text":null,"link_flair_text_color":"dark","link_flair_type":"text","locked":false,"media":null,"media_embed":{},"media_only":false,"mod_note":null,"mod_reason_by":null,"mod_reason_title":null,"mod_reports":[],"name":"t3_kb3de8","no_follow":true,"num_comments":4,"num_crossposts":1,"num_reports":null,"over_18":false,"parent_whitelist_status":"all_ads","permalink":"\/r\/learnpython\/comments\/kb3de8\/caching_the_last_download\/","pinned":false,"pwls":6,"quarantine":false,"removal_reason":null,"removed_by":null,"removed_by_category":null,"report_reasons":null,"saved":false,"score":1,"secure_media":null,"secure_media_embed":{},"selftext":"I made a post in r\/redditdev about a bot that I made that kept stopping. This post can be found at the following link, since crossposting isn't allowed here:\n\n[https:\/\/www.reddit.com\/r\/redditdev\/comments\/k9r4rr\/caching\\_the\\_last\\_download\/](https:\/\/www.reddit.com\/r\/redditdev\/comments\/k9r4rr\/caching_the_last_download\/)\n\nI read into using the `datetime` module, but it seems to me that I can only hard-code dates, as opposed to making them dynamic. What I want to do is \"cache\" the downloads from the previous day in such a way that if the script terminates, I only have to re-scrape a day's worth of data, rather than starting over like I currently do.","selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\"md\"&gt;&lt;p&gt;I made a post in &lt;a href=\"\/r\/redditdev\"&gt;r\/redditdev&lt;\/a&gt; about a bot that I made that kept stopping. This post can be found at the following link, since crossposting isn&amp;#39;t allowed here:&lt;\/p&gt;\n\n&lt;p&gt;&lt;a href=\"https:\/\/www.reddit.com\/r\/redditdev\/comments\/k9r4rr\/caching_the_last_download\/\"&gt;https:\/\/www.reddit.com\/r\/redditdev\/comments\/k9r4rr\/caching_the_last_download\/&lt;\/a&gt;&lt;\/p&gt;\n\n&lt;p&gt;I read into using the &lt;code&gt;datetime&lt;\/code&gt; module, but it seems to me that I can only hard-code dates, as opposed to making them dynamic. What I want to do is &amp;quot;cache&amp;quot; the downloads from the previous day in such a way that if the script terminates, I only have to re-scrape a day&amp;#39;s worth of data, rather than starting over like I currently do.&lt;\/p&gt;\n&lt;\/div&gt;&lt;!-- SC_ON --&gt;","send_replies":true,"spoiler":false,"stickied":false,"subreddit":"learnpython","subreddit_id":"t5_2r8ot","subreddit_name_prefixed":"r\/learnpython","subreddit_subscribers":460696,"subreddit_type":"public","suggested_sort":null,"thumbnail":"self","thumbnail_height":null,"thumbnail_width":null,"title":"Caching the last download","top_awarded_type":null,"total_awards_received":0,"treatment_tags":[],"ups":1,"upvote_ratio":1.0,"url":"https:\/\/www.reddit.com\/r\/learnpython\/comments\/kb3de8\/caching_the_last_download\/","user_reports":[],"view_count":null,"visited":false,"whitelist_status":"all_ads","wls":6}],"author_cakeday":null},"kb6lhm":{"author":"real_jabb0","author_fullname":"t2_4cyqprt4","author_premium":false,"created_utc":["2020-12-11","19:00:33"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kb6lhm\/question_about_data_consistency\/","id":"kb6lhm","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kb6lhm\/question_about_data_consistency\/","selftext":"Hello,\n\nI have ingested all submissions from the PS files between 05-2019 and 04-2020 (inclusive) for the \/r\/worldnews subreddit. The objects are updated using the reddit API to their current version.  \n\n\nI have used the \"removed\\_by\\_category\" to check if a submission has been removed. The goal is to remove Spam from the submissions.  I noticed that the amount of submissions removed changes around december of 2019 (see picture below). \n\nAre there any technical reasons for that you can think of? Otherwise I would have guessed that the sub is now more moderated or the guidelines changed. \n\nI think it is unlikely that users just stopped posting relevant submissions and started to create more spam. This can be seen by the number of submissions with a score &gt; 10 stays the same (see picture 2).\n\nThank you :)\n\n&amp;#x200B;\n\nhttps:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=687beaa93d9c2534653ed418c22415826e2e35c1\n\n&amp;#x200B;\n\nhttps:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=5e99771e603b6bf45a367173aa938a6355826172","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Question about data consistency","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kb6lhm\/question_about_data_consistency\/","comments":{"gffodn9":{"author":"TheElectricSlide2","author_fullname":"t2_1bwcu6ye","author_premium":false,"banned_at_utc":null,"body":"New mods?","created_utc":["2020-12-11","22:07:05"],"id":"gffodn9","link_id":"t3_kb6lhm","parent_id":"t3_kb6lhm","permalink":"\/r\/pushshift\/comments\/kb6lhm\/question_about_data_consistency\/gffodn9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":{"0pnbdxdq9l461":{"e":"Image","id":"0pnbdxdq9l461","m":"image\/jpg","p":[{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b8e0d836dca171e28ec4e0dd7cbe4bf1a49b8e94","x":108,"y":21},{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0af3a36a9c7392a42502d0a562dbf3b2b905889a","x":216,"y":43},{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9fd90582cf3e4908a711ac93b440d1e052c6bd41","x":320,"y":63},{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=46b14ec38df7a5d4782097067babc15c26539fb4","x":640,"y":127},{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7ba94958e64e82569033a2445b67d7cf4dbc2272","x":960,"y":191},{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=5396eb30908b1c80d4469539b9d2aa7b5e430618","x":1080,"y":215}],"s":{"u":"https:\/\/preview.redd.it\/0pnbdxdq9l461.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=5e99771e603b6bf45a367173aa938a6355826172","x":1600,"y":319},"status":"valid"},"i4k0s75n9l461":{"e":"Image","id":"i4k0s75n9l461","m":"image\/jpg","p":[{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2bb64cdc49da90ee729ad7e42fac499994a94fb","x":108,"y":40},{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee55ef18ba221d195195d305136f695cc3b8e61b","x":216,"y":81},{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=22272bd7275459458fad2d5ec4010fe2f16503ec","x":320,"y":121},{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=2265b62b69dbe4c62b6cfff03307eda431a907a5","x":640,"y":242},{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a3a2cbdff7c252a41aae5dc605a5416b02e1c5d6","x":960,"y":363},{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c0d856312b9c496046c758439294c0a4673b5344","x":1080,"y":409}],"s":{"u":"https:\/\/preview.redd.it\/i4k0s75n9l461.jpg?width=1600&amp;format=pjpg&amp;auto=webp&amp;s=687beaa93d9c2534653ed418c22415826e2e35c1","x":1600,"y":606},"status":"valid"}},"thumbnail_height":53.0,"thumbnail_width":140.0,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":true},"kb217n":{"author":"Apprehensive_Ad_5527","author_fullname":"t2_7hwr36hn","author_premium":false,"created_utc":["2020-12-11","14:41:08"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/","id":"kb217n","num_comments":9,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/","selftext":"Dear everyone,\n\nI am a researcher working on certain online communities, which include reddit. I saw in research papers that Pushshift was being used, and from everything I read, it would seem extremely useful to me.  (I read the FAQ here and Baumgartner et al. paper \"The Pushshift Reddit Dataset\".) \n\nThing is, I am too bad at IT to understand how to actually access this data. For example, I can't get any result on the [redditsearch.io](https:\/\/redditsearch.io) website...\n\nMy goal is to locate the archives of old subreddits that have been banned in order to study their content. I would also really like using the functions that allow for advanced searches or compiling graphs. I am not looking for metadata or quantitative analysis, I just want to read through old posts for careful qualitative analysis. \n\nSo my two questions:\n\nCould anyone direct me to a tutorial for beginners?\n\nIs data from the currently banned or quarantined subreddits still available via Pushshift?\n\nI would be eternally grateful if you could help ;)","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Too bad at IT to understand any of this: please help a researcher in distress","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/","comments":{"gfdzlkb":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Yes, banned and quarantined data is available on Pushshift.\n\nThe easiest thing for you to do is get the data from  Google BigQuery. The data dumps on BigQuery stop at 2019, however. It\u2019s going to still require you to use SQL commands: there\u2019s no getting around at least basic coding. You will need to look up SQL queries for your specific use case.\n\nThe Pushshift data are free and public on BigQuery. You\u2019ll need to register and create a project in the Google developer console. Once you\u2019ve done that, you can query the public data dumps.\n\nI recommend searching Stack Overflow for how to do this.","created_utc":["2020-12-11","15:11:03"],"id":"gfdzlkb","link_id":"t3_kb217n","parent_id":"t3_kb217n","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfdzlkb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfe7wyq":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"As someone who isn't familiar at all with BigQuery (but is familiar with the Pushshift API and PSAW): what is the benefit to using BigQuery? Is accessing the Pushshift database easier if you use it?","created_utc":["2020-12-11","16:37:26"],"id":"gfe7wyq","link_id":"t3_kb217n","parent_id":"t1_gfdzlkb","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfe7wyq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfenp64":{"author":"real_jabb0","author_fullname":"t2_4cyqprt4","author_premium":false,"banned_at_utc":null,"body":"I would guess that the query complexity one can achieve in BigQuery is much higher than the Pushshift API (which is backed by elasticsearch?). Also it might have a higher availability. I assume the downside is the data actuality as the most recent data is not in there(?).","created_utc":["2020-12-11","18:51:56"],"id":"gfenp64","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfenp64\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":true},"gfeptrc":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"In my use cases, the chief benefit of the Google BigQuery dumps has been efficiently extracting full archives of subreddits without having to make expensive and slow API calls.\n\nOnce I have archived data from BigQuery, I augment it with \u201clive\u201d data from Pushshift\u2019s API (using PSAW).\n\nI also use BigQuery\u2019s computational power to create very large network edgelists, which is computationally expensive.","created_utc":["2020-12-11","19:06:08"],"id":"gfeptrc","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfeptrc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gff11h3":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"How does BigQuery get it's data? Do you fetch it from Reddit? Does it work with a downloaded Pushshift dump? Or is it somehow able to access the Pushshift API in a faster way?","created_utc":["2020-12-11","20:04:46"],"id":"gff11h3","link_id":"t3_kb217n","parent_id":"t1_gfeptrc","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gff11h3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffgd7l":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I'm pretty sure Stuck_In_the_Matrix uploads it there himself. I'm not sure whether it's live or out of date right now though.","created_utc":["2020-12-11","21:25:09"],"id":"gffgd7l","link_id":"t3_kb217n","parent_id":"t1_gff11h3","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffgd7l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffirst":{"author":"MFA_Nay","author_fullname":"t2_rmmen","author_premium":false,"banned_at_utc":null,"body":"For people without coding backgrounds the web GUI is nice, the SQL-like programming language is well documented and if you're dealing with large amounts of Reddit data using Google's cloud severs can work out quicker then using you local device(s).","created_utc":["2020-12-11","21:37:47"],"id":"gffirst","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffirst\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffj7i2":{"author":"MFA_Nay","author_fullname":"t2_rmmen","author_premium":false,"banned_at_utc":null,"body":"Bit hazy on exact details but Felipe Hoffa (then working for Google) uploaded it with Baumgartner's consent in 2015. Around 2018 Pushshift's livestream was integrated into BigQuery's Pushshift project.","created_utc":["2020-12-11","21:40:03"],"id":"gffj7i2","link_id":"t3_kb217n","parent_id":"t1_gffgd7l","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffj7i2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfi11m7":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-12-12","09:45:49"],"id":"gfi11m7","link_id":"t3_kb217n","parent_id":"t3_kb217n","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfi11m7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"kanjiv":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"created_utc":["2020-12-10","22:34:11"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/","id":"kanjiv","num_comments":11,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"What's the best way to handle a JSON parse when the 'selftext' field includes double quotes? Does anyone have a good regex for this?","url":"https:\/\/i.redd.it\/fikhi73v6f461.png","comments":{"gfbjbwj":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Note that this doesn't only occur between characters, e.g. `5\"9` or `wasn\"t` here, but also at the beginning and end of strings, e.g. `he said \"this\" and I` . Shouldn't these be escaped on the API side of things?","created_utc":["2020-12-10","22:36:55"],"id":"gfbjbwj","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbjbwj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbnxqv":{"author":"TheElectricSlide2","author_fullname":"t2_1bwcu6ye","author_premium":false,"banned_at_utc":null,"body":"r","created_utc":["2020-12-10","23:14:32"],"id":"gfbnxqv","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbnxqv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbr1hx":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"Strange, that looks like `5'9`  when viewed [via browser](http:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=2r2mx5).","created_utc":["2020-12-10","23:39:57"],"id":"gfbr1hx","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbr1hx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbucqs":{"author":"meager_narration","author_fullname":"t2_8yakpzil","author_premium":false,"banned_at_utc":null,"body":"How are you reading it? In general I use pandas in Python and I don't have issues with it.","created_utc":["2020-12-11","00:48:54"],"id":"gfbucqs","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbucqs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfc5bvl":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Yeah! I noticed that when searching by id too! It seems that adding `fields=selftext,id` to the query helped, though I'm not sure why","created_utc":["2020-12-11","02:09:00"],"id":"gfc5bvl","link_id":"t3_kanjiv","parent_id":"t1_gfbr1hx","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfc5bvl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfc9nya":{"author":"ToThePastMe","author_fullname":"t2_yi8qj","author_premium":false,"banned_at_utc":null,"body":"Just curious: Is it through the API or using Reddit Submission \/ Comments Dump file? Because I never ran into that issue after parsing over 10 full dumps with python+pandas (someone else mentioned that too), meaning millions of submission records. And how did you read that file \/ process the API result? Is it possible that the tool you used unescaped some characters or replaced ' by \"?","created_utc":["2020-12-11","02:40:45"],"id":"gfc9nya","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfc9nya\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfckegt":{"author":"noroom","author_fullname":"t2_2tdxy","author_premium":false,"banned_at_utc":null,"body":"They are when they need to be.\n\nExample: https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=k4owfz\n\nMeanwhile, you showed something that appears to have replaced the single quote with a double quote.\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=2r2mx5\n\nThis is something on your end being weird.","created_utc":["2020-12-11","04:15:26"],"id":"gfckegt","link_id":"t3_kanjiv","parent_id":"t1_gfbjbwj","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfckegt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfdwu1g":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Great questions, thanks - I'm realizing that this is probably an issue on my end. I'm using the API with the python requests and json libraries:\n\nresponse = requests.get(URL).json()\nfor post_obj in response['data']:\n   post_text = post_obj['selftext']\n\nIt appears to also be breaking on emoticons, e.g. :) :(","created_utc":["2020-12-11","14:35:53"],"id":"gfdwu1g","link_id":"t3_kanjiv","parent_id":"t1_gfc9nya","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfdwu1g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfdy05p":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Flairing this as **spoiler**, the spoiler being that it's a code issue, not an API issue","created_utc":["2020-12-11","14:51:11"],"id":"gfdy05p","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfdy05p\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg0pvo":{"author":"ToThePastMe","author_fullname":"t2_yi8qj","author_premium":false,"banned_at_utc":null,"body":"Hmmm I'm curious about it, I'll also check quick next week when back home. Never really had issues with the requests package but then I don't use it very frequently.","created_utc":["2020-12-11","23:14:42"],"id":"gfg0pvo","link_id":"t3_kanjiv","parent_id":"t1_gfdwu1g","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfg0pvo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg19zx":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"I switched to using [psaw](https:\/\/github.com\/dmarx\/psaw) and haven't had any issues there. Thanks for your help!","created_utc":["2020-12-11","23:19:24"],"id":"gfg19zx","link_id":"t3_kanjiv","parent_id":"t1_gfg0pvo","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfg19zx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"image","preview":{"enabled":true,"images":[{"id":"vR9YXIqSrRF58_tjsV-dteVDFP8Dr18JoCamj63d9Dk","resolutions":[{"height":45,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b971965c2c90f03c0d04c0ea7179483d3e9808b4","width":108},{"height":91,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3d2fe0ffe909895ab4c2714818483a74328076e5","width":216},{"height":135,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dde48f714ed0fc5bbfdd09d7dab84dad58a553ff","width":320},{"height":270,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a3016ae150599a4a87c86ac4451b79811afad965","width":640},{"height":405,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81a97502385a1ad7bc012dac22ceea2ffd360a84","width":960},{"height":456,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86f2691695a34538b08d34fa2a855abefef3a365","width":1080}],"source":{"height":590,"url":"https:\/\/preview.redd.it\/fikhi73v6f461.png?auto=webp&amp;s=1c7781c44795ca7f1045bb781f4e9772ee9df6cc","width":1396},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":59.0,"thumbnail_width":140.0,"url_overridden_by_dest":"https:\/\/i.redd.it\/fikhi73v6f461.png","crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"ka9j31":{"author":"NateNate60","author_fullname":"t2_bptkkt","author_premium":false,"created_utc":["2020-12-10","07:36:49"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ka9j31\/cryptocurrency_donations\/","id":"ka9j31","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/ka9j31\/cryptocurrency_donations\/","selftext":"I'd like to donate using cryptocurrency because it's more convenient for me and because I like that with cryptocurrency, the recipient gets basically all of the money sent instead of a portion of it getting eaten by fees. Besides, why *not* accept Internet money for an Internet project?\n\nProbably not that hard to integrate a BitPay or Coingate button into the website for cryptocurrency donations.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Cryptocurrency donations?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/ka9j31\/cryptocurrency_donations\/","comments":{"gfananv":{"author":"Xenc","author_fullname":"t2_4v98f","author_premium":true,"banned_at_utc":null,"body":"You could also put a couple of wallet addresses on there, say for Ethereum and Bitcoin.","created_utc":["2020-12-10","18:27:58"],"id":"gfananv","link_id":"t3_ka9j31","parent_id":"t3_ka9j31","permalink":"\/r\/pushshift\/comments\/ka9j31\/cryptocurrency_donations\/gfananv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k9b72d":{"author":"era_explorer","author_fullname":"t2_94alw8op","author_premium":false,"created_utc":["2020-12-08","21:34:37"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k9b72d\/how_can_i_extract_new_posts_and_comments_added_to\/","id":"k9b72d","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k9b72d\/how_can_i_extract_new_posts_and_comments_added_to\/","selftext":"I wanted to extract any recent activities happened in a particular subreddit in last x days or x hours,Activities involves, any new post or new comments to the older post .I understand that we can get it using after, before and subreddit parameters in psaw but will that recent both the above activities or only the new post ?And I can get only 100 post, what if there is more post happened in given time window ? How to extract all the activities in given time without 100 limitation.\n\nThanks in advance, I am pretty new to reddit and pushshift package as well.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How can I extract new posts and comments added to the older post on a subreddit in last 24h or last week ?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k9b72d\/how_can_i_extract_new_posts_and_comments_added_to\/","comments":{"ghtlrqv":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"- Get current UTC timestamp and initialize it for a loop\n- Make requests to different urls in chunks of 100 posts with the \"before\" http paramater getting the 100 most recent posts before that time \n- Save the timestamp of the post for the last post in the chunk to use in your next request \n- Break when the postTimestamp &lt; currentTimestamp - (24 * 60 * 60)","created_utc":["2021-01-02","14:34:06"],"id":"ghtlrqv","link_id":"t3_k9b72d","parent_id":"t3_k9b72d","permalink":"\/r\/pushshift\/comments\/k9b72d\/how_can_i_extract_new_posts_and_comments_added_to\/ghtlrqv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k9avo4":{"author":"era_explorer","author_fullname":"t2_94alw8op","author_premium":false,"created_utc":["2020-12-08","21:19:52"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k9avo4\/how_do_i_extract_post_comments_to_the_existing\/","id":"k9avo4","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k9avo4\/how_do_i_extract_post_comments_to_the_existing\/","selftext":"I wanted to extract any recent activities happened in a particular subreddit in last x days or x hours,  \nActivities involves, any new post or new comments to the older post .  \nI understand that we can get it using after, before and subreddit parameters in psaw but will that recent both the above activities or only the new post ?  \nAnd I can get only 100 post, what if there is more post happened in given time window ? How to extract all the activities in given time without 100 limitation.  \n\n\nThanks in advance, I am pretty new to reddit and pushshift package as well.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How do I extract post \/ comments to the existing post in last 24 hours ?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k9avo4\/how_do_i_extract_post_comments_to_the_existing\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k99nri":{"author":"shirin_boo","author_fullname":"t2_90dfsonm","author_premium":false,"created_utc":["2020-12-08","20:22:02"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k99nri\/submission_extraction\/","id":"k99nri","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k99nri\/submission_extraction\/","selftext":"[removed]","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"submission_extraction","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k99nri\/submission_extraction\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":"reddit","media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k955jc":{"author":"Expensive_Board_31","author_fullname":"t2_983t5e3a","author_premium":false,"created_utc":["2020-12-08","16:31:18"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k955jc\/what_does_pushshift_log_by_way_of_api_users_im\/","id":"k955jc","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k955jc\/what_does_pushshift_log_by_way_of_api_users_im\/","selftext":"Hey all, just had a quick question. I was curious what gets logged when you do an API search. Normally I'd just take a peek at the privacy policy for that, but I can't seem to find one. \n\nFor example, if I had 5 different alt reddit accounts that are all unrelated to each other, and decided to search all of them back to back in the api, is there now some api logs somewhere linking searches of those 5 accounts to a single IP? Or does the api not log searches like that?\n\nThanks!","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"What does pushshift log by way of api users? I'm having trouble finding the privacy policy","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k955jc\/what_does_pushshift_log_by_way_of_api_users_im\/","comments":{"gf2byxe":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"pushshift doesn't have it's own privacy policy but requests are logged for diagnostics and abuse.","created_utc":["2020-12-08","18:10:38"],"id":"gf2byxe","link_id":"t3_k955jc","parent_id":"t3_k955jc","permalink":"\/r\/pushshift\/comments\/k955jc\/what_does_pushshift_log_by_way_of_api_users_im\/gf2byxe\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k92svp":{"author":"availattempt","author_fullname":"t2_iz32i","author_premium":false,"created_utc":["2020-12-08","13:44:23"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/","id":"k92svp","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/","selftext":"Is this possible?\n\nI have a list of about 300 link_ids for reddit threads that I manually picked out for qualitative data analysis. I want to create a text file for each thread\/link_id that displays the thread(identified via the link_id) as you would see it on reddit with a comments and replies to comments. Comments can be sorted by new or best. \n\nI'm quite new to python and have managed to extract submissions and comments using a complex search query in an excell file. But I'm having trouble finding a tutorial or figuring out how to do this with pushshift. I've searched for this online and in this subreddit, but I don't think I am using the right language because I'm not getting relevant results. Any help - especially a link to a tutorial, or even the right language to use to find info on this - would be much appreciated and you will be contributing to the advancement of a research project.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How to reconstruct reddit threads with submission + comments as you would see them on reddit (in a comment tree) using a list of several hundred link_ids?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/","comments":{"gf1zmqd":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"Seems like a waste of time to try to reconstruct reddit. Why not just create a doc with links to the relevant threads you want to review? The researcher can just click the link and review the comment thread on reddit? I'm not seeing the value in trying to do it on your own.\n\nAlso, youd need to create a detailed web app with a complex gui. It would be a lot of work.","created_utc":["2020-12-08","16:24:19"],"id":"gf1zmqd","link_id":"t3_k92svp","parent_id":"t3_k92svp","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf1zmqd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf20wtw":{"author":"Deertreetie","author_fullname":"t2_ha3r5c2","author_premium":false,"banned_at_utc":null,"body":"The aim is to import it into a qualitative data analysis program where the text can be coded. It can be in a PDF, excell file or text document of some sort.\n\nI could do this manually by doing screen captures of each thread and then importing it into the qualitative analysis program. I'm looking to save time (there will be 300+ reddit convos to qualitatively code) by using python in some way.\n\nI figure since I have the submission self-text, the comment text, the position of the comment in the comment tree, and the link ids, that there should be a way to code an output of a text file (either in one file or a separate file for each thread) where you can read the full discussion and see who replied to who. \n\nOnly the actual text is needed and maybe some metadata(usernames, dates, votes, etc). No graphical interface is needed. A comment that replies to another comment can be represented by an indent or even labeled by its position in the comment tree. \n\nWould this require something complex and time consuming like a webapp with a complex gui?","created_utc":["2020-12-08","16:36:41"],"id":"gf20wtw","link_id":"t3_k92svp","parent_id":"t1_gf1zmqd","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf20wtw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf21ln1":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"Oh got it. I've worked with that kind of software before. Each comment should have a parent id that will identify which comment it is replying to. Why not try writing the comments to a text file that maintains the parent\/child comment order?","created_utc":["2020-12-08","16:43:09"],"id":"gf21ln1","link_id":"t3_k92svp","parent_id":"t1_gf20wtw","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf21ln1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf23oc7":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"One other thing, I think the link id only points to the parent post and not parent comment.","created_utc":["2020-12-08","17:01:51"],"id":"gf23oc7","link_id":"t3_k92svp","parent_id":"t1_gf20wtw","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf23oc7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf28e6h":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Yes, link\\_id will always be the top level submission of the comment tree.","created_utc":["2020-12-08","17:41:46"],"id":"gf28e6h","link_id":"t3_k92svp","parent_id":"t1_gf23oc7","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf28e6h\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf3gue5":{"author":"Deertreetie","author_fullname":"t2_ha3r5c2","author_premium":false,"banned_at_utc":null,"body":"Sounds like a good idea. I'm new to python and have mostly been following tutorials. I think I might have something I can tweak to attempt this. Do you have any resources that might help with this as well? Thank you for your help by the way.","created_utc":["2020-12-08","23:25:45"],"id":"gf3gue5","link_id":"t3_k92svp","parent_id":"t1_gf21ln1","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf3gue5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k8uw9u":{"author":"InTheClouds125","author_fullname":"t2_97s672k7","author_premium":false,"created_utc":["2020-12-08","03:55:41"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k8uw9u\/need_help_with_pushshift\/","id":"k8uw9u","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k8uw9u\/need_help_with_pushshift\/","selftext":"I would like my data removed from pushshift and would like to opt out of any future collection.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Need help with pushshift.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k8uw9u\/need_help_with_pushshift\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k7ak8m":{"author":"EsssQueee","author_fullname":"t2_192397nx","author_premium":false,"created_utc":["2020-12-05","18:34:31"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/","id":"k7ak8m","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/","selftext":"Hi,  \nI wanted to create a reddit bot just for fun.   \nThe aim was to get a notification when a specific keyword is mentioned anywhere in reddit  \nI started with the reddit api (  stream.comments and  stream.submissions ) and it was working fine but eventually I felt it is missing quite a lot of comments.  \nThen I tried experimenting with  [http:\/\/stream.pushshift.io\/](http:\/\/stream.pushshift.io\/).\n\nWith reddit's native api I see around 20KBps bandwidth usage but with  [http:\/\/stream.pushshift.io\/](http:\/\/stream.pushshift.io\/) it is around 200KBps ( almost 10x).  \nIs this expected? I don't have that much bandwidth to spare so it there a way to bind down the BW usage?\n\nThanks!","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"http:\/\/stream.pushshift.io\/ data usage","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/","comments":{"gerylby":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"From the [docs](https:\/\/github.com\/pushshift\/reddit_sse_stream)\n&gt;I highly encourage you to connect using compression headers to conserve bandwidth if you use the full feed.\n\n&gt;`curl --verbose --compressed 'http:\/\/stream.pushshift.io\/?type=comments'`\n\nThen you can filter to just the parts of the stream you need like only comments and just the body and link http:\/\/stream.pushshift.io\/?type=comments&amp;filter=body,permalink\n\nOtherwise you could use something like PSAW https:\/\/github.com\/dmarx\/psaw or the pushshift api directly to offload most of the keyword searching\n\nFor comments something like https:\/\/api.pushshift.io\/reddit\/comment\/search?q=keyword&amp;size=100&amp;fields=permalink,body\n\nFor submissions something like\nhttps:\/\/api.pushshift.io\/reddit\/submission\/search?q=keyword&amp;size=100&amp;fields=permalink,selftext","created_utc":["2020-12-06","06:03:38"],"id":"gerylby","link_id":"t3_k7ak8m","parent_id":"t3_k7ak8m","permalink":"\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/gerylby\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ges6wuy":{"author":"EsssQueee","author_fullname":"t2_192397nx","author_premium":false,"banned_at_utc":null,"body":"thanks!  \nI am using the python sse client so I guess it is already using the compression headers for request. But filtering does seem to help. If I use the filter to include only selftext,url,body and permalink I am able to get it down to 30KBps.","created_utc":["2020-12-06","07:08:52"],"id":"ges6wuy","link_id":"t3_k7ak8m","parent_id":"t1_gerylby","permalink":"\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/ges6wuy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k77z3i":{"author":"BadAtPr0gramming","author_fullname":"t2_96ftdm8f","author_premium":false,"created_utc":["2020-12-05","16:01:33"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/","id":"k77z3i","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/","selftext":"Hi everyone, just want to start this post out by saying that I'm fairly new to programming, so there's still all kinds of stuff that I don't know.  \n  \nThat said, I'm currently working on a python script to scrape comments from a popular subreddit and save to my pc for later analysis. My code for getting the comments looks like this:  \n  \nhttps:\/\/gist.github.com\/vankirkm\/0a31ab749d7a620f6e978a69a7dd2ca6  \n  \nSo I guess my question is, is this query breaking the rules of the the rate limits, and am I putting unnecessary stress on the API by doing this this way?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Question about rate limits and data usage","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/","comments":{"gepgz71":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Is that the PSAW API wrapper? It looks like it. If so, it handles rate limiting automatically.","created_utc":["2020-12-05","17:56:11"],"id":"gepgz71","link_id":"t3_k77z3i","parent_id":"t3_k77z3i","permalink":"\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/gepgz71\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gephklz":{"author":"BadAtPr0gramming","author_fullname":"t2_96ftdm8f","author_premium":false,"banned_at_utc":null,"body":"Yeah, I'm using psaw to access the data on pushshift. I thought it handled it automatically, but just wanted to double check that I wasn't causing any issues. Thanks for the info.","created_utc":["2020-12-05","18:01:51"],"id":"gephklz","link_id":"t3_k77z3i","parent_id":"t1_gepgz71","permalink":"\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/gephklz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg","resolutions":[{"height":54,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5811c5bda5fece1040636a6af8702ba790f0fd4","width":108},{"height":108,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=eee576fd4da7535eb53ceb88dd8b52f073048441","width":216},{"height":160,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=72872d880460efa723918c000adca0ed259cf775","width":320},{"height":320,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3545b9335d763c9da9c16bf7bf9a3f907dbd6f6","width":640},{"height":480,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=2d241ace0f1c07088fac3f8469dbad3b05d2d419","width":960},{"height":540,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9055f11bdc00beb0b3589e1cae5817d6070d83bc","width":1080}],"source":{"height":640,"url":"https:\/\/external-preview.redd.it\/4-DxLM-C2Ve3tHmVL5ITI6GRtMVG8PzzdBuCKiaabfE.jpg?auto=webp&amp;s=079a7260ec149880c73263d64811698adb22760a","width":1280},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k6q8qi":{"author":"FireDragonRider","author_fullname":"t2_cgimtz","author_premium":false,"created_utc":["2020-12-04","20:13:36"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/","id":"k6q8qi","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/","selftext":"Hi, I have probably a simple question :D How do I get the number of members of a specific subreddit?  \nThanks :)","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"number of subreddit members","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/","comments":{"gembo0m":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-12-04","20:23:10"],"id":"gembo0m","link_id":"t3_k6q8qi","parent_id":"t3_k6q8qi","permalink":"\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/gembo0m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gend64y":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"The subscriber count as of the most recent submission in the subreddit https:\/\/api.pushshift.io\/reddit\/submission\/search?subreddit=pushshift&amp;size=1&amp;fields=subreddit_subscribers\n\nThe current count\nhttps:\/\/api.reddit.com\/r\/pushshift\/about  \n`\"subscribers\":`","created_utc":["2020-12-05","01:41:15"],"id":"gend64y","link_id":"t3_k6q8qi","parent_id":"t3_k6q8qi","permalink":"\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/gend64y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k6k2q2":{"author":"meager_narration","author_fullname":"t2_8yakpzil","author_premium":false,"created_utc":["2020-12-04","14:34:09"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/","id":"k6k2q2","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/","selftext":"Hi everyone, is there a delay in data ingestion in pushshift?\n\nFor example, the subreddit \/r\/Ptoughneigh has two posts on Reddit, but no data is returned when searching the API: [http:\/\/api.pushshift.io\/reddit\/search\/submission\/?subreddit=Ptoughneigh](http:\/\/api.pushshift.io\/reddit\/search\/submission\/?subreddit=Ptoughneigh)","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Delay in data ingestion","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/","comments":{"geli0ug":{"author":"blakebockrath","author_fullname":"t2_4s21bz3","author_premium":false,"banned_at_utc":null,"body":"There\u2019s definitely a delay in data ingestion. I couldn\u2019t tell you by how much but If you are looking at a newer subreddit it might not be in the database yet.","created_utc":["2020-12-04","16:17:39"],"id":"geli0ug","link_id":"t3_k6k2q2","parent_id":"t3_k6k2q2","permalink":"\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/geli0ug\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gem561g":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"Yes, there's currently about an 8.5 hour delay.","created_utc":["2020-12-04","19:32:05"],"id":"gem561g","link_id":"t3_k6k2q2","parent_id":"t3_k6k2q2","permalink":"\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/gem561g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k60oeo":{"author":"ez613","author_fullname":"t2_1lbcpzsa","author_premium":false,"created_utc":["2020-12-03","18:39:12"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k60oeo\/best_way_of_counting_number_of_posts_in_a_sub_in\/","id":"k60oeo","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k60oeo\/best_way_of_counting_number_of_posts_in_a_sub_in\/","selftext":"Hi,\n\nI work on a program to find how many post there was in a subreddit in a day (using PSAW).\n\nSo my question is : is there a \"good\" way to do that ?\n\nFor now I do that :\n\n    len(list(api.search_submissions(after=int(datetime.now().timestamp())-20*24*60*60,\n                                    before=int(datetime.now().timestamp())-12*60*60,\n                                    subreddit='cursedimages',\n                                   filter=[])))\n\nSo I found that using \"filter=\\[\\] \" seems make the query a lot speeder. I there a way to do better ? \n\nMaybe to return just the metadata ? I didn't find if it's possible ...\n\nThank you in advance !","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Best way of counting number of posts in a sub in a day","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k60oeo\/best_way_of_counting_number_of_posts_in_a_sub_in\/","comments":{"gem7v95":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"I imagine it's a minority of subreddits that have more than 1000 posts in a 24-hour period. For those subs you can just use PRAW to see if you can get more than 24 hours away while checking the 1000 newest posts.","created_utc":["2020-12-04","19:53:11"],"id":"gem7v95","link_id":"t3_k60oeo","parent_id":"t3_k60oeo","permalink":"\/r\/pushshift\/comments\/k60oeo\/best_way_of_counting_number_of_posts_in_a_sub_in\/gem7v95\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k5m2ko":{"author":"TheVicePresident","author_fullname":"t2_5nt1i","author_premium":false,"created_utc":["2020-12-03","02:16:06"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/","id":"k5m2ko","num_comments":6,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Any idea why the GoFundMe is paused?","url":"https:\/\/i.redd.it\/0yowtola7v261.png","comments":{"gefn89o":{"author":"TheVicePresident","author_fullname":"t2_5nt1i","author_premium":false,"banned_at_utc":null,"body":"The Pushshift website links to a GoFundMe page in the donations section but it is paused. Given the recent growth in use and the increased expenses with the new servers, I want to make sure that everyone who wants to donate can donate. I just donated through the Quick Donations link on the same page but I feel like some percent of people are going to to go the GoFundMe, see its paused, and then decide it isn't worth the trouble and end up not donating, and that's just money left on the table.","created_utc":["2020-12-03","02:20:18"],"id":"gefn89o","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gefn89o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geg2vmr":{"author":"Stuck_In_the_Matrix","author_fullname":"t2_bk1iz","author_premium":false,"banned_at_utc":null,"body":"Hey there! Thanks for bringing this to my attention. I think the timeframe may have just ended. Pushshift has a donations page here: https:\/\/pushshift.io\/donations\/\n\nRight now it is only fixed amounts because the API changed and I need to update the code to allow people to select their own donation amount.\n\nThis method also has less fees involved on my end so its better overall. \n\nThanks for thinking of donating!","created_utc":["2020-12-03","04:35:01"],"id":"geg2vmr","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/geg2vmr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelig67":{"author":"blakebockrath","author_fullname":"t2_4s21bz3","author_premium":false,"banned_at_utc":null,"body":"Does anyone know what the benefits of becoming a patron are? I\u2019ve been using the api a bit and would like to become one unless the benefits are nothing and in that case I\u2019d just make a quick donation.","created_utc":["2020-12-04","16:21:46"],"id":"gelig67","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gelig67\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"genge1t":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"No special benefits currently.","created_utc":["2020-12-05","02:12:15"],"id":"genge1t","link_id":"t3_k5m2ko","parent_id":"t1_gelig67","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/genge1t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gzzbmik":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2021-05-30","18:07:44"],"id":"gzzbmik","link_id":"t3_k5m2ko","parent_id":"t1_geg2vmr","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gzzbmik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gzzfylo":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"He's on twitter most often https:\/\/twitter.com\/jasonbaumgartne\n\nBut I don't know of any reliable method to contact him.","created_utc":["2021-05-30","18:45:46"],"id":"gzzfylo","link_id":"t3_k5m2ko","parent_id":"t1_gzzbmik","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gzzfylo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"image","preview":{"enabled":true,"images":[{"id":"euBHXDqX_QLzxJm7DBMQln9_71iumL22NvktpoWqF1E","resolutions":[{"height":69,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=73d8bff665d62ad7c5f010f28cfee149e78879f2","width":108},{"height":138,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0cfd75819af94f66c17d7bc1c113854c637c1773","width":216},{"height":205,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=149a73ebb688216f703b6a347d7eaa89c0f1b03a","width":320},{"height":410,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8dc31094deea7bbedf6f16c6c6ea5a1bcfb73ea2","width":640},{"height":615,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c65e6d9242bc124c27835478c63dad9cdcf252cc","width":960},{"height":692,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=88ce5be7dd99e32e11b0f646014cac38114672a2","width":1080}],"source":{"height":1854,"url":"https:\/\/preview.redd.it\/0yowtola7v261.png?auto=webp&amp;s=11706c5ab6daa7198972f993ce0426b7126526b1","width":2892},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":89.0,"thumbnail_width":140.0,"url_overridden_by_dest":"https:\/\/i.redd.it\/0yowtola7v261.png","crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k5cvoc":{"author":"JFM786mithila","author_fullname":"t2_94ulzzuk","author_premium":false,"created_utc":["2020-12-02","18:46:54"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k5cvoc\/how_to_get_6_pack_abs\/","id":"k5cvoc","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k5cvoc\/how_to_get_6_pack_abs\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"How To Get 6 Pack Abs","url":"https:\/\/docs.google.com\/document\/d\/1J-zT7xSnL3iNTfUmYhHfKWjWz1d4ZBkXTJCqzYt74xk\/edit?usp=sharing","comments":{"gedx32g":{"author":"JFM786mithila","author_fullname":"t2_94ulzzuk","author_premium":false,"banned_at_utc":null,"body":"This link will help you very much.","created_utc":["2020-12-02","18:48:32"],"id":"gedx32g","link_id":"t3_k5cvoc","parent_id":"t3_k5cvoc","permalink":"\/r\/pushshift\/comments\/k5cvoc\/how_to_get_6_pack_abs\/gedx32g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"link","preview":{"enabled":false,"images":[{"id":"YtOyqqOmBVkbXxvbOsmkyUuSlWQjWyMbFcdklgVltpk","resolutions":[{"height":56,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=0ad8e3b6942f02f3008ded325ef5e4c46a00d49d","width":108},{"height":113,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=f246f77cbee45e965e267071cc896bc0513cfe88","width":216},{"height":168,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b6d353fa93ce9390fd190f4e65f5c9a904a7e6da","width":320},{"height":336,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=4e90265cfb12bb3d1a47ddf50a7183de5a19c8ca","width":640},{"height":504,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=7cbaf6af9bfaf81f6ed16b2ff41a721cee4af13c","width":960},{"height":567,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b093daea1fda8120980a37902c2c12599c94971b","width":1080}],"source":{"height":630,"url":"https:\/\/external-preview.redd.it\/YNIZ9pUKI-IOGmiktYsUgZnf8DBRDzVJrUYE8TpCuxs.jpg?auto=webp&amp;s=67cd9b27bcce500734a5a7fd38861c88daec80e9","width":1200},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":73.0,"thumbnail_width":140.0,"url_overridden_by_dest":"https:\/\/docs.google.com\/document\/d\/1J-zT7xSnL3iNTfUmYhHfKWjWz1d4ZBkXTJCqzYt74xk\/edit?usp=sharing","crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k5c1ro":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"created_utc":["2020-12-02","18:07:08"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/","id":"k5c1ro","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/","selftext":"I'm working for a data analytics company and I'm using pushshift for the soft data. I've been testing my api code quiet a bit and  keep getting a non 200 code 525 or 522.  I thought that I figured out the rate limit and my code was working fine a few days ago, but this problem popped back up. Does anyone know if this because of the pushshift servers? Could this be a problem with pulling too much?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"non 200 code 525","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/","comments":{"gee7j11":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Have you tried using PSAW (https:\/\/github.com\/dmarx\/psaw), the unofficial Pushshift API wrapper? \n\nEven so, I had issues with PSAW not properly handling 500-level connection errors, even though it\u2019s supposed to and automatically back off and retry, but I modified the PSAW API code and now it catches errors, waits, and retries successfully.\n\nIt is clearly a server-side issue.","created_utc":["2020-12-02","19:58:09"],"id":"gee7j11","link_id":"t3_k5c1ro","parent_id":"t3_k5c1ro","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gee7j11\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geehc2f":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That's mostly just normal. If you hit a 500 just wait and try again later. The API gets too much traffic and gets overloaded.\n\nOr are you never getting 200's at all?","created_utc":["2020-12-02","21:02:13"],"id":"geehc2f","link_id":"t3_k5c1ro","parent_id":"t3_k5c1ro","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/geehc2f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelvoj2":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"None of my requests are going through no matter how man submissions are pulled","created_utc":["2020-12-04","18:16:43"],"id":"gelvoj2","link_id":"t3_k5c1ro","parent_id":"t1_geehc2f","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelvoj2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelvq16":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"How did you solve it?","created_utc":["2020-12-04","18:17:03"],"id":"gelvq16","link_id":"t3_k5c1ro","parent_id":"t1_geixjye","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelvq16\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelw4zx":{"author":"mtodavk","author_fullname":"t2_6e2c3","author_premium":false,"banned_at_utc":null,"body":"I wasn\u2019t passing a time stamp in as the right data type. \ud83d\ude05","created_utc":["2020-12-04","18:20:22"],"id":"gelw4zx","link_id":"t3_k5c1ro","parent_id":"t1_gelvq16","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelw4zx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k3ji22":{"author":"Cheap_Meeting","author_fullname":"t2_2ibsmh69","author_premium":false,"created_utc":["2020-11-30","01:04:25"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/","id":"k3ji22","num_comments":3,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/","selftext":"Hi, It seems that no new dumps have been released recently. \n\nThe most recent comment dump is from 2019 and the most recent daily dump is from 2020-04-18: [https:\/\/files.pushshift.io\/reddit\/comments\/daily\/](https:\/\/files.pushshift.io\/reddit\/comments\/daily\/)\n\nHow long does it typically take for new dumps to get uploaded?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Reddit Dumps","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/","comments":{"ge4ymvd":{"author":"francisco_rodriguez","author_fullname":"t2_1t4dkgl","author_premium":false,"banned_at_utc":null,"body":"Hi all! I'm having the same issue for gab dumps: [https:\/\/files.pushshift.io\/gab\/](https:\/\/files.pushshift.io\/gab\/)\n\nThe most recent dump is from august 2019","created_utc":["2020-11-30","12:41:31"],"id":"ge4ymvd","link_id":"t3_k3ji22","parent_id":"t3_k3ji22","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/ge4ymvd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geean9x":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Gab disabled the API endpoint that allowed the dumps to be made, SITM is still looking for a workaround but until one is found there likely won't be any more gab dumps.\nhttps:\/\/twitter.com\/jasonbaumgartne\/status\/1276518618692452360","created_utc":["2020-12-02","20:18:05"],"id":"geean9x","link_id":"t3_k3ji22","parent_id":"t1_ge4ymvd","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/geean9x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geh3ddt":{"author":"francisco_rodriguez","author_fullname":"t2_1t4dkgl","author_premium":false,"banned_at_utc":null,"body":"I think Gab changed the API endpoint around May 2020 and the last dump is from August 2019. Maybe data from August 2019-May 2020 could be added to the gab dumps.","created_utc":["2020-12-03","12:30:46"],"id":"geh3ddt","link_id":"t3_k3ji22","parent_id":"t1_geean9x","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/geh3ddt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k2btpb":{"author":"muh_reddit_accout","author_fullname":"t2_s1q046f","author_premium":false,"created_utc":["2020-11-28","01:09:01"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k2btpb\/if_we_could_pass_a_filtering_option_to_the_link\/","id":"k2btpb","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k2btpb\/if_we_could_pass_a_filtering_option_to_the_link\/","selftext":"Hey, my man u\/Stuck_In_the_Matrix, thank you for all the incredible work you did and are doing to keep this thing up to date, incredibly user friendly, and free. That being said, if I were to wish for one feature on this API I would wish for the submission endpoint to have a filterable option based on the link_flair_text parameter. It would drastically cut down on the number of requests I'd have to make too, which I've got to imagine is a win\/win.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"If we could pass a filtering option to the link_flair_text parameter that would be phenomenal.","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k2btpb\/if_we_could_pass_a_filtering_option_to_the_link\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k227g4":{"author":"Penalty_Gloomy","author_fullname":"t2_6p6azj28","author_premium":false,"created_utc":["2020-11-27","16:37:27"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k227g4\/has_the_aggs_argument_been_disabled\/","id":"k227g4","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k227g4\/has_the_aggs_argument_been_disabled\/","selftext":"I have been trying to understand the aggs argument and I tried looking at the example in the link  [https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=subreddit&amp;size=0](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=subreddit&amp;size=0)  from the github  [https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=subreddit&amp;size=0](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=subreddit&amp;size=0). I was unable to see any code. Is there anyway to code something like a problem like Number of submissions to r\/technology and r\/news containing 'phone' in September 2018 .","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Has the aggs argument been disabled?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k227g4\/has_the_aggs_argument_been_disabled\/","comments":{"gdro9x6":{"author":"ObsidianDreamsRedux","author_fullname":"t2_czj3bdp","author_premium":false,"banned_at_utc":null,"body":"Yes, it was disabled due to performance issues caused by some.\n\nhttps:\/\/old.reddit.com\/r\/pushshift\/comments\/jm8yyt\/aggregations_have_been_temporarily_disabled_to\/","created_utc":["2020-11-27","16:47:16"],"id":"gdro9x6","link_id":"t3_k227g4","parent_id":"t3_k227g4","permalink":"\/r\/pushshift\/comments\/k227g4\/has_the_aggs_argument_been_disabled\/gdro9x6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k1x0lg":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"created_utc":["2020-11-27","09:56:37"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/","id":"k1x0lg","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/","selftext":"Hi, I am trying to get the frequency by day for a keyword in a subreddit. There is an example on the docs here: [https:\/\/github.com\/pushshift\/api\/blob\/master\/README.md#using-the-time-frequency-created\\_utc-aggregation](https:\/\/github.com\/pushshift\/api\/blob\/master\/README.md#using-the-time-frequency-created_utc-aggregation) but it shows this api call: [https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=created\\_utc&amp;frequency=hour&amp;size=0](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?q=trump&amp;after=7d&amp;aggs=created_utc&amp;frequency=hour&amp;size=0) which does not have any aggs variable. Does anyone know if they changed how this is done?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"No aggs argument?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/","comments":{"gdqxlsx":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"Just saw that it was temporarily disabled. Is there any other way to get posts\/day or comments\/day in a subreddit besides slowly cycling through everyday and getting the total from the metadata?","created_utc":["2020-11-27","10:07:17"],"id":"gdqxlsx","link_id":"t3_k1x0lg","parent_id":"t3_k1x0lg","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdqxlsx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdqy2hj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Iterating through pages is usually fine unless it's a really large sub. The whole point of disabling the aggs argument was to spread out the processing across several calls instead of doing it all at once.","created_utc":["2020-11-27","10:14:32"],"id":"gdqy2hj","link_id":"t3_k1x0lg","parent_id":"t1_gdqxlsx","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdqy2hj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdrvq77":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"I want see the frequency of a thousand over a few years of data. I can definitely use threads to speed it up but I still I think it would take quite some time. Would you recommend me running the query on something like aws?","created_utc":["2020-11-27","18:01:46"],"id":"gdrvq77","link_id":"t3_k1x0lg","parent_id":"t1_gdqy2hj","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdrvq77\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdsh6js":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well no, that's the opposite of what you should be doing. The whole point is to not overwhelm the pushshift servers by making a bunch of requests really fast.\n\nPushshift isn't a big company, it's run by one guy from servers in his living room. It's important to minimize how much ask of it. Which in this case means spreading out the requests as much as possible.\n\nI would just set up the script and leave it running overnight, or even over a few days if I had to. Are you searching in a specific subreddit or across all of reddit? What subreddit?","created_utc":["2020-11-27","20:50:54"],"id":"gdsh6js","link_id":"t3_k1x0lg","parent_id":"t1_gdrvq77","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdsh6js\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdsnx53":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"Ok sounds good! I\u2019d hate to overwhelm the servers or anything. I\u2019ll just run it over a couple of days. Thanks for your help!","created_utc":["2020-11-27","21:52:46"],"id":"gdsnx53","link_id":"t3_k1x0lg","parent_id":"t1_gdsh6js","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdsnx53\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":"self","preview":{"enabled":false,"images":[{"id":"oNiluQrWEx1JPsVsHd_rm3Nq4tzdWT7AnI4IpbTZvrA","resolutions":[{"height":108,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=bbaafc64e8bc5294f675836a9b0d40325beaa35f","width":108},{"height":216,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=34e975f5cccf43d120bdfe2de85c5db86ab33e87","width":216},{"height":320,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=9f2fe11c3a6150e043c08062c8fd76582aa86c79","width":320}],"source":{"height":346,"url":"https:\/\/external-preview.redd.it\/KaCEZAA4nF0PQcsrCxsKsuINRMQyXCVVYaJUeYRuOnM.jpg?auto=webp&amp;s=327bffbaa7bfda971fc78c62fb736fcf4c083aee","width":346},"variants":{}}]},"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k16rbr":{"author":"FishStewie","author_fullname":"t2_m4zkt2h","author_premium":false,"created_utc":["2020-11-26","04:42:52"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/","id":"k16rbr","num_comments":2,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/","selftext":"I noticed https:\/\/www.reveddit.com\/info\/ displays archive delay for submissions and comments. \n\nWhat is the API endpoint for archive delay times? I'd like the ability to switch to praw when delays are several hours behind.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"API Endpoint for Archive Delay Times?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/","comments":{"gdmfy5k":{"author":"Ninja-Waffles","author_fullname":"t2_lh6uk","author_premium":true,"banned_at_utc":null,"body":"I'm not sure if there's a specific endpoint for getting the delays; however, if you're using the API, you can ping it every minute (or however long of a delay you want to add), find the UTC of the first (latest) entry and then match that against the current UTC (on your machine\/server) to get the ingest delay.\n\nFor example, when writing this comment, the first entry on [this](https:\/\/api.pushshift.io\/reddit\/comment\/search) page has `created_utc` as `1606359841` and the current UTC is `1606361198`, so we can do `(1606361198 - 1606359841) \/ 60 = ~22 minutes`. This seems to match up with the output that reveddit is displaying too.","created_utc":["2020-11-26","05:29:27"],"id":"gdmfy5k","link_id":"t3_k16rbr","parent_id":"t3_k16rbr","permalink":"\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/gdmfy5k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdmreak":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"This is exactly what I do for u\/RemindMeBot. There's a bit of abstraction, but the code I have for it is [here](https:\/\/github.com\/Watchful1\/PrawWrapper\/blob\/master\/praw_wrapper\/__init__.py#L136-L150).","created_utc":["2020-11-26","07:40:31"],"id":"gdmreak","link_id":"t3_k16rbr","parent_id":"t1_gdmfy5k","permalink":"\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/gdmreak\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"k0suiz":{"author":"meager_narration","author_fullname":"t2_8yakpzil","author_premium":false,"created_utc":["2020-11-25","16:11:31"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k0suiz\/rate_limits\/","id":"k0suiz","num_comments":1,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/k0suiz\/rate_limits\/","selftext":"Hi everyone,\n\nCan you please help with the rate limits of the pushshift API? How many calls can I make per minute? I have a project where I want to monitor some subreddits every 24 hours.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Rate limits","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/k0suiz\/rate_limits\/","comments":{"gdkeekw":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"60 calls a minute \"one per second\" as per [the announcement a few months ago](https:\/\/www.reddit.com\/r\/pushshift\/comments\/g7125k\/in_an_effort_to_relieve_some_of_the_strain_on_the\/) and a maximum of 100 results per query.","created_utc":["2020-11-25","18:29:39"],"id":"gdkeekw","link_id":"t3_k0suiz","parent_id":"t3_k0suiz","permalink":"\/r\/pushshift\/comments\/k0suiz\/rate_limits\/gdkeekw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"jzmc3t":{"author":"Ruoter","author_fullname":"t2_cfvx3","author_premium":false,"created_utc":["2020-11-23","19:41:25"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jzmc3t\/getting_significantly_fewer_results_if_choosing\/","id":"jzmc3t","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/jzmc3t\/getting_significantly_fewer_results_if_choosing\/","selftext":"I want to get the top n posts (based on number of comments) in a given subreddit during an interval. \n\nI know about the pagination the API does which is why I'm not getting my requested 1000 posts but why is me choosing to sort giving fewer results?\n\n    def get_posts(subreddit, start, end, n):\n        res = api.search_submissions(after=int(start.timestamp()), before=int(end.timestamp()),\n                                     subreddit=subreddit,\n                                     filter=['url', 'author', 'title', 'ups', 'downs', 'score', 'num_comments', 'view_count'],\n                                     score='&gt;200',\n                                     sort_type='num_comments', sort='desc', \n                                     size=n)\n        return [(int(i.created_utc), i.url, str(i.author), i.title, i.ups, i.downs, i.score, i.num_comments, i.view_count) for i in res]\n    \n    results = [get_posts(subreddit, months[d].left, months[d].right, 1000) for d in range(3)]\n    len(results)\n    \n    # without `sort` and `sort_type` arguments\n    [len(i) for i in results] # [92, 95, 92]\n    \n    # with`sort` and `sort_type` arguments\n    [len(i) for i in results] # [170, 188, 119]\n\nIf someone has an alternate way to get the top n posts in a subreddit I would appreciate that as well.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Getting significantly fewer results if choosing to sort on num_comments","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/jzmc3t\/getting_significantly_fewer_results_if_choosing\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lbqrly":{"author":"VBGBeveryday","author_fullname":"t2_7jtsh","author_premium":false,"created_utc":["2021-02-03","18:01:57"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/","id":"lbqrly","num_comments":21,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/","selftext":"I've been meaning to do this for a while (think the redditsearch UI could be much better), but now that it doesn't work whatsoever I guess there's no better time to do it.\n\nI'm going to be building over the next couple of days off of the reddit API, and when PushShift comes up with the new infrastructure, I'll enable that as a source.\n\nFor current users of [redditsearch.io](https:\/\/redditsearch.io), what would you like to see in an alternative? As far as the reddit API allows, I'm going to start with:\n\n* recent post &amp; comment search by keyword\n* filter by sub &amp; users\n* visualize results by subreddit and time\n\nLet me know if you'd like to see anything else, and I'll post updates as I have them.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Making alternative to redditsearch.io while pushshift is down","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/","comments":{"glvitc0":{"author":"AutomaticManager888","author_fullname":"t2_2xhuiavu","author_premium":false,"banned_at_utc":null,"body":"The thing I want to see most is an up to date search for the newest posts in a subreddit. Reddit's own api seems to no longer support this, and I found pushshift as an alternative for that until a few minutes ago that I realized there is a serious problem with pushshift. So, after finding a post about that on this sub, I go back to this sub and see this post 8 minutes ago as of the time I'm writing this, and I could not ask for a better timing of events (pure sarcasm), however I wish reddit would have kept this in their own api and I see NO reason they should have removed it..... My app relies entirely on getting hte newest content available, and this task seems to be impossible so far. \n\n&amp;#x200B;\n\nps. This is my first reddit app and first experience with their api... this system is garbage so far","created_utc":["2021-02-03","18:14:20"],"id":"glvitc0","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvitc0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvrqhh":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"PushShift is [being transitioned](https:\/\/twitter.com\/jasonbaumgartne\/status\/1352611879642927107) from a bunch of servers in a basement to the AWS cloud. I'm not sure most people realize the scale and storage requirements of this endeavour. As of last June, the platform was ingesting half a petabyte of uncompressed data each month and serving 50-100 TB of data via the APIs and [data.pushshift.io](https:\/\/data.pushshift.io). The projected costs for the new infrastructure are [$15k-20k per month](https:\/\/twitter.com\/jasonbaumgartne\/status\/1355040356543377414). The reality is the existing hardware can no longer keep up with the current rate of content generation on Reddit (huge spam problems), so we're going to have to wait until the transition is complete.","created_utc":["2021-02-03","19:14:34"],"id":"glvrqhh","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvrqhh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvtz60":{"author":"VBGBeveryday","author_fullname":"t2_7jtsh","author_premium":false,"banned_at_utc":null,"body":"Wow! What a project. Definitely have nothing but love for PushShift. I'm excited for the new infrastructure, I'm sure that will open up many more possibilities for app developers and end-users.\n\nIn the meantime, I'm hoping to provider a decent alternative to [redditsearch.io](https:\/\/redditsearch.io) for those that use the tool and are not able to do so at this time.","created_utc":["2021-02-03","19:29:27"],"id":"glvtz60","link_id":"t3_lbqrly","parent_id":"t1_glvrqhh","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvtz60\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvw0op":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"If you need a starting point, here's an alternate someone made: [https:\/\/camas.github.io\/reddit-search\/](https:\/\/camas.github.io\/reddit-search\/)\n\nObviously it's not working right now due to PushShift being offline.","created_utc":["2021-02-03","19:43:08"],"id":"glvw0op","link_id":"t3_lbqrly","parent_id":"t1_glvtz60","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvw0op\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvzwx8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Have you heard anything from Jason about it since then? I don't want to complain about a free project, but he's missed lots of his own deadlines in the past, so I'm not really hopeful of this transition happening anytime soon.","created_utc":["2021-02-03","20:07:57"],"id":"glvzwx8","link_id":"t3_lbqrly","parent_id":"t1_glvrqhh","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvzwx8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glw1xkw":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2021-02-03","20:21:08"],"id":"glw1xkw","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glw1xkw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":1612374740.0,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lbimf0":{"author":"osiworx","author_fullname":"t2_84gi8ey8","author_premium":false,"created_utc":["2021-02-03","09:51:07"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbimf0\/pushshift_alternatives\/","id":"lbimf0","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lbimf0\/pushshift_alternatives\/","selftext":"With pushshift down, I read I can also take the data directly from reddit api's? Are they ok with me copying data in the same volume I was taking from pushshift? \n\nAre there any restrictions I did not find yet?\n\nWhat are your experiences?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Pushshift alternatives?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbimf0\/pushshift_alternatives\/","comments":{"glvtxww":{"author":"Ricsta76","author_fullname":"t2_q59yz","author_premium":false,"banned_at_utc":null,"body":"Unfortunately, the search feature on the official Reddit API is limited. There is no way to search with date parameters (like \"posts from today only\") and the max amount of results is 100 per API call. I also don't think there is a way to generally search for comments without specifying a specific submission. So I guess it depends what data you are looking for, but Pushshift exists because of these limitations.\n\nI think there might be a way to set up a comment stream from the Reddit API, but I don't know much about that.","created_utc":["2021-02-03","19:29:14"],"id":"glvtxww","link_id":"t3_lbimf0","parent_id":"t3_lbimf0","permalink":"\/r\/pushshift\/comments\/lbimf0\/pushshift_alternatives\/glvtxww\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lbflbm":{"author":"Vortex_Charge","author_fullname":"t2_91nwkkyj","author_premium":false,"created_utc":["2021-02-03","06:32:05"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/","id":"lbflbm","num_comments":8,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/","selftext":"","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Anyone got an alternative of push shift to use while it\u2019s down?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/","comments":{"gltwb38":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I know this is unhelpful and you don't want to hear this, but nothing comes close. Depending on what you're doing, you might be able to get by with torrenting some of the static comments\/submissions files and drawing your data from there.","created_utc":["2021-02-03","07:28:49"],"id":"gltwb38","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/gltwb38\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gltyjvp":{"author":"Silver__Bug","author_fullname":"t2_53v0ksem","author_premium":false,"banned_at_utc":null,"body":"You can try the dumps available [Here](http:\/\/files.pushshift.io\/) or the bigquery api, though they are not up to date.","created_utc":["2021-02-03","07:53:10"],"id":"gltyjvp","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/gltyjvp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glu905l":{"author":"spicyboi97","author_fullname":"t2_1325a8","author_premium":false,"banned_at_utc":null,"body":"If you\u2019re using python, use PRAW\n\nif you\u2019re using Node, use Snoowrap\n\nI use both actively and they\u2019re both great.","created_utc":["2021-02-03","10:07:55"],"id":"glu905l","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/glu905l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glunzgz":{"author":"immibis","author_fullname":"t2_dj2ua","author_premium":true,"banned_at_utc":null,"body":"Reddit API...?","created_utc":["2021-02-03","13:50:32"],"id":"glunzgz","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/glunzgz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lb6p1l":{"author":"Swiss-Rock","author_fullname":"t2_98nxwgah","author_premium":true,"created_utc":["2021-02-02","23:18:33"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lb6p1l\/no_data\/","id":"lb6p1l","num_comments":0,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lb6p1l\/no_data\/","selftext":"Do we any information on when will the extraction of data be possible again? Thanks a lot","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"No Data","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lb6p1l\/no_data\/","comments":{},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lb4n9h":{"author":"syncretistMind","author_fullname":"t2_a0u4cvsq","author_premium":false,"created_utc":["2021-02-02","21:50:44"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/","id":"lb4n9h","num_comments":5,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/","selftext":"Hi guys,\n\nso it seems Pushshift has been down for a couple of days. The creator and maintainer of Pushshift has mentioned on Twitter that he is scaling up the storage and revamping the infrastructure. Is that the reason for it? Does anybody know if the comments and posts that are currently being posted on Reddit will still be available once Pushshift comes back online i.e. no gaps in the data?","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"Pushshift down due to revamping of infrastructure?","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/","comments":{"glru39s":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I don't think it's related. Though hopefully the new infrastructure will reduce the amount of downtime once it's online.\n\nI've given up reporting the outages to him.","created_utc":["2021-02-02","21:54:27"],"id":"glru39s","link_id":"t3_lb4n9h","parent_id":"t3_lb4n9h","permalink":"\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/glru39s\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gltvbuv":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Yeah I love SITM and his work but he goes AWOL for frequent periods of time. I don't mean to judge since I don't know what's going on with him, but he can be hard to reach\/get updates from.","created_utc":["2021-02-03","07:18:42"],"id":"gltvbuv","link_id":"t3_lb4n9h","parent_id":"t1_glru39s","permalink":"\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/gltvbuv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null},"lambkn":{"author":"Sparkybear","author_fullname":"t2_4g8rr","author_premium":true,"created_utc":["2021-02-02","05:29:15"],"full_link":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/","id":"lambkn","num_comments":4,"num_crossposts":0,"permalink":"\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/","selftext":"https:\/\/api.pushshift.io\/reddit\/comment\/search\/?subreddit=wallstreetbets&amp;link_id=109rxj&amp;nest_level=10\n\nReturns 10 of 24 comments\n\nhttps:\/\/api.pushshift.io\/reddit\/submission\/comment_ids\/109rxj\n\nReturns nothing\n\n\nTrying to get all comment data for a given post id but so far have been unable to do so.","subreddit":"pushshift","subreddit_id":"t5_37z6f","title":"submission\/comment_ids returns an empty list, search\/comments\/?link_id returns an incomplete list","url":"https:\/\/www.reddit.com\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/","comments":{"glpawn0":{"author":"osiworx","author_fullname":"t2_84gi8ey8","author_premium":false,"banned_at_utc":null,"body":"They seem to be down since a few days now. I hope they come back anytime soon.","created_utc":["2021-02-02","08:27:22"],"id":"glpawn0","link_id":"t3_lambkn","parent_id":"t3_lambkn","permalink":"\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/glpawn0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glrterp":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Disappointing - over the last 6-8 months, the reddit side of pushshift is feeling more and more abandoned.  Got all these promises of a new ingest, and the beta went up, but then it kind of went no where after that - except disabling aggregations, really crippling a lot of use for the tool - and without migrating to the new ingest it's becoming less and less reliable with big lag time and huge data gaps now.","created_utc":["2021-02-02","21:49:57"],"id":"glrterp","link_id":"t3_lambkn","parent_id":"t3_lambkn","permalink":"\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/glrterp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}},"post_hint":null,"preview":null,"edited":null,"removed_by_category":null,"media_metadata":null,"thumbnail_height":null,"thumbnail_width":null,"url_overridden_by_dest":null,"crosspost_parent":null,"crosspost_parent_list":null,"author_cakeday":null}}