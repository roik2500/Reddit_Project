{"g8u4vmx":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Say you wanted to get the comments of \/r\/pushshift.\n\n`api.search_comments(subreddit=\"Pushshift\")`\n\nhttps:\/\/github.com\/dmarx\/psaw","created_utc":["2020-08-07","11:27:36"],"id":"g0nmiiq","link_id":"t3_i5a2s7","parent_id":"t3_i5a2s7","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g0nmiiq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8u8o2c":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"No, there are no per subreddit dumps. There are dumps of all posts\/comments, but put together it's nearly a terrabyte of data, and much more uncompressed. It would be much faster to just use the API.","created_utc":["2020-08-12","19:56:55"],"id":"g18c0xf","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g18c0xf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ua5ty":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Thanks. The combined dumps are beyond what my machine xan handle","created_utc":["2020-08-12","20:23:45"],"id":"g18fppy","link_id":"t3_i8dlzs","parent_id":"t1_g18c0xf","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g18fppy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ulmvd":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-08-12","23:19:11"],"id":"g193djy","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g193djy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uqb9y":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"    import requests\n    import time\n\n    subreddit = 'aoe2'\n    maxThings = -1\n    printWait = 2\n    requestSize = 100\n\n\n    def requestJSON(url):\n        while True:\n            try:\n                r = requests.get(url)\n                if r.status_code != 200:\n                    print('error code', r.status_code)\n                    time.sleep(5)\n                    continue\n                else:\n                    break\n            except Exception as e:\n                print(e)\n                time.sleep(5)\n                continue\n        return r.json()\n\n    meta = requestJSON('https:\/\/api.pushshift.io\/meta')\n    limitPerMinute = meta['server_ratelimit_per_minute']\n    requestWait = 60 \/ limitPerMinute\n\n    print('server_ratelimit_per_minute', limitPerMinute)\n\n    things = ('submission', 'comment')\n\n    for thing in things:\n        i = 0\n\n        with open(subreddit + '_' + thing + '_' + str(int(time.time())) + '.txt', 'w') as f:\n            print('\\n[starting', thing + 's]')\n\n            if maxThings &lt; 0:\n\n                url = 'https:\/\/api.pushshift.io\/reddit\/search\/'\\\n                      + thing + '\/?subreddit='\\\n                      + subreddit\\\n                      + '&amp;metadata=true&amp;size=0'\n                \n                json = requestJSON(url)\n                \n                totalResults = json['metadata']['total_results']\n                print('total ' + thing + 's', 'in', subreddit,':', totalResults)\n            else:\n                totalResults = maxThings\n                print('downloading most recent', maxThings)\n\n\n            created_utc = ''\n\n            startTime = time.time()\n            timePrint = startTime\n            while True:\n                url = 'http:\/\/api.pushshift.io\/reddit\/search\/'\\\n                      + thing + '\/?subreddit=' + subreddit\\\n                      + '&amp;size=' + str(requestSize)\\\n                      + '&amp;before=' + str(created_utc)\n\n                json = requestJSON(url)\n\n                if len(json['data']) == 0:\n                    break\n\n                doneHere = False\n                for post in json['data']:\n                    created_utc = post[\"created_utc\"]\n                    f.write(str(post) + '\\n')\n                    i += 1\n                    if i &gt;= totalResults:\n                        doneHere = True\n                        break\n\n                if doneHere:\n                    break\n                \n                if time.time() - timePrint &gt; printWait:\n                    timePrint = time.time()\n                    percent = i \/ totalResults * 100\n                    \n                    timePassed = time.time() - startTime\n                    timeLeft = timePassed \/ i * totalResults\n                    \n                    print('{:.2f}'.format(percent) + '%', '|',\n                          time.strftime(\"%H:%M:%S\", time.gmtime(timePassed)),\n                          '| approx time left',\n                          time.strftime(\"%H:%M:%S\", time.gmtime(timeLeft)))\n\n\n                time.sleep(requestWait)","created_utc":["2020-08-12","23:22:39"],"id":"g193tsq","link_id":"t3_i8dlzs","parent_id":"t3_i8dlzs","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g193tsq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uqqxe":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Many thanks. I'll try this out\n\nIf you have a github and work on social media analysis, I would love to follow your work","created_utc":["2020-08-12","23:42:19"],"id":"g196e3x","link_id":"t3_i8dlzs","parent_id":"t1_g193tsq","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g196e3x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ur10q":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Don't really have anything worth githubbing","created_utc":["2020-08-12","23:49:41"],"id":"g197c0e","link_id":"t3_i8dlzs","parent_id":"t1_g196e3x","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g197c0e\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8ur6c0":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Okay, thanks nonetheless","created_utc":["2020-08-13","01:21:04"],"id":"g19ivbz","link_id":"t3_i8dlzs","parent_id":"t1_g197c0e","permalink":"\/r\/pushshift\/comments\/i8dlzs\/how_to_download_all_posts_from_a_subreddit\/g19ivbz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uux2a":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"stop adding everyone who comments in r\/INTJ you stupid dumb ass shitty faggot retard no one wants to join your pathetic low-quality sub you fucking moron","created_utc":["2020-08-13","18:43:57"],"id":"g1c63i1","link_id":"t3_i5a2s7","parent_id":"t3_i5a2s7","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1c63i1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8uvjct":{"author":"Xzanium2","author_fullname":"t2_719l7c1v","author_premium":false,"banned_at_utc":null,"body":"Excuse me how is this related to r\/pushshift?","created_utc":["2020-08-13","19:38:09"],"id":"g1cdb0n","link_id":"t3_i5a2s7","parent_id":"t1_g1c63i1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cdb0n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8v6s8o":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"its what you're using to annoy the fuck out of everyone who comments on r\/INTJ, dont act fucking stupid","created_utc":["2020-08-13","19:39:21"],"id":"g1cdh42","link_id":"t3_i5a2s7","parent_id":"t1_g1cdb0n","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cdh42\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8v6vz9":{"author":"ScamWatchReporter","author_fullname":"t2_5w21c3a3","author_premium":false,"banned_at_utc":null,"body":"Report his account, the bot account, and the sub account to rEDDIT.com\/report if you have been added as authorized user to a sub you didn't subscribe to by him and his bots. He's had several accounts banned for it already","created_utc":["2020-08-13","21:54:49"],"id":"g1cvwy1","link_id":"t3_i5a2s7","parent_id":"t1_g1cdh42","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cvwy1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8xlc79":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"I hadn't considered that so thanks so much for bringing that to my attention! I'm about to submit this report right now since this guy and his shitty bot has been annoying the hell out of me for over a week.","created_utc":["2020-08-13","22:00:53"],"id":"g1cwpzn","link_id":"t3_i5a2s7","parent_id":"t1_g1cvwy1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1cwpzn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g8z24n2":{"author":"ObsidianDreamsRedux","author_fullname":"t2_czj3bdp","author_premium":false,"banned_at_utc":null,"body":"Seems fine to me. I searched for a few generic words and got results right away.  Firefox on Linux. And I do have pihole doing ad blocking for the network, and it was not a factor at all.","created_utc":["2020-08-13","22:42:29"],"id":"g1d2b0z","link_id":"t3_i96aa7","parent_id":"t3_i96aa7","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1d2b0z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g91k0ik":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Working for me.","created_utc":["2020-08-14","00:27:27"],"id":"g1dfs37","link_id":"t3_i96aa7","parent_id":"t3_i96aa7","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dfs37\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g92n8hh":{"author":"Carbon_Rod","author_fullname":"t2_a5z8k","author_premium":false,"banned_at_utc":null,"body":"Think I figured it out; I had redditsearch.io in a pinned tab. However, it won't work unless you have the full https:\/\/www.redditsearch.io (figured that out trying to use Firefox instead). So, I unpinned and repinned with the https address and it's working again.","created_utc":["2020-08-14","00:40:47"],"id":"g1dhh4o","link_id":"t3_i96aa7","parent_id":"t1_g1d2b0z","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dhh4o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g92p90k":{"author":"Carbon_Rod","author_fullname":"t2_a5z8k","author_premium":false,"banned_at_utc":null,"body":"Think my problem was using redditsearch.io, when it wanted the full https:\/\/www.redditsearch.io address (at least, it works when I do that, so problem solved).","created_utc":["2020-08-14","00:42:05"],"id":"g1dhn00","link_id":"t3_i96aa7","parent_id":"t1_g1dfs37","permalink":"\/r\/pushshift\/comments\/i96aa7\/is_redditsearchio_down_for_everyone\/g1dhn00\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gid847y":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"In a specific subreddit?","created_utc":["2020-08-14","13:47:47"],"id":"g1fg4zd","link_id":"t3_i9iipd","parent_id":"t3_i9iipd","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1fg4zd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g72xqgp":{"author":"Composer_Own","author_fullname":"t2_6q7qs4oz","author_premium":false,"banned_at_utc":null,"body":"Yes I have a list of 11 subreddits for which I need to find all posts and comments since 2010, fh-bigquery has tables after 12-2015 which I can query but how do I get information before that date, do we have any other dataset, considering the fact that pushshift.rt\\_reddit does not have data.","created_utc":["2020-08-14","17:52:48"],"id":"g1g3m1j","link_id":"t3_i9iipd","parent_id":"t1_g1fg4zd","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1g3m1j\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g72yqhx":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"Please don't use language like that.","created_utc":["2020-08-15","06:33:53"],"id":"g1iomnv","link_id":"t3_i5a2s7","parent_id":"t1_g1c63i1","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1iomnv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g73c5tf":{"author":"nutsak420","author_fullname":"t2_3kaopoms","author_premium":false,"banned_at_utc":null,"body":"shut the fuck up Karen I wasn't talking to your bichass","created_utc":["2020-08-15","07:07:45"],"id":"g1itdy5","link_id":"t3_i5a2s7","parent_id":"t1_g1iomnv","permalink":"\/r\/pushshift\/comments\/i5a2s7\/how_do_you_find_the_comments_for_a_specific\/g1itdy5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g73lih0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That's mostly normal. It just means you made too many requests and pushshift told you to slow down, so PSAW is slowing down. If you're getting it a lot, every few seconds maybe, something is wrong. But if it's only occasionally you're fine.","created_utc":["2020-08-15","07:18:21"],"id":"g1iuvzu","link_id":"t3_ia0ztt","parent_id":"t3_ia0ztt","permalink":"\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/g1iuvzu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g5zy37b":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"Thank you!","created_utc":["2020-08-15","07:21:25"],"id":"g1ivck0","link_id":"t3_ia0ztt","parent_id":"t1_g1iuvzu","permalink":"\/r\/pushshift\/comments\/ia0ztt\/what_happens_when_the_script_is_retrying_after\/g1ivck0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g600uuk":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Look at \"parent\\_id\" prefix. **t3_** is a link, **t1_** is a comment.","created_utc":["2020-08-16","21:41:12"],"id":"g1rclau","link_id":"t3_iaxn86","parent_id":"t3_iaxn86","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rclau\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g5ybip9":{"author":"benji700","author_fullname":"t2_bvgas","author_premium":false,"banned_at_utc":null,"body":"Thanks so much!","created_utc":["2020-08-16","21:50:27"],"id":"g1rdnwq","link_id":"t3_iaxn86","parent_id":"t1_g1rclau","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rdnwq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g5znutt":{"author":"benji700","author_fullname":"t2_bvgas","author_premium":false,"banned_at_utc":null,"body":"How about t2? Are those submissions (original posts) that aren't links?","created_utc":["2020-08-16","23:40:40"],"id":"g1rqgd7","link_id":"t3_iaxn86","parent_id":"t1_g1rclau","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rqgd7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g616eb5":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"per this https:\/\/www.reddit.com\/dev\/api#fullnames it should be an account","created_utc":["2020-08-16","23:58:57"],"id":"g1rsmvj","link_id":"t3_iaxn86","parent_id":"t1_g1rqgd7","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rsmvj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g616lt0":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The `t2_` prefix is for the 'author\\_fullname' field","created_utc":["2020-08-16","23:59:00"],"id":"g1rsn44","link_id":"t3_iaxn86","parent_id":"t1_g1rqgd7","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1rsn44\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g6172fs":{"author":"comradeswitch","author_fullname":"t2_118shh","author_premium":false,"banned_at_utc":null,"body":"The simplest way is to check the parent id against the link id. If they're the same, you have a root comment.","created_utc":["2020-08-17","02:36:43"],"id":"g1s9ml9","link_id":"t3_iaxn86","parent_id":"t3_iaxn86","permalink":"\/r\/pushshift\/comments\/iaxn86\/distinguishing_responses_to_a_comment_vs\/g1s9ml9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcriud1":{"author":"comradeswitch","author_fullname":"t2_118shh","author_premium":false,"banned_at_utc":null,"body":"The fh-bigquery \"fullcorpus\" table timestamped with Dec 2015 has all the data collected prior to that date for posts. You can check the date of the first post in there with \"select timestamp_seconds(min(created_utc)\" or the like. The v2 dataset has separate tables for 2010 and a few years after but I'm not sure what that is- there are posts in the first dataset that aren't in the second and vice versa for the same time period.","created_utc":["2020-08-17","02:42:55"],"id":"g1sa8xo","link_id":"t3_i9iipd","parent_id":"t1_g1g3m1j","permalink":"\/r\/pushshift\/comments\/i9iipd\/bigquery_data_source_for_getting_all_posts_in_a\/g1sa8xo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcs96bd":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Could you post your code?","created_utc":["2020-08-18","07:37:47"],"id":"g1y8jjh","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g1y8jjh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcsfhcc":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"pfff just buy more ram: https:\/\/blog.codinghorror.com\/hardware-is-cheap-programmers-are-expensive\/","created_utc":["2020-08-18","15:12:00"],"id":"g1z21di","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g1z21di\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcu052w":{"author":"HecrouxIdiot","author_fullname":"t2_29hmup0c","author_premium":false,"banned_at_utc":null,"body":"depends on the source, the source file type if its from the file dumps.For zstd, you have no other option (afaik) but to put whole file in memory. for others, you can extract data by chunks.\n\nSince your time frame is not that large, why not use the web api? you can reduce the time taken to scrap for low data rate using threads. I had a similar problem regarding zstd files specifically (the other extensions i figured out) so i just went with the web api, before the rate limit was decreased. \n\nEdit: r\/trees is not a low activity subreddit as i thought would be. Even so, there is a tradeoff w.r.t time and memory comparing web api vs file dump.The time aspect is enhanced by the recent rate limit of 100 records per request(last time I checked)","created_utc":["2020-08-18","20:03:54"],"id":"g206en4","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g206en4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcvwg4z":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"FYI, the API size limit was reduced to 100 results per query last month [due to abuse that impacted system latency](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hncg6q\/has_the_commentsubmission_size_limit_decreased\/fxgx0hz\/).","created_utc":["2020-08-19","05:55:11"],"id":"g229bxk","link_id":"t3_icfhvx","parent_id":"t3_icfhvx","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g229bxk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcvwlz6":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Okay. So if I run a request at limit=1000, is it doing 100x10 for 10 requests?","created_utc":["2020-08-19","06:05:18"],"id":"g22af12","link_id":"t3_icfhvx","parent_id":"t1_g229bxk","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22af12\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw2xtb":{"author":"thottius","author_fullname":"t2_hmbrjf6","author_premium":false,"banned_at_utc":null,"body":"If you\u2019re using push shift directly and not some wrapper I\u2019m pretty sure it\u2019ll just return 100 results\n\nIf that is the case, and you\u2019d still like to get 1000 comments, you\u2019ll have to make a work around for that. If you pm me I can link you to the code I\u2019ve made for that","created_utc":["2020-08-19","06:10:46"],"id":"g22b04b","link_id":"t3_icfhvx","parent_id":"t1_g22af12","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22b04b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw3e4h":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"That's 1 API call right?  Any idea on why it can take 30s-5m?","created_utc":["2020-08-19","06:12:36"],"id":"g22b6yg","link_id":"t3_icfhvx","parent_id":"t1_g22b04b","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22b6yg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw3gsv":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"If you're performing the queries yourself with GET requests from api.pushshift.io, then it will only return 100 results. You would have to handle pagination using the `after` argument. If you're using [PSAW](https:\/\/github.com\/dmarx\/psaw), it should handle pagination for you with 10x requests with 100 results each.","created_utc":["2020-08-19","06:13:57"],"id":"g22bc41","link_id":"t3_icfhvx","parent_id":"t1_g22af12","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22bc41\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw49gk":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I'm using a get request using the before and after arguments.  Before in some point the very distant past(say 1-2 years).  The before argument changes based on the created\\_utc so it moves towards that past date.  I'm just confused as to why it can take a few minutes to get 100 results for 1 api call.  I thought I was going over the limit.","created_utc":["2020-08-19","06:20:12"],"id":"g22c028","link_id":"t3_icfhvx","parent_id":"t1_g22bc41","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22c028\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw4kge":{"author":"thottius","author_fullname":"t2_hmbrjf6","author_premium":false,"banned_at_utc":null,"body":"Yes that\u2019s one call. No reasons in particular come to mind, especially for a the simple query it sounds like you\u2019re making. Save aggregations, the calls I make usually take seconds. Could try loading the query in browser to check if it\u2019s something in your code. If that took the same time for me, I\u2019d start blaming my WiFi, but you may have already checked that.","created_utc":["2020-08-19","06:27:09"],"id":"g22cq1g","link_id":"t3_icfhvx","parent_id":"t1_g22b6yg","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22cq1g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcw5209":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I grabbed the time before the request and the time after and it took 63 seconds.  So i'm not sure.  It may be my wifi as I have issues with packet loss","created_utc":["2020-08-19","06:33:39"],"id":"g22de7m","link_id":"t3_icfhvx","parent_id":"t1_g22cq1g","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22de7m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcwgzwa":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"I've only ever worked from the current to the past by adjusting the `after` argument. I wonder if there's some weird performance hit going from the past to the current.","created_utc":["2020-08-19","06:47:22"],"id":"g22essq","link_id":"t3_icfhvx","parent_id":"t1_g22c028","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g22essq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcyeqeu":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you saying you send a get request and it takes 5 minutes to respond?","created_utc":["2020-08-19","20:55:51"],"id":"g24iqaw","link_id":"t3_icfhvx","parent_id":"t3_icfhvx","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24iqaw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf15idu":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I sent a get request and it took approx 63 seconds.  If i'm doing 10 calls for 1000 posts thats 10 minutes, if a subreddit has 10000 posts thats approx 1hr and a half.","created_utc":["2020-08-19","21:00:25"],"id":"g24jb51","link_id":"t3_icfhvx","parent_id":"t1_g24iqaw","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24jb51\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf28e6d":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That definitely doesn't sound right. Is there anything complicated about the request or are just filtering on `subreddit` and `before`? Are you requesting really old posts?\n\nThe API should just return an error if you're sending requests too fast.","created_utc":["2020-08-19","21:13:46"],"id":"g24l0jy","link_id":"t3_icfhvx","parent_id":"t1_g24jb51","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24l0jy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf31b5x":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Can you run the request with `&amp;metadata=true` and check the `execution_time_milliseconds` field?","created_utc":["2020-08-19","21:20:15"],"id":"g24lw3l","link_id":"t3_icfhvx","parent_id":"t1_g24jb51","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24lw3l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc9csha":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Heres the url,\n\n    after = 1547856000\n    \n    url = \"https:\/\/api.pushshift.io\/reddit\/submission\/search\/?subreddit={}&amp;sort=desc&amp;sort_type=created_utc&amp;size={}&amp;after={}&amp;before={}\".format(subreddit, size, after, befor)\n    \n    response = requests.get(url, headers=headers).json()\n\nThat's roughly Jan 2018 if I remember correctly.  So it will request posts from Jan 2018 onward until it reaches today.  Yeah, sometimes its super quick like 2 seconds and then it just hangs for 1 to 5 minutes.","created_utc":["2020-08-19","21:35:23"],"id":"g24nygu","link_id":"t3_icfhvx","parent_id":"t1_g24l0jy","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24nygu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc9nxz3":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Any particular reason you're using both after and before? I usually just pick a start time, set the before OR after, look at the date of the most recent or oldest post depending on what direction I'm going and use that for the next request. It's definitely possible having both is tripping something up.\n\nEdit: I also don't think you need sort_type, that's the default.","created_utc":["2020-08-19","21:37:43"],"id":"g24o9yr","link_id":"t3_icfhvx","parent_id":"t1_g24nygu","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24o9yr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc9qisb":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"&gt;execution\\_time\\_milliseconds\n\nFor one request with size=100, \n\n    148.99\n\nThe average over 5 requests is,\n\n    201.91363636363633","created_utc":["2020-08-19","22:19:53"],"id":"g24u0gk","link_id":"t3_icfhvx","parent_id":"t1_g24lw3l","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24u0gk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcoemu0":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"&gt;Any particular reason you're using both after and before?\n\nIt was the first way I found when googling how to get all the posts in a subreddit.\n\nWhat is the preferable way?\n\n  \nEdit:\n\nI also provided in another comment, the average execution in milliseconds over 5 requests is 201.91.","created_utc":["2020-08-19","22:22:09"],"id":"g24ucmz","link_id":"t3_icfhvx","parent_id":"t1_g24o9yr","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24ucmz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzq3py":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"[Here's the script I wrote](https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py) to download posts for a user. The url I use is just\n\n    https:\/\/api.pushshift.io\/reddit\/{}\/search?limit=1000&amp;sort=desc&amp;author={}&amp;before=\n\nIt's definitely possible that having both is causing issues with the way pushshift's databases work. Doesn't hurt to just try without it.","created_utc":["2020-08-19","22:26:06"],"id":"g24uxbb","link_id":"t3_icfhvx","parent_id":"t1_g24ucmz","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24uxbb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzqd5z":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"Thanks!  I actually ran a request the below url,\n\n    https:\/\/api.pushshift.io\/reddit\/submission\/search\/?subreddit=music&amp;sort=desc&amp;size=1000&amp;before=1597861083&amp;metadata=true\n\nthe average execution time over 5 requests was 320.47666666666663. Perhaps, when PushShift API gets bogged down thats what can cause the long delays?","created_utc":["2020-08-19","22:35:16"],"id":"g24w4k0","link_id":"t3_icfhvx","parent_id":"t1_g24uxbb","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24w4k0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzqwu5":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"300 milliseconds is like a third of a second. For requests for data that hasn't been requested recently, so the server has to do the lookup rather than pulling it from a cache, that's pretty normal. It wouldn't explain a 30 second response time, much less a 5 minute one.","created_utc":["2020-08-19","22:39:55"],"id":"g24wqc1","link_id":"t3_icfhvx","parent_id":"t1_g24w4k0","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g24wqc1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbzs0z3":{"author":"ScoopJr","author_fullname":"t2_h8aha","author_premium":false,"banned_at_utc":null,"body":"I know when I tracked the time yesterday to receive a response it took 63 seconds.  Today, it took 1-2s which is very good(using time.time()).  Thanks for the code snippet and the info on the api!  I appreciate that brother","created_utc":["2020-08-19","23:26:58"],"id":"g252pb8","link_id":"t3_icfhvx","parent_id":"t1_g24wqc1","permalink":"\/r\/pushshift\/comments\/icfhvx\/speeding_up_api_calls\/g252pb8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0abv5":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"As requested, here's the code I'm using. A work in progress, obviously.\n\nfrom psaw import PushshiftAPIimport csvimport datetime as dt\n\napi = PushshiftAPI()\n\nstart\\_epoch=int(dt.datetime(2012, 11,9).timestamp())\n\nsubreddit = input('Which subreddit would you like to scrape? ')\n\ncorpus = list(api.search\\_submissions(after = start\\_epoch,subreddit = subreddit,limit = 5000))\n\nwith open('D:\/CAMER\/%s.csv' % subreddit, 'w', encoding='utf-8') as csvfile:fieldnames = \\['id', 'title', 'author', 'original content', 'selfpost', 'stickied', 'selftext','time created', 'locked', 'number of comments', 'NSFW', 'permalink', 'score', 'upvote ratio','url'\\]filewriter = csv.DictWriter(csvfile, fieldnames=fieldnames)filewriter.writeheader()\n\nfor i in corpus:\n\nfilewriter.writerow({'id': i.id,'title': i.title,'author': i.author,'original content': i.is\\_original\\_content,'selfpost': i.is\\_self,'time created': i.created\\_utc,'stickied': i.stickied,'locked': i.locked,'NSFW': i.over\\_18,'selftext': i.selftext,#'comment forest': cache,'number of comments': i.num\\_comments,'score': i.score,'upvote ratio': i.upvote\\_ratio,'permalink': i.permalink,'url': i.url})\n\nprint ('We did it!')","created_utc":["2020-08-20","00:44:28"],"id":"g25cacf","link_id":"t3_ibt07s","parent_id":"t3_ibt07s","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25cacf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0bcdv":{"author":"mminich1","author_fullname":"t2_7hjwmn8w","author_premium":false,"banned_at_utc":null,"body":"posted :)","created_utc":["2020-08-20","00:44:43"],"id":"g25cbgm","link_id":"t3_ibt07s","parent_id":"t1_g1y8jjh","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25cbgm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0n8iz":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well 5000 objects shouldn't cause memory errors, but if you're trying to do more, just don't put them in a list. You can just do\n\n    for i in api.search_submissions(after = start_epoch,subreddit = subreddit,limit = 5000):","created_utc":["2020-08-20","00:50:49"],"id":"g25d2nl","link_id":"t3_ibt07s","parent_id":"t1_g25cacf","permalink":"\/r\/pushshift\/comments\/ibt07s\/another_novice_question_any_easy_workarounds_for\/g25d2nl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc0t9s6":{"author":"inspiredby","author_fullname":"t2_5kk2e","author_premium":false,"banned_at_utc":null,"body":"User search still exists in the Pushshift API.\n\nAlso reddit's API is open, it even has RSS if you want to subscribe to a certain user.","created_utc":["2020-08-20","02:07:08"],"id":"g25lt08","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25lt08\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcoio79":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"[here](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?author=AKnightAlone&amp;fields=permalink,url,full_link,link,title,body,selftext,self_text&amp;size=100) are your last 100 comments , using pushshift .\n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.\n\ndo you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","02:10:42"],"id":"g25m7h1","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25m7h1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcp35ax":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You know pushshift isn't run by reddit right? The owner of pushshift has nothing to do with whether subs are banned or not.\n\nIt's actually rather insulting that you would accuse someone who spends thousands of dollars of his own money each month providing this service of being paid off. If he didn't want to do it you would have no way at all of searching for comments.","created_utc":["2020-08-20","02:11:02"],"id":"g25m8t8","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25m8t8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbztm8s":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"Sorry, last 100 comments is accessible by scrolling through my recent pages. What is it? 25 accessible recent pages or so?","created_utc":["2020-08-20","02:27:41"],"id":"g25o4gm","link_id":"t3_icykt5","parent_id":"t1_g25m7h1","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25o4gm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxhi9m":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"I just told you I found someone with 5000 mentions of \"Monsanto\" in 5 years. Are you implying there's no incentive for shills to hide their efforts? Hell, I'd pay this guy *several* thousand if I was specifically Monsanto. Why is it surprising to imagine literally *anyone* with money wanting to obscure Reddit user scrutiny?","created_utc":["2020-08-20","02:30:38"],"id":"g25og92","link_id":"t3_icykt5","parent_id":"t1_g25m8t8","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25og92\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbytlmn":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"Man you can use the create\\_utc of the last comment and ask pushshift for the 100 comments that happened before the 100th comment. You can repeat that with a loop and get all your data .\n\nAlso the 100 results used to be 1000 , and now for some reason (which maybe is temporal) is 100 .\n\nYou did not address the :\n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.  \n&gt;  \n&gt;do you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","02:32:29"],"id":"g25onmk","link_id":"t3_icykt5","parent_id":"t1_g25o4gm","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25onmk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc4zh39":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; do you have any evidence to support your claim or you are just trashing ?\n\nEvidence seems to have become the age old cry of the oppressor. Is \"capitalism\" evidence, or do you consider that a \"conspiracy theory.\" I'm going to guess you simultaneously ignore all the flaws and corrupt incentives of capitalism, *and* you strangely think there's nothing conspiratorial about entire immense parts of society, which possess extreme power, just coincidentally kind of successfully using that to their advantage in every way that's visible *as well* as plenty of ways that aren't legal(as they constantly get caught) or directly obvious on first glance.\n\n&gt;Man you can use the create_utc of the last comment and ask pushshift for the 100 comments that happened before the 100th comment. You can repeat that with a loop and get all your data .\n\nIs this programmer jargon? Okay, let's wait until someone does that again and makes it the equivalent of uBlock Origin so people don't have to keep jumping around to new exploiters.","created_utc":["2020-08-20","02:36:57"],"id":"g25p5kw","link_id":"t3_icykt5","parent_id":"t1_g25onmk","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25p5kw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc5tvwl":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Monsanto really doesn't care about people talking about them on the internet. They certainly don't care enough to pay someone thousands of dollars. And on top of that, the owner of pushshift is a really stand up guy, there's zero chance he would accept money to hide content.\n\nThat's totally aside from the fact that user search wasn't removed. Anyone with a fraction of technical knowledge can still easily download a users entire comment history.\n\nYou sound like a conspiracy theorist.","created_utc":["2020-08-20","02:37:08"],"id":"g25p6ak","link_id":"t3_icykt5","parent_id":"t1_g25og92","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25p6ak\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gll3ve9":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; Monsanto really doesn't care about people talking about them on the internet. They certainly don't care enough to pay someone thousands of dollars.\n\nYou're denying shills? You're denying that PR is *vastly* more important than literally anything a corporation does? \n\nA corporation could directly murder people, and if their PR people convinced everyone it didn't happen, then they wouldn't need to be sued. That's pretty much not even an analogy. That's literally what corporations do. I'm a hemophiliac. Tell me your stance on Bayer and HIV.\n\n&gt;You sound like a conspiracy theorist.\n\nThe \"theory\" part is a little condescending, don't you think?","created_utc":["2020-08-20","02:43:13"],"id":"g25pued","link_id":"t3_icykt5","parent_id":"t1_g25p6ak","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25pued\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc38s34":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Seriously, just remove this post.","created_utc":["2020-08-20","02:52:12"],"id":"g25qug8","link_id":"t3_icykt5","parent_id":"t1_g25lt08","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25qug8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc3f1fk":{"author":"raunakdaga","author_fullname":"t2_obupsj3","author_premium":true,"banned_at_utc":null,"body":"Can this guy be banned mods?","created_utc":["2020-08-20","03:01:12"],"id":"g25rubt","link_id":"t3_icykt5","parent_id":"t3_icykt5","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25rubt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gd875f2":{"author":"dtiftw","author_fullname":"t2_gl20o","author_premium":false,"banned_at_utc":null,"body":"&gt; Hell, I'd pay this guy several thousand if I was specifically Monsanto.\n\nWhy? Do you think that reddit is a target audience for agricultural technology for a company that no longer exists?","created_utc":["2020-08-20","03:01:40"],"id":"g25rw4y","link_id":"t3_icykt5","parent_id":"t1_g25og92","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25rw4y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gd6t859":{"author":"BayesianProtoss","author_fullname":"t2_ut8g0pw","author_premium":false,"banned_at_utc":null,"body":"You just don\u2019t know what you\u2019re doing lol. This dude seriously talking about \u201canalyzing\u201d posts and can\u2019t navigate a basic Json api lmfao","created_utc":["2020-08-20","03:02:43"],"id":"g25s0dr","link_id":"t3_icykt5","parent_id":"t1_g25p5kw","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25s0dr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gd6t94q":{"author":"liaguris","author_fullname":"t2_1ihbqq55","author_premium":false,"banned_at_utc":null,"body":"Regarding your first paragraph I will just post [this](https:\/\/external-content.duckduckgo.com\/iu\/?u=http%3A%2F%2Fwww.taoofcolor.com%2Fwp-content%2Fuploads%2FiStock_000003088605Small.jpg&amp;f=1&amp;nofb=1) picture as an answer.\n\nRegarding your second paragraph : does [this](https:\/\/camas.github.io\/reddit-search\/) make you happy ?\n\nYou still have not addressed this : \n\n&gt;The only reason user search was removed was because powerful people paid to have it removed.  \ndo you have any evidence to support your claim or you are just trashing ?","created_utc":["2020-08-20","03:03:06"],"id":"g25s1vp","link_id":"t3_icykt5","parent_id":"t1_g25p5kw","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25s1vp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbw5qga":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-08-20","03:20:49"],"id":"g25u0pn","link_id":"t3_icykt5","parent_id":"t1_g25rubt","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25u0pn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbw67az":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"Oh, goodness. I've already got you tagged as \"Shill Monsanto.\" Fucking eerie. This is straight up horrifying, actually.\n\nI ***specifically*** called you out *over a year ago* ***because*** of Pushshift search functions: https:\/\/www.reddit.com\/r\/TheseFuckingAccounts\/comments\/bclgr3\/monsanto_shill_accounts_found_hunting_for_new\/\n\nHow did you end up in this thread?","created_utc":["2020-08-20","03:23:27"],"id":"g25ubcf","link_id":"t3_icykt5","parent_id":"t1_g25rw4y","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25ubcf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwdk8j":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"There's a Monsanto defender *literally* in this thread I called out over a year ago specifically because I mentioned Monsanto in here. Is that not a little creepy to you?\n\nAnti-Sanders efforts on your part. Interesting. Runs in line with the pro-corporate idea I'm bringing up.","created_utc":["2020-08-20","03:28:52"],"id":"g25ux2a","link_id":"t3_icykt5","parent_id":"t1_g25s0dr","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25ux2a\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwdq8y":{"author":"AKnightAlone","author_fullname":"t2_4971h","author_premium":false,"banned_at_utc":null,"body":"&gt; The only reason user search was removed was because powerful people paid to have it removed.\n&gt; do you have any evidence to support your claim or you are just trashing ?\n\nOver a year ago, I made this post: https:\/\/www.reddit.com\/r\/TheseFuckingAccounts\/comments\/bclgr3\/monsanto_shill_accounts_found_hunting_for_new\/\n\nIn this thread, I see this: \n\nApparently I can't upload to Imgur now, ***but*** the the same guy I called out within the hour was badgering me in this thread. Is that coincidental to you? I ***specifically*** called him out in that thread ***because*** I used Pushshift to look at words he used in the past, and *coincidentally*, immediately when I say \"Monsanto\" in this thread, he's here and telling me it's no longer a business. Even though I ALSO FUCKING MENTION BAYER, THEIR NEW OWNER, KILLED MY FELLOW MINORITY HEMOPHILIACS.","created_utc":["2020-08-20","03:40:31"],"id":"g25w7rk","link_id":"t3_icykt5","parent_id":"t1_g25s1vp","permalink":"\/r\/pushshift\/comments\/icykt5\/the_only_reason_user_search_was_removed_was\/g25w7rk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwlau6":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"This looks interesting, can you share more information?","created_utc":["2020-08-21","02:52:51"],"id":"g29t9vl","link_id":"t3_icg4px","parent_id":"t3_icg4px","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g29t9vl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbwnhy2":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"The README.md\n\nhttps:\/\/github.com\/Fitzy1293\/redditsfinder\/blob\/master\/README.md\n\nIt outputs a specified users posted in subs sorted by how many times they posted. \n\nEach post and its most important attributes are read in groups of 100 from pushshift, avoiding rate-limiting, then manipulated to be readable pieces of json. That file is located .\/users\/'yourUser'\/all_posts.json","created_utc":["2020-08-21","03:30:52"],"id":"g29xd0y","link_id":"t3_icg4px","parent_id":"t1_g29t9vl","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g29xd0y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxjq68":{"author":"pk12_","author_fullname":"t2_83khuoz","author_premium":false,"banned_at_utc":null,"body":"Very cool","created_utc":["2020-08-21","04:29:42"],"id":"g2a3wsr","link_id":"t3_icg4px","parent_id":"t1_g29xd0y","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g2a3wsr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxjt7n":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"Thanks","created_utc":["2020-08-21","04:58:29"],"id":"g2a71uc","link_id":"t3_icg4px","parent_id":"t1_g2a3wsr","permalink":"\/r\/pushshift\/comments\/icg4px\/added_nice_terminal_output_boxes_for_my_pushshift\/g2a71uc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbxvbr6":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I've always used `limit`, but I imagine that `size` is just an alias.\n\nThe max limit was changed to 100 sometime in the last few months.","created_utc":["2020-08-26","23:49:22"],"id":"g2y8t69","link_id":"t3_ih66b8","parent_id":"t3_ih66b8","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2y8t69\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyjts1":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Ah. That was very confusing\/frustrating. The [docs on github should be  updated](https:\/\/github.com\/pushshift\/api). \n\nAny suggestions on how to get more than a 100? (I've been learning\/testing with 5-100 the past week and was finally ready to grab a couple hundred for analysis.) \nCan we pay to remove limits? (My project is a small one-off one.)\nPerhaps someone could share their Python function that does multiple pulls over a date range by breaking them up into smaller contiguous requests?","created_utc":["2020-08-27","01:16:18"],"id":"g2yjrff","link_id":"t3_ih66b8","parent_id":"t1_g2y8t69","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2yjrff\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbynchp":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You just take the created_utc timestamp of the last object and use it with the before parameter.\n\nI have a script [here](https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py) that downloads all of a users posts\/comments which has an example.","created_utc":["2020-08-27","01:42:59"],"id":"g2ymx66","link_id":"t3_ih66b8","parent_id":"t1_g2yjrff","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g2ymx66\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbynni9":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Thanks. I added `collect_pushshift_results()` to [my own code](https:\/\/github.com\/reagle\/reddit\/blob\/master\/reddit-query.py#L158) and it seems to be working.","created_utc":["2020-08-27","20:21:24"],"id":"g31iv6i","link_id":"t3_ih66b8","parent_id":"t1_g2ymx66","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/g31iv6i\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyo6fi":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"It's helpful to start with some clarification of terms. There are separate endpoints for `comment` and `submission`. Both of your calls are to the `comment` endpoint. \n\nA better approach would be to use the `submission` endpoint with iterating before\/after arguments to get all submissions from the subreddit, then gather comments using the appropriate endpoint and a `link_id` argument","created_utc":["2020-08-27","23:55:07"],"id":"g32bn4t","link_id":"t3_ihslxi","parent_id":"t3_ihslxi","permalink":"\/r\/pushshift\/comments\/ihslxi\/trying_to_scrape_all_comments_from_a_subreddit\/g32bn4t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbyqckd":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I doubt anyone can help you without seeing your code.","created_utc":["2020-08-28","02:02:27"],"id":"g32qzt6","link_id":"t3_ihv2qw","parent_id":"t3_ihv2qw","permalink":"\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/g32qzt6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbz0t68":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That was two months ago right? It's certainly possible for pushshift to collect data, but something goes wrong with the database and it's never inserted. So it could fall way behind, then catch up instantly.\n\nSomething like that happened just a few days ago, [here's a graph](https:\/\/i.imgur.com\/RoHgEpq.png). You can see the normal daily lag bumps and then a big spike in the middle where it fell behind.","created_utc":["2020-08-28","03:03:06"],"id":"g32xooj","link_id":"t3_ihw6r2","parent_id":"t3_ihw6r2","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g32xooj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbz2vd8":{"author":"jmreagle","author_fullname":"t2_a6ftd","author_premium":false,"banned_at_utc":null,"body":"It could be that the limit went from 1000, 500, 100 per request over the past year. (Just learned this myself!)","created_utc":["2020-08-28","06:08:36"],"id":"g33hvwf","link_id":"t3_ihv2qw","parent_id":"t3_ihv2qw","permalink":"\/r\/pushshift\/comments\/ihv2qw\/cannot_get_all_submissions_that_i_am_looking_for\/g33hvwf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc1nk2g":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Yes, from months ago, and thank you for the graph, which confirms that pattern isn't unusual.","created_utc":["2020-08-28","14:39:11"],"id":"g34hx9l","link_id":"t3_ihw6r2","parent_id":"t1_g32xooj","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g34hx9l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc20whp":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"`{i}_{o}`","created_utc":["2020-08-28","17:01:08"],"id":"g34v08q","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34v08q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc2qi1q":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"I'm not sure what's up with your code, but just wanted to say that you can make your life immeasurably easier by using the PSAW package to scrape Pushshift with python.","created_utc":["2020-08-28","17:03:20"],"id":"g34v9m7","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34v9m7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc39fil":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Pushshift collects posts pretty close to real time so the score that shows up is the score that existed very soon after the post was made.  Therefore most scores will be very low and do not refelect the actual score of a post over time.\n\nIf you need an accurate score I recommend combining Pushshift with PRAW, which will in fact go get you the most up to date data, but tends to be a little slower.  Example here:\n\n[https:\/\/github.com\/dmarx\/psaw](https:\/\/github.com\/dmarx\/psaw)","created_utc":["2020-08-28","17:06:24"],"id":"g34vmgv","link_id":"t3_ii69a9","parent_id":"t3_ii69a9","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g34vmgv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc39nq2":{"author":"DailyGoofy","author_fullname":"t2_7vtx29y3","author_premium":false,"banned_at_utc":null,"body":"it just creates the csv files: bitcoin\\_0.csv, bitcoin\\_1.csv, ...","created_utc":["2020-08-28","17:28:35"],"id":"g34ybzv","link_id":"t3_ii789u","parent_id":"t1_g34v08q","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g34ybzv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gc3b9b4":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"Apart from potentially creating 15k files, what wrong is that you don't respect rate limits and throwing away exceptions","created_utc":["2020-08-28","18:07:01"],"id":"g35345c","link_id":"t3_ii789u","parent_id":"t1_g34ybzv","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g35345c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcaj67g":{"author":"DailyGoofy","author_fullname":"t2_7vtx29y3","author_premium":false,"banned_at_utc":null,"body":"ah okay, do i need to use something like time.sleep(1) to make sure that not too many requests are send per minute?","created_utc":["2020-08-28","18:12:31"],"id":"g353rz0","link_id":"t3_ii789u","parent_id":"t1_g35345c","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g353rz0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gcakdnh":{"author":"VR_DEVELOPER","author_fullname":"t2_11ak44","author_premium":false,"banned_at_utc":null,"body":"better yet, use PSAW it will handle things like this for you","created_utc":["2020-08-28","18:19:43"],"id":"g354n6g","link_id":"t3_ii789u","parent_id":"t1_g353rz0","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g354n6g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbprq8d":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"Hi @Vivd_Walrus, as just [discussed here](https:\/\/www.reddit.com\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/), elapsed time between created_utc and retrieved_utc ranges from 0--24 hours over the course of weeks. But that's not my concern here.\n\nI simply don't understand how Pushshift can report 137 comments at time of ingestion and a score of 1. (That's not the only case, just one.)","created_utc":["2020-08-28","18:48:30"],"id":"g3586rq","link_id":"t3_ii69a9","parent_id":"t1_g34vmgv","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g3586rq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpscr6":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well pushshift ingests the comments too. Maybe it's updating the comment count on the post itself.","created_utc":["2020-08-28","19:27:12"],"id":"g35d0xi","link_id":"t3_ii69a9","parent_id":"t1_g3586rq","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35d0xi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpt0gh":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-08-28","19:55:53"],"id":"g35gp14","link_id":"t3_ii69a9","parent_id":"t1_g35d0xi","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35gp14\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbptq0g":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"That would make sense. \n\n1. PS sees a submission, ingests it along with author, title, body_text, score, num_comments, etc.\n2. As comments come in, it store those and update their parent's num_comments but does not revisit the initial score or if the author or score have changed.\n\nI wonder if it is true? I've been surprised that num_counts on push_shift is so close to actual counts on Reddit.","created_utc":["2020-08-28","19:56:52"],"id":"g35gtlw","link_id":"t3_ii69a9","parent_id":"t1_g35d0xi","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35gtlw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbpuetu":{"author":"theoddjosh","author_fullname":"t2_3r7pv","author_premium":false,"banned_at_utc":null,"body":"Can you explain more about how combining Pushshift with PRAW can get more updated data?\n\nI've been using Pushshift to retrieve some submission post selftext data, but oftentimes I see submissions as [removed] (because Pushshift took it's snapshot very quickly, and those particular posts were caught in the Automod filter and removed). The posts were later approved but since pushshift took its snapshot already the selftext data was never updated.","created_utc":["2020-08-28","20:36:33"],"id":"g35lwik","link_id":"t3_ii69a9","parent_id":"t1_g34vmgv","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g35lwik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbqv8d0":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Seconded VR_DEVELOPER. The PSAW library automatically handles rate limiting, back offs, and retries.","created_utc":["2020-08-28","20:37:54"],"id":"g35m2rs","link_id":"t3_ii789u","parent_id":"t1_g353rz0","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g35m2rs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbqvt96":{"author":"reagle-research","author_fullname":"t2_6m1rw9ui","author_premium":false,"banned_at_utc":null,"body":"You'd use the ID found from pushshift and query reddit itself. See [my code here](https:\/\/github.com\/reagle\/reddit\/blob\/master\/reddit-query.py).","created_utc":["2020-08-29","02:02:33"],"id":"g36pz4k","link_id":"t3_ii69a9","parent_id":"t1_g35lwik","permalink":"\/r\/pushshift\/comments\/ii69a9\/why_does_score_bottom_out_but_not_num_comments\/g36pz4k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu3nm0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"They just haven't been ingested yet. The process that ingests and creates the files is separate from the one that puts them in the database that backs the API.\n\n\/u\/Stuck_In_the_Matrix is working on catching everything up, but no timeline yet.","created_utc":["2020-08-29","02:52:02"],"id":"g36vaey","link_id":"t3_iihqfn","parent_id":"t3_iihqfn","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g36vaey\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu3sjp":{"author":"mct1","author_fullname":"t2_4o6k6","author_premium":false,"banned_at_utc":null,"body":"I figured it was something like that. Danke, good sir.","created_utc":["2020-08-29","05:07:37"],"id":"g37913u","link_id":"t3_iihqfn","parent_id":"t1_g36vaey","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g37913u\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu56jm":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Sweet chart! Is there any way to share a live view?","created_utc":["2020-08-29","06:18:30"],"id":"g37fwa1","link_id":"t3_ihw6r2","parent_id":"t1_g32xooj","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37fwa1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6av4":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Unfortunately not. It's a private grafana server I use to monitor my various bots.","created_utc":["2020-08-29","06:27:50"],"id":"g37grwq","link_id":"t3_ihw6r2","parent_id":"t1_g37fwa1","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37grwq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6kdf":{"author":"Administrative-Two41","author_fullname":"t2_71u2jyl3","author_premium":false,"banned_at_utc":null,"body":"Your try-except area is pretty dangerous. You're catching any and all errors as a 'Finished Crawling' behaviour. If, like someone else says, you hit a rate-limit exception, you will crash out, but not know why. You'll want something like this:\n\n     for subreddit in subreddits:\n        for page in pages:\n            try:\n                response = requests.get(```your url```)\n            except Exception as err:\n                if &lt;specific error that is exit condition&gt; in str(err):\n                    print(\"Done\")\n                    break\n                else:\n                    print('Failure on page %d of subreddit %s\" % (page, subreddit)\n                    print('With error: %s) % (err))\n    \n    You can then use something like this to get around sleep conditions:\n    try:\n        data = requests.get(```theurl```)\n    except Exception as error:\n        if str(END_CONDITON) in error:\n            print(\"Done\")\n            break\n        else if str(RATE_LIMIT_ERROR) in error:\n            sleep(DELAY_INTERVAL)\n        else:\n            print(error)","created_utc":["2020-08-29","07:19:21"],"id":"g37lcdi","link_id":"t3_ii789u","parent_id":"t3_ii789u","permalink":"\/r\/pushshift\/comments\/ii789u\/pushshift_does_not_work_like_it_used_to_what_am_i\/g37lcdi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu6wy5":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Cloudflare can cache private servers, or certain routes (including ones that return json), with very little setup. The server would only get one hit every 2 hours. And if you still want a live view for yourself you can make that separate.","created_utc":["2020-08-29","10:07:31"],"id":"g37xj3d","link_id":"t3_ihw6r2","parent_id":"t1_g37grwq","permalink":"\/r\/pushshift\/comments\/ihw6r2\/why_is_there_weird_pattern_in_when_pushshift\/g37xj3d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbu7gi9":{"author":"Sonoff","author_fullname":"t2_bkv9dju","author_premium":false,"banned_at_utc":null,"body":"I am sure that releasing these dumps would be a relief for the API, I am currently calling the API so many times to get 2020 comments whereas it would be one query on BigQuery... just saying ! u\/Stuck_In_the_Matrix","created_utc":["2020-08-29","11:00:12"],"id":"g380pfq","link_id":"t3_iihqfn","parent_id":"t3_iihqfn","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g380pfq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbubvjq":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"How long does the second script take to complete? It should take literally hours to finish.\n\nWithout seeing any output, I'm guessing it's hitting an error from the reddit api at some point and crashing.","created_utc":["2020-08-29","21:17:35"],"id":"g39kfw4","link_id":"t3_iitwp3","parent_id":"t3_iitwp3","permalink":"\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/g39kfw4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbufuif":{"author":"TrAWei09","author_fullname":"t2_59rpqnzb","author_premium":false,"banned_at_utc":null,"body":"It took a while to finish. Maybe it was just a crapshoot - Im trying the same for r\/coolguides for example. The .pickle file is 100MB large, the .txt file is reaching 70Mb and still running. Maybe the attempt with r\/pewdiepiesubmissions was just a fail?","created_utc":["2020-08-29","21:32:54"],"id":"g39m808","link_id":"t3_iitwp3","parent_id":"t1_g39kfw4","permalink":"\/r\/pushshift\/comments\/iitwp3\/getting_all_comments_from_submission_low_file_size\/g39m808\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbugmkw":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":true,"banned_at_utc":null,"body":"Not by the API, you can request SITM to do it but he seems pretty far behind in general at the moment.\n\nLike IIUC the two big things right now are the ingest rewrite so it's not bogged down by spam anymore and removals which seem to be increasing in volume and there isn't an automated way to handle them setup yet.","created_utc":["2020-08-31","23:45:07"],"id":"g3iikdf","link_id":"t3_ik3x1w","parent_id":"t3_ik3x1w","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3iikdf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbunx37":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"I guess I can modify my script so it manually gets the PRAW data for my subreddit instead.","created_utc":["2020-09-01","00:22:15"],"id":"g3ink91","link_id":"t3_ik3x1w","parent_id":"t1_g3iikdf","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3ink91\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gbtzjo5":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":true,"banned_at_utc":null,"body":"SITM has done it in the past with quarantined subs and stuff but he was lot more active on the sub at the time so YMMV.\n\nIt can be done I just think it's unlikely until after the ingest rewrite is complete which was expected to be complete months ago and i've not seen anything about lately.","created_utc":["2020-09-01","00:31:45"],"id":"g3iovwd","link_id":"t3_ik3x1w","parent_id":"t1_g3ink91","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3iovwd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"garfjz8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Pretty sure the ingest rewrite is done, he just hasn't updated the main ingest to use it yet.","created_utc":["2020-09-01","05:37:19"],"id":"g3joeo2","link_id":"t3_ik3x1w","parent_id":"t1_g3iikdf","permalink":"\/r\/pushshift\/comments\/ik3x1w\/is_there_anyway_i_can_cause_pushshift_to_ingest\/g3joeo2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"garfojt":{"author":"douglasg14b","author_fullname":"t2_535da","author_premium":false,"banned_at_utc":null,"body":"Wait, so all the comments exist in the database, but not in exported files?\n\nI would think exporting would just be streaming a query to a JSON transform?\n\nSure it would be intensive, but even a modest\/slow ETL script can process a few hundred thousand records a second. Decent ones can do a few million records\/s.\n\nIs there a problem of some sort going on? I was just about to start a large analytical project based on reddit to prove\/disprove a few thoeries, and post it publicly, but with 8 months of recent data missing that's pretty much pointless.","created_utc":["2020-09-01","07:02:36"],"id":"g3jx02z","link_id":"t3_iihqfn","parent_id":"t1_g36vaey","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3jx02z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gari2te":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"There's a number of reasons that \/u\/Stuck_In_the_Matrix would want to reingest the data rather than just pulling it from the database. The biggest one is that things are often updated\/deleted in the first few days after being created, so the database doesn't have the most up to date version. Reingesting gives a more accurate copy of the data.","created_utc":["2020-09-01","07:23:11"],"id":"g3jytmk","link_id":"t3_iihqfn","parent_id":"t1_g3jx02z","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3jytmk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gapuua5":{"author":"douglasg14b","author_fullname":"t2_535da","author_premium":false,"banned_at_utc":null,"body":"Ah, gotcha.","created_utc":["2020-09-01","08:57:51"],"id":"g3k60u6","link_id":"t3_iihqfn","parent_id":"t1_g3jytmk","permalink":"\/r\/pushshift\/comments\/iihqfn\/have_comments_been_ingested_for_2020\/g3k60u6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gapv4xw":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"&gt;Why does removeddit not show any comments or posts that are under one hour old?\n\npushshift was probably behind again, it does that lately due to high volumes of spam on reddit exceeding it's ingest capacity.\n\n&gt;How long must a comment be present\n\nLong enough for pushshift to see it which is typically only a couple seconds but as above when it gets behind it could be several hours.","created_utc":["2020-09-01","17:57:20"],"id":"g3lg5jj","link_id":"t3_ikh5cn","parent_id":"t3_ikh5cn","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3lg5jj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaq03pr":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Hi, you can try [reveddit](https:\/\/www.reveddit.com). It's a fork of removeddit that's been completely rewritten, and it will also tell you when Pushshift is behind,\n\nhttps:\/\/www.reveddit.com\/info\/","created_utc":["2020-09-02","08:45:04"],"id":"g3orgrm","link_id":"t3_ikh5cn","parent_id":"t3_ikh5cn","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3orgrm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaq1jz9":{"author":"Unrealist99","author_fullname":"t2_2r8u5pzo","author_premium":true,"banned_at_utc":null,"body":"Thanks! I'll check it out.","created_utc":["2020-09-03","10:35:49"],"id":"g3tjdxv","link_id":"t3_ikh5cn","parent_id":"t1_g3orgrm","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3tjdxv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gar4yxl":{"author":"Unrealist99","author_fullname":"t2_2r8u5pzo","author_premium":true,"banned_at_utc":null,"body":"So it's variable for any comment. Thought there would be a minimum fixed timeframe but that's not the case then.","created_utc":["2020-09-03","10:36:52"],"id":"g3tjg2f","link_id":"t3_ikh5cn","parent_id":"t1_g3lg5jj","permalink":"\/r\/pushshift\/comments\/ikh5cn\/doubts_about_removeddit\/g3tjg2f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gasqyz5":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"No one can help you without seeing your code.","created_utc":["2020-09-20","12:02:21"],"id":"g5ybip9","link_id":"t3_iw81i2","parent_id":"t3_iw81i2","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g5ybip9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gavqodf":{"author":"QLZX","author_fullname":"t2_5knmwlnx","author_premium":false,"banned_at_utc":null,"body":"This was more of a general question. I don\u2019t wanna spend time trying to fix it if there\u2019s a cap to how many results you can get","created_utc":["2020-09-20","18:32:14"],"id":"g5znutt","link_id":"t3_iw81i2","parent_id":"t1_g5ybip9","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g5znutt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6teyy":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"Here's a video of what it does \n\nhttps:\/\/streamable.com\/d6laxg","created_utc":["2020-09-20","19:50:00"],"id":"g5zy37b","link_id":"t3_iwegfs","parent_id":"t3_iwegfs","permalink":"\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/g5zy37b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6tqfm":{"author":"f_k_a_g_n","author_fullname":"t2_5wrff1n","author_premium":false,"banned_at_utc":null,"body":"Looks good! Thanks for sharing.","created_utc":["2020-09-20","20:13:31"],"id":"g600uuk","link_id":"t3_iwegfs","parent_id":"t1_g5zy37b","permalink":"\/r\/pushshift\/comments\/iwegfs\/made_my_pushshift_program_have_very_handy\/g600uuk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6tw4z":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"The default is 25. You have to include a `size` argument in your calls. The current size limit is 100.","created_utc":["2020-09-21","00:29:20"],"id":"g616eb5","link_id":"t3_iw81i2","parent_id":"t1_g5znutt","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g616eb5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6u6zs":{"author":"QLZX","author_fullname":"t2_5knmwlnx","author_premium":false,"banned_at_utc":null,"body":"Oh, that\u2019s an issue for getting all comments from the last day\n\nThank you, though","created_utc":["2020-09-21","00:30:33"],"id":"g616lt0","link_id":"t3_iw81i2","parent_id":"t1_g616eb5","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g616lt0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6uldu":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"You'll have to make multiple time-specific calls to get all comments from within the last day (depending on the sub) using `before` with the last timestamp from the previous batch of data. There's another way to do it without a timestamp but I don't remember the method off the top of my head, sorry.","created_utc":["2020-09-21","00:32:57"],"id":"g6172fs","link_id":"t3_iw81i2","parent_id":"t1_g616lt0","permalink":"\/r\/pushshift\/comments\/iw81i2\/can_you_only_search_for_25_comments_at_a_time\/g6172fs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6uo5k":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Increase your chunk size. That\u2019s too few chunks, which itself can generate a memory error.","created_utc":["2020-09-29","20:07:10"],"id":"g72xqgp","link_id":"t3_j1x4uw","parent_id":"t3_j1x4uw","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g72xqgp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6v4ib":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Thanks, I would have never thought of that!","created_utc":["2020-09-29","20:14:26"],"id":"g72yqhx","link_id":"t3_j1x4uw","parent_id":"t1_g72xqgp","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g72yqhx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6x32e":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Chunksize is probably in bytes, not lines. So 100 bytes is likely way less than a single line.\n\nThere are a few really big comments out there, I usually set my chunk size to like 16 megabytes, which is `16,777,216` bytes.","created_utc":["2020-09-29","21:56:56"],"id":"g73c5tf","link_id":"t3_j1x4uw","parent_id":"t3_j1x4uw","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g73c5tf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga6yg74":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"This totally makes sense.   I was definitely thinking of it in terms of lines.  Thanks.","created_utc":["2020-09-29","23:04:54"],"id":"g73lih0","link_id":"t3_j1x4uw","parent_id":"t1_g73c5tf","permalink":"\/r\/pushshift\/comments\/j1x4uw\/odd_problem_with_pushshift_archive_files\/g73lih0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gaaf61i":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It's probably 3-4 terrabytes at most.\n\nBut extracting is cheap, all my scripts that use it just stream the compressed file and extract as I go. Even if you're exporting to some form of database, there's probably a way to compress it there.","created_utc":["2020-10-15","00:25:07"],"id":"g8u4vmx","link_id":"t3_jb8ozw","parent_id":"t3_jb8ozw","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8u4vmx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3lg5jj":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Is there any way to extract from those compressed files in chunks?","created_utc":["2020-10-15","00:59:02"],"id":"g8u8o2c","link_id":"t3_jb8ozw","parent_id":"t1_g8u4vmx","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8u8o2c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3orgrm":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"In theory yes, you could skip forward in the archive and directly pull out a certain day\/hour. But it might take a bit to find, since there's no way to know where exactly a certain timespan is other than to jump around till you find it. It is chronological, but the different days could have different volumes of comments.\n\nPlus that's not something I've ever done before.","created_utc":["2020-10-15","01:12:39"],"id":"g8ua5ty","link_id":"t3_jb8ozw","parent_id":"t1_g8u8o2c","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ua5ty\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3tjdxv":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"That's an interesting idea.  I hadn't thought of that.  I was more wondering what the best way was to iterate through those huge files in chunks of say 100,000 posts at a time.  I'm just not familiar with how to pull items out of a compressed file in chunks via python.","created_utc":["2020-10-15","02:59:46"],"id":"g8ulmvd","link_id":"t3_jb8ozw","parent_id":"t1_g8ua5ty","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ulmvd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3tjg2f":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"\nCan you direct me to how to do it? I\u2019m using a variation of this python file I showed here https:\/\/reddit.com\/r\/pushshift\/comments\/j9qx5f\/filter_monthly_comments_data_into_individual\/\n\n, but still unable to process the file.\n\nI have a laptop with 16GB RAM, which I believe is choking at the decompression step itself. Is there a way to stream the decompressed file to my python program?","created_utc":["2020-10-15","03:44:44"],"id":"g8uqb9y","link_id":"t3_jb8ozw","parent_id":"t1_g8u4vmx","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uqb9y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9olwfp":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-10-15","03:48:55"],"id":"g8uqqxe","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uqqxe\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9omalz":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Try this,\n\nhttps:\/\/github.com\/reveddit\/ragger\/blob\/242cd3461c7aadcfdb6f6b5a60a24a44178f4051\/pushshift_file_reader_writer.py#L21\n\nIt can handle `.xz`, `.bz`, `.gz` and `.zst`\n\ncc \/u\/swapripper \/u\/MakeYourMarks","created_utc":["2020-10-15","03:51:33"],"id":"g8ur10q","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ur10q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9oongy":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I'll give this a run tonight, thanks!","created_utc":["2020-10-15","03:52:58"],"id":"g8ur6c0","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8ur6c0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9opql1":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-10-15","04:29:22"],"id":"g8uux2a","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uux2a\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9oreuz":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The pushshift author has an example [here](https:\/\/github.com\/pushshift\/zreader). It doesn't look like that example you posted does any decompression at all, it's expecting already decompressed files.","created_utc":["2020-10-15","04:35:29"],"id":"g8uvjct","link_id":"t3_jb8ozw","parent_id":"t1_g8uqb9y","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8uvjct\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9ortlg":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"Thanks!!","created_utc":["2020-10-15","06:31:54"],"id":"g8v6s8o","link_id":"t3_jb8ozw","parent_id":"t1_g8ur10q","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8v6s8o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g9os6ew":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"Thank you. I\u2019ll give it a try.","created_utc":["2020-10-15","06:33:06"],"id":"g8v6vz9","link_id":"t3_jb8ozw","parent_id":"t1_g8uvjct","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8v6vz9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0q9yp":{"author":"immibis","author_fullname":"t2_dj2ua","author_premium":false,"banned_at_utc":null,"body":"Decompress each one and recompress them in chunks of 100000 posts","created_utc":["2020-10-15","23:09:04"],"id":"g8xlc79","link_id":"t3_jb8ozw","parent_id":"t1_g8ulmvd","permalink":"\/r\/pushshift\/comments\/jb8ozw\/how_much_data_is_there_in_all_of_the_unzipped\/g8xlc79\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0uwdu":{"author":"swapripper","author_fullname":"t2_g4v8h","author_premium":false,"banned_at_utc":null,"body":"This is great! Do you plan to write in depth covering the tech stack, major changes in this version and the technical limitations driving them. \n\nLearning from this massive scale of a project would be immensely helpful to the developer community. Would really like to see a blog\/article covering Pushshift ingestion, processing &amp; serving layers.","created_utc":["2020-10-16","07:41:10"],"id":"g8z24n2","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g8z24n2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0v8cm":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Seems to be an hour behind already.  :\/\n\nE: oh, maybe not.  The main pushshift.io page isn't using this yet, apparently.","created_utc":["2020-10-17","00:13:42"],"id":"g91k0ik","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g91k0ik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vb0n":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"I'm guessing this is not yet finalized since there are two endpoints for reddit comments,\n\n* Search Reddit Comments - (Elastic DB)\n* Search Reddit Db Comments - ?\n\nWill these be consolidated into one client-facing API? Or, are there advantages to querying one over the other?","created_utc":["2020-10-17","07:09:10"],"id":"g92n8hh","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g92n8hh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vs6o":{"author":"rhaksw","author_fullname":"t2_23j8dbfp","author_premium":false,"banned_at_utc":null,"body":"Setting size too large no longer gracefully responds,\n\n* old way: https:\/\/api.pushshift.io\/reddit\/comment\/search\/?size=999\n * returns 100 items\n* new way: https:\/\/beta.pushshift.io\/search\/reddit\/comments?size=251\n * `\"msg\": \"ensure this value is less than or equal to 250\"`\n\nSo for the beta, if you write code to request 250 items and then Pushshift later lowers this to 200, your script will break.","created_utc":["2020-10-17","07:34:19"],"id":"g92p90k","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/g92p90k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0vx2c":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"For the file downloads? I'm pretty sure it's 5 concurrent connections rather than requests per second. So if you're still downloading other files you have to wait for them to complete.\n\nAlso if you're downloading lots of files, consider [donating](https:\/\/pushshift.io\/donations\/) to stuck_in_the_matrix, he pays for the bandwidth out of pocket.","created_utc":["2020-10-22","23:02:26"],"id":"g9olwfp","link_id":"t3_jg6x0n","parent_id":"t3_jg6x0n","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9olwfp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga0wgsu":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Well the problem is I'm getting 429 errors even though I'm not downloading files. I'm running wget as follows:\n\n    wget -m -np -R \"index.html,*.js,*.css\" --wait 2 --random-wait https:\/\/files.pushshift.io\/reddit\/\n\nEven though I already have a full copy and no downloading is being done (yet), I'm still getting tons of \"429 Too Many Requests\" errors. It seems likely there's some per second request limit, because I don't hit it immediately.\n\nAlso, I'm already donating, that's the first thing I did when I started mirroring his files. That's the least I can do.","created_utc":["2020-10-22","23:05:16"],"id":"g9omalz","link_id":"t3_jg6x0n","parent_id":"t1_g9olwfp","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9omalz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga1qll0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Ah, you're trying to get the index of files. If I was doing this, I would definitely slow way down from once a second. You could easily wait a minute or more between requests.\n\nIs there a specific reason you need to check that fast?","created_utc":["2020-10-22","23:21:27"],"id":"g9oongy","link_id":"t3_jg6x0n","parent_id":"t1_g9omalz","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9oongy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga811no":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Well I'm using wget in mirror mode - it doesn't give much flexibility on how often requests are done. If I slow it down to a minute between each request, it'll take 2 days to run wget in mirror mode (it does a request for every file with the \"If-Modified-Since\" header). Is there any custom tooling to mirror the files that'd work better? FTP or rsync access would be ideal, but I doubt that exists.","created_utc":["2020-10-22","23:29:03"],"id":"g9opql1","link_id":"t3_jg6x0n","parent_id":"t1_g9oongy","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9opql1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga815eh":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Ah, I wasn't familiar with that.\n\nI can't think of any better way other than to write something custom yourself. You could try DM'ing stuck_in_the_matrix on twitter and asking about FTP access. It really seems like something that would be easy for him to set up.","created_utc":["2020-10-22","23:41:12"],"id":"g9oreuz","link_id":"t3_jg6x0n","parent_id":"t1_g9opql1","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9oreuz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ga8kiwi":{"author":"jbondhus","author_fullname":"t2_7fwt6","author_premium":true,"banned_at_utc":null,"body":"Why on twitter specifically? Does he prefer that over reddit PMs?","created_utc":["2020-10-22","23:44:06"],"id":"g9ortlg","link_id":"t3_jg6x0n","parent_id":"t1_g9oreuz","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9ortlg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g36vaey":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"He definitely checks his twitter dms more often than his reddit ones.","created_utc":["2020-10-22","23:46:42"],"id":"g9os6ew","link_id":"t3_jg6x0n","parent_id":"t1_g9ortlg","permalink":"\/r\/pushshift\/comments\/jg6x0n\/what_is_filespushshiftios_request_limit\/g9os6ew\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37913u":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"I think the parameter you\u2019re looking for is is_self. It accepts a Boolean. https:\/\/pushshift.io\/api-parameters\/","created_utc":["2020-10-25","08:40:51"],"id":"ga0q9yp","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0q9yp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g380pfq":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"I thought that was to eliminate cross posts","created_utc":["2020-10-25","09:16:24"],"id":"ga0uwdu","link_id":"t3_jhawh5","parent_id":"t1_ga0q9yp","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0uwdu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3jx02z":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe I\u2019m not understanding your question? I think is_self pulled posts that had a body to them. If is_self is marked as yes, our will only posts that are self.subreddit posts and not include posts that links to external sources whether a cross post or not. I think.","created_utc":["2020-10-25","09:19:32"],"id":"ga0v8cm","link_id":"t3_jhawh5","parent_id":"t1_ga0uwdu","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0v8cm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3jytmk":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"What about photo posts with no body?","created_utc":["2020-10-25","09:20:16"],"id":"ga0vb0n","link_id":"t3_jhawh5","parent_id":"t1_ga0v8cm","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vb0n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3k60u6":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"I think it will ignore pictures too. Take a look at [this](https:\/\/imgur.com\/a\/njZbGsp). See how the top post says (i.redd.it) and the bottom post says (self.puzzles)? I think the is_self parameter will only pull the self.[subreddit] posts, which typically don't include pictures to my knowledge.","created_utc":["2020-10-25","09:24:21"],"id":"ga0vs6o","link_id":"t3_jhawh5","parent_id":"t1_ga0vb0n","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vs6o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3iikdf":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe just give it a shot and see if it accomplishes what you want? Also, if you have an example of a post that is a picture but is a self.post, shoot a link here and someone can take a look.","created_utc":["2020-10-25","09:25:28"],"id":"ga0vx2c","link_id":"t3_jhawh5","parent_id":"t1_ga0vs6o","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0vx2c\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3ink91":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"However, upon further investigation, if someone submits a text post with only a title and no body (which I guess is allowed in some subreddits like \/r\/AskReddit), it will pull that as a self_post even though there is no body.\n\nIn that case, you could probably use the selftext parameter to indicate you only want non empty selftext posts. That *might* not be something you can do with the API, but you could build it into a function.","created_utc":["2020-10-25","09:30:23"],"id":"ga0wgsu","link_id":"t3_jhawh5","parent_id":"t1_ga0vb0n","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga0wgsu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3iovwd":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"If that\u2019s what this property does then it needs to be more clearly documented. The current description for is_self makes no sense to someone looking at the api doc for the first time.","created_utc":["2020-10-25","14:04:56"],"id":"ga1qll0","link_id":"t3_jhawh5","parent_id":"t1_ga0wgsu","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga1qll0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3joeo2":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you talking about the file dumps or the API? That schedule page definitely isn't reliable.","created_utc":["2020-10-26","20:01:55"],"id":"ga6teyy","link_id":"t3_jiioav","parent_id":"t3_jiioav","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6teyy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g39kfw4":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"the api","created_utc":["2020-10-26","20:04:25"],"id":"ga6tqfm","link_id":"t3_jiioav","parent_id":"t1_ga6teyy","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6tqfm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g39m808":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The API is usually pretty up to date. Right now it's about 30 minutes behind real time, but that varies depending on the time of day.","created_utc":["2020-10-26","20:05:41"],"id":"ga6tw4z","link_id":"t3_jiioav","parent_id":"t1_ga6tqfm","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6tw4z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34v08q":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"The page with the monthly archive files is quite far behind (Apr 2020 for submissions, Dec 2019 for comments) but the actual Pushshift API tends to be somewhere between near real-time and several hours behind.","created_utc":["2020-10-26","20:08:08"],"id":"ga6u6zs","link_id":"t3_jiioav","parent_id":"t3_jiioav","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6u6zs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34v9m7":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"oh wow that's cool, thank you very much","created_utc":["2020-10-26","20:11:22"],"id":"ga6uldu","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6uldu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34ybzv":{"author":"much-smoocho","author_fullname":"t2_4m5ijuus","author_premium":false,"banned_at_utc":null,"body":"thank you, I was asking about the api, so that is very good news.","created_utc":["2020-10-26","20:11:59"],"id":"ga6uo5k","link_id":"t3_jiioav","parent_id":"t1_ga6u6zs","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6uo5k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35345c":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"It can sometimes fall behind by quite a few hours, but generally it's pretty up to date.    I believe the ingest scripts were recently upgraded (or will be) so that should also help.","created_utc":["2020-10-26","20:15:36"],"id":"ga6v4ib","link_id":"t3_jiioav","parent_id":"t1_ga6uo5k","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6v4ib\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g353rz0":{"author":"subredditsummarybot","author_fullname":"t2_ydo5x7u","author_premium":false,"banned_at_utc":null,"body":"Do you know how far behind the re-ingest is with the updated scores?","created_utc":["2020-10-26","20:31:19"],"id":"ga6x32e","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6x32e\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g354n6g":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Nope, that would be way harder to track consistently.","created_utc":["2020-10-26","20:42:12"],"id":"ga6yg74","link_id":"t3_jiioav","parent_id":"t1_ga6x32e","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/ga6yg74\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35m2rs":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-10-27","02:16:37"],"id":"ga811no","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga811no\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37lcdi":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"[Well fuck](https:\/\/github.com\/pushshift\/api\/issues\/61)","created_utc":["2020-10-27","02:17:35"],"id":"ga815eh","link_id":"t3_jhawh5","parent_id":"t3_jhawh5","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga815eh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34vmgv":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Maybe it's because the post was also deleted by the author? I don't know how to get around that though...","created_utc":["2020-10-27","05:11:15"],"id":"ga8kiwi","link_id":"t3_jhawh5","parent_id":"t1_ga815eh","permalink":"\/r\/pushshift\/comments\/jhawh5\/trying_to_filter_api_results_by_submission_type\/ga8kiwi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g3586rq":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"The [new (beta) ingest](https:\/\/beta.pushshift.io\/search\/reddit\/comments?size=10) appears to be only a few seconds behind.","created_utc":["2020-10-27","18:56:15"],"id":"gaaf61i","link_id":"t3_jiioav","parent_id":"t1_ga6tw4z","permalink":"\/r\/pushshift\/comments\/jiioav\/how_far_behind_is_the_pushshift_reddit_data\/gaaf61i\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35d0xi":{"author":"elisewinn","author_fullname":"t2_wt8lr","author_premium":false,"banned_at_utc":null,"body":"Ha, the good ol' 502 cloudfare screen confirms my diagnosis: [https:\/\/imgur.com\/a\/z7taLG4](https:\/\/imgur.com\/a\/z7taLG4)","created_utc":["2020-10-31","20:02:01"],"id":"gapuua5","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gapuua5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35gp14":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"I did too; but after backing off, I was still able to get results. Weird.","created_utc":["2020-10-31","20:04:30"],"id":"gapv4xw","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gapv4xw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35gtlw":{"author":"pandalolz","author_fullname":"t2_4xq41","author_premium":false,"banned_at_utc":null,"body":"It's still working for me, but it's very slow and I'm having to retry requests a lot.","created_utc":["2020-10-31","20:49:25"],"id":"gaq03pr","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gaq03pr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g35lwik":{"author":"sneaky_dragon","author_fullname":"t2_5l2j2","author_premium":true,"banned_at_utc":null,"body":"I've been seeing the same for the past 24 hours as well. It works sometimes and doesn't work other times.","created_utc":["2020-10-31","21:02:46"],"id":"gaq1jz9","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gaq1jz9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g36pz4k":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"It\u2019s throttling","created_utc":["2020-11-01","03:47:32"],"id":"gar4yxl","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gar4yxl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g32xooj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That usually happens when it returns an error instead of the result you're expecting. The cloudflare error page is html so when the json decoder tries to parse it, that's the error you get.\n\nNothing you can do other than catching the error, waiting and trying again.","created_utc":["2020-11-01","05:53:19"],"id":"garfjz8","link_id":"t3_jlvnhn","parent_id":"t3_jlvnhn","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/garfjz8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g34hx9l":{"author":"confusid1","author_fullname":"t2_5pudz","author_premium":false,"banned_at_utc":null,"body":"Thanks for the reply. So by waiting and trying again, does that mean it should resolve itself in some period of time? Or would I have to implement a try\/except and work around the error?","created_utc":["2020-11-01","05:54:57"],"id":"garfojt","link_id":"t3_jlvnhn","parent_id":"t1_garfjz8","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/garfojt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37fwa1":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It should resolve itself","created_utc":["2020-11-01","06:26:45"],"id":"gari2te","link_id":"t3_jlvnhn","parent_id":"t1_garfojt","permalink":"\/r\/pushshift\/comments\/jlvnhn\/jsondecodeerror_help\/gari2te\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37grwq":{"author":"nick4fake","author_fullname":"t2_n8xxw","author_premium":false,"banned_at_utc":null,"body":"I am getting 500, 502 and 504 erros","created_utc":["2020-11-01","17:15:28"],"id":"gasqyz5","link_id":"t3_jlmuvq","parent_id":"t3_jlmuvq","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gasqyz5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g37xj3d":{"author":"elisewinn","author_fullname":"t2_wt8lr","author_premium":false,"banned_at_utc":null,"body":"Me too. I have a strong need for consistency so I had to stop the requests. I'll try again today...","created_utc":["2020-11-02","11:09:24"],"id":"gavqodf","link_id":"t3_jlmuvq","parent_id":"t1_gasqyz5","permalink":"\/r\/pushshift\/comments\/jlmuvq\/502_errors\/gavqodf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g32qzt6":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"How can you know if there is under\/over fetch without knowing what resources are being queried behind the scenes?","created_utc":["2020-11-09","15:17:28"],"id":"gbprq8d","link_id":"t3_jqx234","parent_id":"t3_jqx234","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbprq8d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g33hvwf":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"I need only id, title,  url, created\\_utc of the submission. Why do I need download: author,  domain,  full\\_link etc? There are a lot of data downloaded when I make a request. I would prefer choose what data I need in one query rather than download a bulk of data. \n\nI am talking about pushshift - my script over-fetching. I don't know what you are talking about. I am an amateur, I may not have understood something that you told correctly.","created_utc":["2020-11-09","15:24:56"],"id":"gbpscr6","link_id":"t3_jqx234","parent_id":"t1_gbprq8d","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpscr6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g32bn4t":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"No worries.\n\nI'm not saying GraphQL is a bad idea, just that it might not fix performance issues, because it really depends how pushshift makes its queries to its datastores, if they can be broken up then yes, maybe when you only care about id, title, url etc then it can resolve  just that and save making some other expensive queries, but if it all comes from the same query then GraphQL isn't going to help pushshift (although you of course will see some benefit in the form of only receiving the fields you asked for... but pushshift still had to fetch them, it's just that GraphQL threw them away)","created_utc":["2020-11-09","15:32:34"],"id":"gbpt0gh","link_id":"t3_jqx234","parent_id":"t1_gbpscr6","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpt0gh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2y8t69":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"Ahh. So my query would go this way. Query -&gt; pushshift -&gt; pushshift\\_database  \nand response -&gt; pushshift -&gt; me? And you are talking about its hardware and software implementation performance.","created_utc":["2020-11-09","15:40:32"],"id":"gbptq0g","link_id":"t3_jqx234","parent_id":"t1_gbpt0gh","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbptq0g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2yjrff":{"author":"PUSH_AX","author_fullname":"t2_8a8nd","author_premium":false,"banned_at_utc":null,"body":"Pretty much, at the point where it's `pushshift -&gt; pushshift_database` I don't know how that part works, so I can't be sure if GraphQL can have benefit to performance.\n\nMaybe it could though?","created_utc":["2020-11-09","15:48:09"],"id":"gbpuetu","link_id":"t3_jqx234","parent_id":"t1_gbptq0g","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbpuetu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2ymx66":{"author":"forgothesemicolon","author_fullname":"t2_z9nhqn1","author_premium":false,"banned_at_utc":null,"body":"I believe you can use the `fields` parameter in order to restrict what data is returned. For example:\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/submission?size=5&amp;subreddit=pushshift&amp;fields=id,title,url,created_utc  \n\nThere I set fields to `id,title,url,created_utc` which means only the fields you mentioned are returned. I'm not sure what happens behind the scenes but that should stop your script over fetching at least. Unless I've misunderstood what your asking?","created_utc":["2020-11-09","21:03:20"],"id":"gbqv8d0","link_id":"t3_jqx234","parent_id":"t1_gbpscr6","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbqv8d0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g31iv6i":{"author":"Yekoss","author_fullname":"t2_ags03f","author_premium":false,"banned_at_utc":null,"body":"Thanks. That's what I meant and didn't know about the fields. The problem is with public APIs which are overused by people like me and fetch a lot of data. People like me don't know about the existence of such params.","created_utc":["2020-11-09","21:08:01"],"id":"gbqvt96","link_id":"t3_jqx234","parent_id":"t1_gbqv8d0","permalink":"\/r\/pushshift\/comments\/jqx234\/why_rest_not_graphql\/gbqvt96\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"giskxwm":{"author":"Subduction","author_fullname":"t2_4ahvv","author_premium":false,"banned_at_utc":null,"body":"And while we're on pushshift.io, if I search for comments since the beginning of my sub it tells me the total number, but if I search for posts it doesn't.\n\nIs there some way to get the total count of posts since the start?","created_utc":["2020-11-10","17:05:21"],"id":"gbtzjo5","link_id":"t3_jrm3ew","parent_id":"t3_jrm3ew","permalink":"\/r\/pushshift\/comments\/jrm3ew\/redditsearchio_question\/gbtzjo5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"giw4137":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The current PushShift ingest of Reddit can lag significantly behind real time, so it probably just hasn't been ingested yet.","created_utc":["2020-11-10","17:40:00"],"id":"gbu3nm0","link_id":"t3_jrn1cn","parent_id":"t3_jrn1cn","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu3nm0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25lt08":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"this was a post from several months ago","created_utc":["2020-11-10","17:41:09"],"id":"gbu3sjp","link_id":"t3_jrn1cn","parent_id":"t1_gbu3nm0","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu3sjp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25m7h1":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Variety of possible reasons but yes, deleted too quickly is the most likely reason.\n\npushshift aims to get every post\/comment within a few seconds of posting but currently regularly falls behind due to reddit's increasing volume, reddit API limitations and spam. (this is being worked on)\n\nSo how fast is too quickly can vary between a few seconds and several hours.\n\n\nThen there are unusual but not uncommon scenarios like;  \nIn the event something is caught by redddit's spam filter or automoderator pushshift will capture it as deleted even if it's later approved by a moderator unless pushshift was running far enough behind that a moderator was able to approve it before pushshift first saw it.\n\nPushshift also allows people to opt out\/remove their data.","created_utc":["2020-11-10","17:52:25"],"id":"gbu56jm","link_id":"t3_jrn1cn","parent_id":"t3_jrn1cn","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu56jm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25m8t8":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"When you say \"it isn't retrieved\" does that mean the content of the post was not returned (i.e. it shows as deleted or removed)? Or does it not show up at all?","created_utc":["2020-11-10","18:01:24"],"id":"gbu6av4","link_id":"t3_jrn1cn","parent_id":"t1_gbu3sjp","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6av4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25o4gm":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"it shows as  \"\\[removed\\]\"","created_utc":["2020-11-10","18:03:33"],"id":"gbu6kdf","link_id":"t3_jrn1cn","parent_id":"t1_gbu6av4","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6kdf\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25og92":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"Ah in that case the post was technically successfully ingested by Pushshift. However the content of the submission had already been deleted by the user or removed by a moderator prior to getting ingested so it is unavailable. PushShift cannot archive content that was already deleted\/removed on Reddit at the time of ingest.","created_utc":["2020-11-10","18:06:23"],"id":"gbu6wy5","link_id":"t3_jrn1cn","parent_id":"t1_gbu6kdf","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu6wy5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25onmk":{"author":"Particular_Crew6684","author_fullname":"t2_8pqzv0ua","author_premium":false,"banned_at_utc":null,"body":"okay, that makes sense! How long does it take for PushShift to ingest something? thanks","created_utc":["2020-11-10","18:10:39"],"id":"gbu7gi9","link_id":"t3_jrn1cn","parent_id":"t1_gbu6wy5","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbu7gi9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25p5kw":{"author":"deleteaway11","author_fullname":"t2_8scc6vpc","author_premium":false,"banned_at_utc":null,"body":"Do they respond back to their followers for such requests and how long does it typically take? I realize they are probably busy with work etc.  I also realize it\u2019s a thankless job to do this for so many users. They deserve respect for treating people\u2019s privacy how it should be which is fortunate.","created_utc":["2020-11-10","18:45:28"],"id":"gbubvjq","link_id":"t3_jrn1cn","parent_id":"t1_gbu56jm","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbubvjq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25p6ak":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"There is a [Data Deletion Request Megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/) run by abrownn he usually replies to every request, time varies as it tends to be done in batches.\n\nI've not tried to keep track of the usual times but off hand it seems to be several days, there is work being done to make the process more reliable and eventually self service.\n\n&gt;[Removal requests will be made easier by allowing users who still have an active Reddit account to simply log in via their Reddit account to prove ownership and then be given the ability to remove their data from the cluster. This will automate and speed up removal requests for users who are concerned about their privacy. This page will also allow a user to download all of their comments and posts if they choose to do so before removing their data.](https:\/\/www.reddit.com\/r\/pushshift\/comments\/jplcs1\/growing_pains_and_moving_forward_to_bigger_and\/#form-t3_jplcs1gjs:~:text=Removal%20requests%20will%20be%20made%20easier,do%20so%20before%20removing%20their%20data.)","created_utc":["2020-11-10","19:16:06"],"id":"gbufuif","link_id":"t3_jrn1cn","parent_id":"t1_gbubvjq","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbufuif\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25pued":{"author":"deleteaway11","author_fullname":"t2_8scc6vpc","author_premium":false,"banned_at_utc":null,"body":"Okay. I did see that thread. I also saw people emailing but guessing a response may take awhile? Not sure what others have experienced.","created_utc":["2020-11-10","19:22:05"],"id":"gbugmkw","link_id":"t3_jrn1cn","parent_id":"t1_gbufuif","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbugmkw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25qug8":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"From what i'm able to see from here comments in the megathread most reliably get replies usually within a day but the actual removal takes longer.\n\nSITM does the actual removals so it may be faster to contact him directly, the most reliable option being via his twitter @jasonbaumgartne","created_utc":["2020-11-10","20:18:19"],"id":"gbunx37","link_id":"t3_jrn1cn","parent_id":"t1_gbugmkw","permalink":"\/r\/pushshift\/comments\/jrn1cn\/some_things_dont_get_caught_in_pushshift_why\/gbunx37\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25rubt":{"author":"huckingfoes","author_fullname":"t2_61kzg","author_premium":true,"banned_at_utc":null,"body":"I suggest posting this over at redditdev too; those folks I imagine would be greatly receptive to helping out this project.","created_utc":["2020-11-11","03:52:25"],"id":"gbw5qga","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbw5qga\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25rw4y":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Sent mods a message asking permission. Great suggestion; thank you!","created_utc":["2020-11-11","03:56:46"],"id":"gbw67az","link_id":"t3_jrylb2","parent_id":"t1_gbw5qga","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbw67az\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25s0dr":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"No idea why you are getting downvoted. This is a great suggestion and you went so far as to ask permission which is the right move.","created_utc":["2020-11-11","05:05:54"],"id":"gbwdk8j","link_id":"t3_jrylb2","parent_id":"t1_gbw67az","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwdk8j\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25s1vp":{"author":"crazylegs888","author_fullname":"t2_5icns","author_premium":false,"banned_at_utc":null,"body":"I'll definitely donate before the end of the month.","created_utc":["2020-11-11","05:07:31"],"id":"gbwdq8y","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwdq8y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25u0pn":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Awesome! Thank you.","created_utc":["2020-11-11","06:23:45"],"id":"gbwlau6","link_id":"t3_jrylb2","parent_id":"t1_gbwdq8y","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwlau6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25ubcf":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Update, I posted over there: https:\/\/old.reddit.com\/r\/redditdev\/comments\/js1mse\/funding_pushshift_please_help_if_you_can\/?  \n\nThank you for the suggestion and visibility.","created_utc":["2020-11-11","06:48:07"],"id":"gbwnhy2","link_id":"t3_jrylb2","parent_id":"t1_gbw5qga","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbwnhy2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25ux2a":{"author":"Dark_Randor","author_fullname":"t2_101p0z","author_premium":true,"banned_at_utc":null,"body":"No, the compressed files only contain the content of that  month, so in your case April 2020. To my knowlege there is no complete file, only the monthly files (and they are already huge if decompressde, so I don\u00b4t think you acutally want a complete file). Theroretically you should be able to create one by attaching all the monthly JSONs to each other.","created_utc":["2020-11-11","14:42:49"],"id":"gbxhi9m","link_id":"t3_js69o5","parent_id":"t3_js69o5","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gbxhi9m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25w7rk":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":false,"banned_at_utc":null,"body":"Maybe dataisbeautiful too?","created_utc":["2020-11-11","15:11:30"],"id":"gbxjq68","link_id":"t3_jrylb2","parent_id":"t1_gbwnhy2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxjq68\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g29t9vl":{"author":"DaveChild","author_fullname":"t2_35wpy","author_premium":false,"banned_at_utc":null,"body":"Thanks for this, I've become a Patreon as well. The only way something like this stays free to use is with support through efforts like Patreon.","created_utc":["2020-11-11","15:12:32"],"id":"gbxjt7n","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxjt7n\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g29xd0y":{"author":"whiplash_06","author_fullname":"t2_4lgr7ozq","author_premium":false,"banned_at_utc":null,"body":"Do you mind if I aggressively cross-post this across the ML and social science subreddits?","created_utc":["2020-11-11","17:10:17"],"id":"gbxvbr6","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbxvbr6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2a3wsr":{"author":"bluzkluz","author_fullname":"t2_4eb1ygn5","author_premium":false,"banned_at_utc":null,"body":"Thanks Jason &amp; Co. This is a very useful resource. I contributed. But I suggest you start charging for heavy usage with a basic free plan. It would make this more reliable and could become somebody's full-time gig.","created_utc":["2020-11-11","20:34:46"],"id":"gbyjts1","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyjts1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g2a71uc":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Tagging \/u\/CharBram because I'm replying to their comment as well. It would be cool if people could get their rate limit increased by contributing a little extra. That being said, not sure if that melds with Jason's philosophy on open access.","created_utc":["2020-11-11","21:03:41"],"id":"gbynchp","link_id":"t3_jrylb2","parent_id":"t1_gbyjts1","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbynchp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g229bxk":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"No thank you necessary. All I did was make a post. Thank you so much for your support. You are absolutely right that the only way to keep these amazing projects alive is with some help. Hats off.","created_utc":["2020-11-11","21:06:14"],"id":"gbynni9","link_id":"t3_jrylb2","parent_id":"t1_gbxjt7n","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbynni9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22af12":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I do not mind. In fact, that's exactly what I would love to see. Please, aggressively post!","created_utc":["2020-11-11","21:10:39"],"id":"gbyo6fi","link_id":"t3_jrylb2","parent_id":"t1_gbxvbr6","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyo6fi\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22b04b":{"author":"bluzkluz","author_fullname":"t2_4eb1ygn5","author_premium":false,"banned_at_utc":null,"body":"I too am a big believer in open access and there is no contradiction with charging for it. Heavy users can get higher rate limits, reliability uptime, and\/or other benefits with some kind of subscription package. It would be a pity if this immensely useful project shuts down due to funding.","created_utc":["2020-11-11","21:28:41"],"id":"gbyqckd","link_id":"t3_jrylb2","parent_id":"t1_gbynchp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbyqckd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22b6yg":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"Couple heads-ups about patching together the old files \/u\/sbs1799: you'll have to stitch together a handful of compression and packaging schemas to get it all together. You're going to be missing about 7 months of recent data that you'll probably need to use the API for but I don't know that there's a good way to pull a *lot* of data through that.","created_utc":["2020-11-11","21:55:22"],"id":"gbytlmn","link_id":"t3_js69o5","parent_id":"t1_gbxhi9m","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gbytlmn\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22bc41":{"author":"CharBram","author_fullname":"t2_gzjeb","author_premium":false,"banned_at_utc":null,"body":"Exactly. In its current form it\u2019s simply not sustainable. Costs will keep increasing and without a monetization model it\u2019s eventually going to shut down. What\u2019s he going to do when it\u2019s costing him 3k every month?\n\nAnd this is such a niche use case it\u2019s not great for most types of corporate sponsorship. You can\u2019t use ads because it\u2019s an API and also I doubt there\u2019s a massive amount of users. \n\nOnly option is to charge money for it. \n\nIf no one is willing to pay then it obviously isn\u2019t as valuable as we thought.","created_utc":["2020-11-11","22:57:24"],"id":"gbz0t68","link_id":"t3_jrylb2","parent_id":"t1_gbyqckd","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbz0t68\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22c028":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"He's definitely said in the past that once he implements user tokens you'll be able to pay for a higher rate limit.","created_utc":["2020-11-11","23:34:50"],"id":"gbz2vd8","link_id":"t3_jrylb2","parent_id":"t1_gbynchp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gbz2vd8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22cq1g":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Comments are [here](https:\/\/files.pushshift.io\/reddit\/comments\/), submissions are [here](https:\/\/files.pushshift.io\/reddit\/submissions\/). Each file is one months worth. There's also three different compression schemes since pushshift has been running for years.\n\nThey are also somewhat out of date, the last 6 months of submissions and 10 months of comments still haven't been uploaded yet. So you'll need to use the API for those timespans.","created_utc":["2020-11-12","03:03:12"],"id":"gbzq3py","link_id":"t3_jskr1m","parent_id":"t3_jskr1m","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzq3py\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22de7m":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"So you'd have to download every file? There's nothing cumulative?\n\nIt may be possible to concatenate all those files so you have one big file. That'd at least make the directory structure and coding easier to work with.","created_utc":["2020-11-12","03:05:21"],"id":"gbzqd5z","link_id":"t3_jskr1m","parent_id":"t1_gbzq3py","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzqd5z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g22essq":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well the files are multiple gigabytes each. The whole thing put together, even compressed, would be like 500 gigabytes. Uncompressed it would be over a terabyte.\n\nThere isn't much point having that as one big file, it's too unwieldy to do anything with.","created_utc":["2020-11-12","03:09:49"],"id":"gbzqwu5","link_id":"t3_jskr1m","parent_id":"t1_gbzqd5z","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzqwu5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24iqaw":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Good point. I suppose what I can do is download each file, reduce it to just having the comments\/submisisons I'm interested in, and then save that.\n\nOr alternatively I could just make a dump of all those things with the API, and only update it by getting data dated from later on.","created_utc":["2020-11-12","03:18:38"],"id":"gbzs0z3","link_id":"t3_jskr1m","parent_id":"t1_gbzqwu5","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gbzs0z3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24jb51":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"The aggs parameter [has been temporarily disabled](https:\/\/www.reddit.com\/r\/pushshift\/comments\/jm8yyt\/aggregations_have_been_temporarily_disabled_to\/) due to it negatively impacting platform stability.","created_utc":["2020-11-12","03:30:57"],"id":"gbztm8s","link_id":"t3_jsl235","parent_id":"t3_jsl235","permalink":"\/r\/pushshift\/comments\/jsl235\/getting_a_count_of_comments_by_author\/gbztm8s\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24l0jy":{"author":"f_k_a_g_n","author_fullname":"t2_5wrff1n","author_premium":false,"banned_at_utc":null,"body":"There are over 8 billion reddit comments. I don't think downloading all the data dumps is going to be more efficient than just using the API based on what you described.\n\nData through 2019 is also available on BigQuery. There are some links in the[FAQ](https:\/\/www.reddit.com\/r\/pushshift\/comments\/bcxguf\/new_to_pushshift_read_this_faq\/)","created_utc":["2020-11-12","06:11:08"],"id":"gc0abv5","link_id":"t3_jskr1m","parent_id":"t3_jskr1m","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0abv5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24lw3l":{"author":"inspiredby","author_fullname":"t2_5kk2e","author_premium":false,"banned_at_utc":null,"body":"&gt; Good point. I suppose what I can do is download each file, reduce it to just having the comments\/submisisons I'm interested in, and then save that.\n\n&gt; Or alternatively I could just make a dump of all those things with the API, and only update it by getting data dated from later on.\n\nYSK the data in the compressed files is different from what's in the API. From the [FAQ](https:\/\/www.reddit.com\/r\/pushshift\/comments\/bcxguf\/new_to_pushshift_read_this_faq\/),\n\n&gt; The [Pushshift API](https:\/\/github.com\/pushshift\/api) serves a copy of reddit objects. Currently, data is copied into Pushshift at the time it is posted to reddit. Therefore, scores and other meta such as edits to a submission's `selftext` or a comment's `body` field may not reflect what is displayed by reddit.\n\n&gt; ...\n\n&gt; -\n\n&gt; The files in [files\/comments](https:\/\/files.pushshift.io\/reddit\/comments\/) and [files\/submissions](https:\/\/files.pushshift.io\/reddit\/submissions\/) each represent a copy of one month's worth of objects as they appeared on reddit at the time of the download. For example `RS_2018-08.xz` contains submissions made to reddit in August 2018 as they appeared on September 20th.\n\nScores and other meta may have between the time the data was grabbed for the API and when it was downloaded for the monthly file.","created_utc":["2020-11-12","06:22:08"],"id":"gc0bcdv","link_id":"t3_jskr1m","parent_id":"t1_gbzs0z3","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0bcdv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24nygu":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Also, Watchful1, I've been wondering: what's the monthly cost of running \/u\/RemindMeBot? It's a great bot so I'm curious how much a bot like it would cost to run.","created_utc":["2020-11-12","08:21:39"],"id":"gc0n8iz","link_id":"t3_jskr1m","parent_id":"t1_gbzqwu5","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0n8iz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24o9yr":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You could easily run it on a $5 a month VPS.","created_utc":["2020-11-12","09:28:39"],"id":"gc0t9s6","link_id":"t3_jskr1m","parent_id":"t1_gc0n8iz","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gc0t9s6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24u0gk":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"Why would anyone donate when we can't even perform a simple search by author? Put that option back, and I'll gladly donate.","created_utc":["2020-11-12","16:10:28"],"id":"gc1nk2g","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc1nk2g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24ucmz":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Searching by author was [added back to the API shortly after it was removed](https:\/\/www.reddit.com\/r\/pushshift\/comments\/eivj0a\/the_author_parameter_has_been_restored_for_the\/) but it remains unavailable from [redditsearch.io](https:\/\/redditsearch.io\/)","created_utc":["2020-11-12","18:06:39"],"id":"gc20whp","link_id":"t3_jrylb2","parent_id":"t1_gc1nk2g","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc20whp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24uxbb":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"\/u\/Rapiolli you heard it from here.  \n&gt; Put that option back, and I'll gladly donate.  \n\nThis means you'll be donating, right?","created_utc":["2020-11-12","21:15:49"],"id":"gc2qi1q","link_id":"t3_jrylb2","parent_id":"t1_gc20whp","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc2qi1q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24w4k0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I actually just responded to an identical post about this over in r\/redditdev \n\nhttps:\/\/www.reddit.com\/r\/redditdev\/comments\/jsrdpx\/psaw_returns_mismatching_number_of_posts\/","created_utc":["2020-11-12","23:41:46"],"id":"gc38s34","link_id":"t3_jt3w2p","parent_id":"t3_jt3w2p","permalink":"\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/gc38s34\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g24wqc1":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"&gt; Searching by author remains unavailable from redditsearch.io\n \nI will, once it's fixed.","created_utc":["2020-11-12","23:47:08"],"id":"gc39fil","link_id":"t3_jrylb2","parent_id":"t1_gc2qi1q","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc39fil\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g252pb8":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"This is an open source project that uses pushshift and allows you to search by author: https:\/\/camas.github.io\/reddit-search\/","created_utc":["2020-11-12","23:49:01"],"id":"gc39nq2","link_id":"t3_jrylb2","parent_id":"t1_gc39fil","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc39nq2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1y8jjh":{"author":"Rapiolli","author_fullname":"t2_8uy5aoan","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-11-13","00:02:06"],"id":"gc3b9b4","link_id":"t3_jrylb2","parent_id":"t1_gc39nq2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gc3b9b4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1z21di":{"author":"buff_pls","author_fullname":"t2_26kogxm1","author_premium":false,"banned_at_utc":null,"body":"Great, thanks!","created_utc":["2020-11-13","00:34:25"],"id":"gc3f1fk","link_id":"t3_jt3w2p","parent_id":"t1_gc38s34","permalink":"\/r\/pushshift\/comments\/jt3w2p\/limit_of_500_posts_on_psaw\/gc3f1fk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g206en4":{"author":"sbs1799","author_fullname":"t2_769nh3nc","author_premium":false,"banned_at_utc":null,"body":"A  follow-up question: why is that size of files ([https:\/\/files.pushshift.io\/reddit\/comments\/](https:\/\/files.pushshift.io\/reddit\/comments\/)) increases with time. Should it not remain the roughly same for all periods?","created_utc":["2020-11-13","11:03:24"],"id":"gc4zh39","link_id":"t3_js69o5","parent_id":"t1_gbytlmn","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gc4zh39\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25cacf":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"I think what you're not accounting for is the growth of Reddit over time. You can get a rough idea of how many more people were active year over year just by the file sizes.","created_utc":["2020-11-13","17:58:40"],"id":"gc5tvwl","link_id":"t3_js69o5","parent_id":"t1_gc4zh39","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gc5tvwl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25cbgm":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"ASAP which is typically within a few seconds.\n\nThe API has been having problems with large volumes of spam overwhelming it resulting in it getting multiple hours behind but this seems to have been resolved in the beta.","created_utc":["2020-11-14","17:39:41"],"id":"gc9csha","link_id":"t3_jtzo6c","parent_id":"t3_jtzo6c","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9csha\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g25d2nl":{"author":"holdingroad","author_fullname":"t2_8dfhv0vb","author_premium":false,"banned_at_utc":null,"body":"Writing blank text, so i can edit so we will test this (18:27)","created_utc":["2020-11-14","19:27:41"],"id":"gc9nxz3","link_id":"t3_jtzo6c","parent_id":"t1_gc9csha","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9nxz3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rclau":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"reveddit has a page that shows the current delay https:\/\/www.reveddit.com\/info\/\n\nYou can also calculate it using the time difference between \"created_utc\" and \"retrieved_on\" (or local system UTC time which will have slightly more variance but wont break if ingest stops working entirely) for the most recent comment or submission.","created_utc":["2020-11-14","19:51:28"],"id":"gc9qisb","link_id":"t3_jtzo6c","parent_id":"t1_gc9nxz3","permalink":"\/r\/pushshift\/comments\/jtzo6c\/after_what_time_comments_get_archived\/gc9qisb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rdnwq":{"author":"Remount_Kings_Troop_","author_fullname":"t2_la8cw","author_premium":true,"banned_at_utc":null,"body":"I'm in!","created_utc":["2020-11-14","23:34:45"],"id":"gcaj67g","link_id":"t3_jrylb2","parent_id":"t3_jrylb2","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gcaj67g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rqgd7":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Thank you! Wow, I can't believe how far we've come thanks to you guys! We're only $19\/month away from $500\/month. Huge difference. Hats off to you and everyone else who has pitched in. Thank you so much!","created_utc":["2020-11-14","23:43:49"],"id":"gcakdnh","link_id":"t3_jrylb2","parent_id":"t1_gcaj67g","permalink":"\/r\/pushshift\/comments\/jrylb2\/funding_pushshift_please_help\/gcakdnh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rsmvj":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"Best you're going to get is the markdown with included regex. You'll have to run the body value through a markdown to html converter and sanitizer. This npm package [marked](https:\/\/www.npmjs.com\/package\/marked) will help a lot. You'll also want to check out DOMPurify if you'd like to sanitize the output html.","created_utc":["2020-11-18","04:10:40"],"id":"gcoemu0","link_id":"t3_jw4p4f","parent_id":"t3_jw4p4f","permalink":"\/r\/pushshift\/comments\/jw4p4f\/is_there_a_way_to_get_the_html_of_a_comment_from\/gcoemu0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1rsn44":{"author":"new_in_montreal","author_fullname":"t2_8y7bo","author_premium":false,"banned_at_utc":null,"body":"Do we have any information on possible dump updates?","created_utc":["2020-11-18","04:48:30"],"id":"gcoio79","link_id":"t3_jskr1m","parent_id":"t1_gbzq3py","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gcoio79\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1s9ml9":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Nope, nothing new.","created_utc":["2020-11-18","08:44:04"],"id":"gcp35ax","link_id":"t3_jskr1m","parent_id":"t1_gcoio79","permalink":"\/r\/pushshift\/comments\/jskr1m\/where_to_download_cumulative_reddit_dumps_all\/gcp35ax\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1iuvzu":{"author":"thetrombonist","author_fullname":"t2_6nilj","author_premium":false,"banned_at_utc":null,"body":"lol nope\n\nI\u2019ve requested multiple times","created_utc":["2020-11-19","00:00:58"],"id":"gcriud1","link_id":"t3_jwj00o","parent_id":"t3_jwj00o","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcriud1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1ivck0":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Did you check yours recently to see?","created_utc":["2020-11-19","03:39:59"],"id":"gcs96bd","link_id":"t3_jwj00o","parent_id":"t1_gcriud1","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcs96bd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1fg4zd":{"author":"thetrombonist","author_fullname":"t2_6nilj","author_premium":false,"banned_at_utc":null,"body":"[Yep!](https:\/\/api.pushshift.io\/reddit\/search\/comment\/?author=thetrombonist)","created_utc":["2020-11-19","04:36:43"],"id":"gcsfhcc","link_id":"t3_jwj00o","parent_id":"t1_gcs96bd","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcsfhcc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1g3m1j":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Anyone try contacting the admins? May have forgotten.","created_utc":["2020-11-19","16:51:28"],"id":"gcu052w","link_id":"t3_jwj00o","parent_id":"t1_gcsfhcc","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcu052w\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1sa8xo":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"I got a response from \/u\/Stuck_In_the_Matrix this evening, he's planning on running a batch deletion this weekend.","created_utc":["2020-11-20","01:29:05"],"id":"gcvwg4z","link_id":"t3_jwj00o","parent_id":"t1_gcu052w","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcvwg4z\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1d2b0z":{"author":"kylereece747","author_fullname":"t2_8u8fejcf","author_premium":false,"banned_at_utc":null,"body":"Awesome! Thanks.","created_utc":["2020-11-20","01:30:20"],"id":"gcvwlz6","link_id":"t3_jwj00o","parent_id":"t1_gcvwg4z","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcvwlz6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dfs37":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"What list do you need to be on for that to happen?","created_utc":["2020-11-20","02:25:21"],"id":"gcw2xtb","link_id":"t3_jwj00o","parent_id":"t1_gcvwg4z","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw2xtb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dhh4o":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"The [deletion request megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/)","created_utc":["2020-11-20","02:29:25"],"id":"gcw3e4h","link_id":"t3_jwj00o","parent_id":"t1_gcw2xtb","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw3e4h\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1dhn00":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"Thank you. Is there a way for me to pm someone my old account name?","created_utc":["2020-11-20","02:30:06"],"id":"gcw3gsv","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw3gsv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g18c0xf":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"you want \/u\/abrownn but he has been afk for a couple days so it may be a while before you get a response.","created_utc":["2020-11-20","02:37:23"],"id":"gcw49gk","link_id":"t3_jwj00o","parent_id":"t1_gcw3gsv","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw49gk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g18fppy":{"author":"Comprehensive-Quit15","author_fullname":"t2_8xe7kybj","author_premium":false,"banned_at_utc":null,"body":"Thank you, how likely is it that u\/stuck_in_the_matrix would be able to go through the entire list this weekend? I would probably need to send proof of ownership of the old account or something. Not something I can do on a megathread unfortunately.","created_utc":["2020-11-20","02:40:12"],"id":"gcw4kge","link_id":"t3_jwj00o","parent_id":"t1_gcw49gk","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw4kge\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g193djy":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"assuming something doesn't go wrong between now and then he should be able to do the entire list but SITM usually gets the list from abrownn so we're also counting on him getting back by this weekend.","created_utc":["2020-11-20","02:44:43"],"id":"gcw5209","link_id":"t3_jwj00o","parent_id":"t1_gcw4kge","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcw5209\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g193tsq":{"author":"delet_this_throwaway","author_fullname":"t2_8n2bxnfu","author_premium":false,"banned_at_utc":null,"body":"Are DM requests going to be addressed in the batch deletion as well? I've reached out to the mod team and don't want to give the account info until I hear back from one of them.","created_utc":["2020-11-20","04:33:47"],"id":"gcwgzwa","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcwgzwa\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g196e3x":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"abrownn should include the requests he's gotten via dm in the batch.\nHe is still handling the queue but i've not heard anything from him in a few days.","created_utc":["2020-11-20","18:45:44"],"id":"gcyeqeu","link_id":"t3_jwj00o","parent_id":"t1_gcwgzwa","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gcyeqeu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g197c0e":{"author":"d3rr","author_fullname":"t2_lf73j","author_premium":false,"banned_at_utc":null,"body":"It looks like a pushshift issue, I can reproduce this with conspiracy data:\n\n    python .\/fetch_links.py conspiracy 2020-9-1 2020-9-1\n\nGives\n\n    ...\n     2020-08-31 16:23:28 fetch link ik14eb comments 15\/15\n    2020-08-31 16:25:41 fetch link ik15w0 comments 43\/43\n    2020-08-31 16:26:12 fetch link ik167v comments 8\/8\n    got 10 links, wrote 10 and 292 comments\n    \/home\/ME\/.local\/lib\/python3.8\/site-packages\/psaw\/PushshiftAPI.py:192: UserWarning: Got non 200 code 500\n      warnings.warn(\"Got non 200 code %s\" % response.status_code)\n    \/home\/ME\/.local\/lib\/python3.8\/site-packages\/psaw\/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n      warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n\n\nThe script could handle this 500 error more gracefully, it seems like it never retries. As a workaround, skip that day.","created_utc":["2020-11-22","10:36:58"],"id":"gd6t859","link_id":"t3_jxs3zw","parent_id":"t3_jxs3zw","permalink":"\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/gd6t859\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g19ivbz":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-11-22","10:37:09"],"id":"gd6t94q","link_id":"t3_jxs3zw","parent_id":"t1_gd6t859","permalink":"\/r\/pushshift\/comments\/jxs3zw\/having_difficulty_downloading_data_from_2020\/gd6t94q\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g0nmiiq":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Only the submissions\/comments while it's public.\n\nManual intervention is required to get it to request the back content.","created_utc":["2020-11-22","17:42:44"],"id":"gd875f2","link_id":"t3_jywudk","parent_id":"t3_jywudk","permalink":"\/r\/pushshift\/comments\/jywudk\/when_a_subreddit_is_deprivatized_does_pushshift\/gd875f2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1c63i1":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"60 calls a minute \"one per second\" as per [the announcement a few months ago](https:\/\/www.reddit.com\/r\/pushshift\/comments\/g7125k\/in_an_effort_to_relieve_some_of_the_strain_on_the\/) and a maximum of 100 results per query.","created_utc":["2020-11-25","18:29:39"],"id":"gdkeekw","link_id":"t3_k0suiz","parent_id":"t3_k0suiz","permalink":"\/r\/pushshift\/comments\/k0suiz\/rate_limits\/gdkeekw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cdb0n":{"author":"Ninja-Waffles","author_fullname":"t2_lh6uk","author_premium":true,"banned_at_utc":null,"body":"I'm not sure if there's a specific endpoint for getting the delays; however, if you're using the API, you can ping it every minute (or however long of a delay you want to add), find the UTC of the first (latest) entry and then match that against the current UTC (on your machine\/server) to get the ingest delay.\n\nFor example, when writing this comment, the first entry on [this](https:\/\/api.pushshift.io\/reddit\/comment\/search) page has `created_utc` as `1606359841` and the current UTC is `1606361198`, so we can do `(1606361198 - 1606359841) \/ 60 = ~22 minutes`. This seems to match up with the output that reveddit is displaying too.","created_utc":["2020-11-26","05:29:27"],"id":"gdmfy5k","link_id":"t3_k16rbr","parent_id":"t3_k16rbr","permalink":"\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/gdmfy5k\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cdh42":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"This is exactly what I do for u\/RemindMeBot. There's a bit of abstraction, but the code I have for it is [here](https:\/\/github.com\/Watchful1\/PrawWrapper\/blob\/master\/praw_wrapper\/__init__.py#L136-L150).","created_utc":["2020-11-26","07:40:31"],"id":"gdmreak","link_id":"t3_k16rbr","parent_id":"t1_gdmfy5k","permalink":"\/r\/pushshift\/comments\/k16rbr\/api_endpoint_for_archive_delay_times\/gdmreak\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cvwy1":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"Just saw that it was temporarily disabled. Is there any other way to get posts\/day or comments\/day in a subreddit besides slowly cycling through everyday and getting the total from the metadata?","created_utc":["2020-11-27","10:07:17"],"id":"gdqxlsx","link_id":"t3_k1x0lg","parent_id":"t3_k1x0lg","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdqxlsx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1cwpzn":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Iterating through pages is usually fine unless it's a really large sub. The whole point of disabling the aggs argument was to spread out the processing across several calls instead of doing it all at once.","created_utc":["2020-11-27","10:14:32"],"id":"gdqy2hj","link_id":"t3_k1x0lg","parent_id":"t1_gdqxlsx","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdqy2hj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1iomnv":{"author":"ObsidianDreamsRedux","author_fullname":"t2_czj3bdp","author_premium":false,"banned_at_utc":null,"body":"Yes, it was disabled due to performance issues caused by some.\n\nhttps:\/\/old.reddit.com\/r\/pushshift\/comments\/jm8yyt\/aggregations_have_been_temporarily_disabled_to\/","created_utc":["2020-11-27","16:47:16"],"id":"gdro9x6","link_id":"t3_k227g4","parent_id":"t3_k227g4","permalink":"\/r\/pushshift\/comments\/k227g4\/has_the_aggs_argument_been_disabled\/gdro9x6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"g1itdy5":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"I want see the frequency of a thousand over a few years of data. I can definitely use threads to speed it up but I still I think it would take quite some time. Would you recommend me running the query on something like aws?","created_utc":["2020-11-27","18:01:46"],"id":"gdrvq77","link_id":"t3_k1x0lg","parent_id":"t1_gdqy2hj","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdrvq77\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghcx2yg":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Well no, that's the opposite of what you should be doing. The whole point is to not overwhelm the pushshift servers by making a bunch of requests really fast.\n\nPushshift isn't a big company, it's run by one guy from servers in his living room. It's important to minimize how much ask of it. Which in this case means spreading out the requests as much as possible.\n\nI would just set up the script and leave it running overnight, or even over a few days if I had to. Are you searching in a specific subreddit or across all of reddit? What subreddit?","created_utc":["2020-11-27","20:50:54"],"id":"gdsh6js","link_id":"t3_k1x0lg","parent_id":"t1_gdrvq77","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdsh6js\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghczwu4":{"author":"Bloody_Biscuit","author_fullname":"t2_104tm8","author_premium":false,"banned_at_utc":null,"body":"Ok sounds good! I\u2019d hate to overwhelm the servers or anything. I\u2019ll just run it over a couple of days. Thanks for your help!","created_utc":["2020-11-27","21:52:46"],"id":"gdsnx53","link_id":"t3_k1x0lg","parent_id":"t1_gdsh6js","permalink":"\/r\/pushshift\/comments\/k1x0lg\/no_aggs_argument\/gdsnx53\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghe9f4t":{"author":"francisco_rodriguez","author_fullname":"t2_1t4dkgl","author_premium":false,"banned_at_utc":null,"body":"Hi all! I'm having the same issue for gab dumps: [https:\/\/files.pushshift.io\/gab\/](https:\/\/files.pushshift.io\/gab\/)\n\nThe most recent dump is from august 2019","created_utc":["2020-11-30","12:41:31"],"id":"ge4ymvd","link_id":"t3_k3ji22","parent_id":"t3_k3ji22","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/ge4ymvd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gheg2ud":{"author":"JFM786mithila","author_fullname":"t2_94ulzzuk","author_premium":false,"banned_at_utc":null,"body":"This link will help you very much.","created_utc":["2020-12-02","18:48:32"],"id":"gedx32g","link_id":"t3_k5cvoc","parent_id":"t3_k5cvoc","permalink":"\/r\/pushshift\/comments\/k5cvoc\/how_to_get_6_pack_abs\/gedx32g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghevdi5":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Have you tried using PSAW (https:\/\/github.com\/dmarx\/psaw), the unofficial Pushshift API wrapper? \n\nEven so, I had issues with PSAW not properly handling 500-level connection errors, even though it\u2019s supposed to and automatically back off and retry, but I modified the PSAW API code and now it catches errors, waits, and retries successfully.\n\nIt is clearly a server-side issue.","created_utc":["2020-12-02","19:58:09"],"id":"gee7j11","link_id":"t3_k5c1ro","parent_id":"t3_k5c1ro","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gee7j11\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghf1tq2":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Gab disabled the API endpoint that allowed the dumps to be made, SITM is still looking for a workaround but until one is found there likely won't be any more gab dumps.\nhttps:\/\/twitter.com\/jasonbaumgartne\/status\/1276518618692452360","created_utc":["2020-12-02","20:18:05"],"id":"geean9x","link_id":"t3_k3ji22","parent_id":"t1_ge4ymvd","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/geean9x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghf1yo6":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"That's mostly just normal. If you hit a 500 just wait and try again later. The API gets too much traffic and gets overloaded.\n\nOr are you never getting 200's at all?","created_utc":["2020-12-02","21:02:13"],"id":"geehc2f","link_id":"t3_k5c1ro","parent_id":"t3_k5c1ro","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/geehc2f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghfbc40":{"author":"TheVicePresident","author_fullname":"t2_5nt1i","author_premium":false,"banned_at_utc":null,"body":"The Pushshift website links to a GoFundMe page in the donations section but it is paused. Given the recent growth in use and the increased expenses with the new servers, I want to make sure that everyone who wants to donate can donate. I just donated through the Quick Donations link on the same page but I feel like some percent of people are going to to go the GoFundMe, see its paused, and then decide it isn't worth the trouble and end up not donating, and that's just money left on the table.","created_utc":["2020-12-03","02:20:18"],"id":"gefn89o","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gefn89o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghfyb08":{"author":"Stuck_In_the_Matrix","author_fullname":"t2_bk1iz","author_premium":false,"banned_at_utc":null,"body":"Hey there! Thanks for bringing this to my attention. I think the timeframe may have just ended. Pushshift has a donations page here: https:\/\/pushshift.io\/donations\/\n\nRight now it is only fixed amounts because the API changed and I need to update the code to allow people to select their own donation amount.\n\nThis method also has less fees involved on my end so its better overall. \n\nThanks for thinking of donating!","created_utc":["2020-12-03","04:35:01"],"id":"geg2vmr","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/geg2vmr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh4knfr":{"author":"francisco_rodriguez","author_fullname":"t2_1t4dkgl","author_premium":false,"banned_at_utc":null,"body":"I think Gab changed the API endpoint around May 2020 and the last dump is from August 2019. Maybe data from August 2019-May 2020 could be added to the gab dumps.","created_utc":["2020-12-03","12:30:46"],"id":"geh3ddt","link_id":"t3_k3ji22","parent_id":"t1_geean9x","permalink":"\/r\/pushshift\/comments\/k3ji22\/reddit_dumps\/geh3ddt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh4ro1x":{"author":"blakebockrath","author_fullname":"t2_4s21bz3","author_premium":false,"banned_at_utc":null,"body":"There\u2019s definitely a delay in data ingestion. I couldn\u2019t tell you by how much but If you are looking at a newer subreddit it might not be in the database yet.","created_utc":["2020-12-04","16:17:39"],"id":"geli0ug","link_id":"t3_k6k2q2","parent_id":"t3_k6k2q2","permalink":"\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/geli0ug\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghkwf05":{"author":"blakebockrath","author_fullname":"t2_4s21bz3","author_premium":false,"banned_at_utc":null,"body":"Does anyone know what the benefits of becoming a patron are? I\u2019ve been using the api a bit and would like to become one unless the benefits are nothing and in that case I\u2019d just make a quick donation.","created_utc":["2020-12-04","16:21:46"],"id":"gelig67","link_id":"t3_k5m2ko","parent_id":"t3_k5m2ko","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gelig67\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghlnao8":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"None of my requests are going through no matter how man submissions are pulled","created_utc":["2020-12-04","18:16:43"],"id":"gelvoj2","link_id":"t3_k5c1ro","parent_id":"t1_geehc2f","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelvoj2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh1yyt8":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"How did you solve it?","created_utc":["2020-12-04","18:17:03"],"id":"gelvq16","link_id":"t3_k5c1ro","parent_id":"t1_geixjye","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelvq16\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh23nyy":{"author":"mtodavk","author_fullname":"t2_6e2c3","author_premium":false,"banned_at_utc":null,"body":"I wasn\u2019t passing a time stamp in as the right data type. \ud83d\ude05","created_utc":["2020-12-04","18:20:22"],"id":"gelw4zx","link_id":"t3_k5c1ro","parent_id":"t1_gelvq16","permalink":"\/r\/pushshift\/comments\/k5c1ro\/non_200_code_525\/gelw4zx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh3b4ey":{"author":"abrownn","author_fullname":"t2_e7va4","author_premium":false,"banned_at_utc":null,"body":"Yes, there's currently about an 8.5 hour delay.","created_utc":["2020-12-04","19:32:05"],"id":"gem561g","link_id":"t3_k6k2q2","parent_id":"t3_k6k2q2","permalink":"\/r\/pushshift\/comments\/k6k2q2\/delay_in_data_ingestion\/gem561g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh127pp":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"I imagine it's a minority of subreddits that have more than 1000 posts in a 24-hour period. For those subs you can just use PRAW to see if you can get more than 24 hours away while checking the 1000 newest posts.","created_utc":["2020-12-04","19:53:11"],"id":"gem7v95","link_id":"t3_k60oeo","parent_id":"t3_k60oeo","permalink":"\/r\/pushshift\/comments\/k60oeo\/best_way_of_counting_number_of_posts_in_a_sub_in\/gem7v95\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh128tk":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2020-12-04","20:23:10"],"id":"gembo0m","link_id":"t3_k6q8qi","parent_id":"t3_k6q8qi","permalink":"\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/gembo0m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh12ezh":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"The subscriber count as of the most recent submission in the subreddit https:\/\/api.pushshift.io\/reddit\/submission\/search?subreddit=pushshift&amp;size=1&amp;fields=subreddit_subscribers\n\nThe current count\nhttps:\/\/api.reddit.com\/r\/pushshift\/about  \n`\"subscribers\":`","created_utc":["2020-12-05","01:41:15"],"id":"gend64y","link_id":"t3_k6q8qi","parent_id":"t3_k6q8qi","permalink":"\/r\/pushshift\/comments\/k6q8qi\/number_of_subreddit_members\/gend64y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh12ikx":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"No special benefits currently.","created_utc":["2020-12-05","02:12:15"],"id":"genge1t","link_id":"t3_k5m2ko","parent_id":"t1_gelig67","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/genge1t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh18v5x":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Is that the PSAW API wrapper? It looks like it. If so, it handles rate limiting automatically.","created_utc":["2020-12-05","17:56:11"],"id":"gepgz71","link_id":"t3_k77z3i","parent_id":"t3_k77z3i","permalink":"\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/gepgz71\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gh19reh":{"author":"BadAtPr0gramming","author_fullname":"t2_96ftdm8f","author_premium":false,"banned_at_utc":null,"body":"Yeah, I'm using psaw to access the data on pushshift. I thought it handled it automatically, but just wanted to double check that I wasn't causing any issues. Thanks for the info.","created_utc":["2020-12-05","18:01:51"],"id":"gephklz","link_id":"t3_k77z3i","parent_id":"t1_gepgz71","permalink":"\/r\/pushshift\/comments\/k77z3i\/question_about_rate_limits_and_data_usage\/gephklz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghflwhu":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"From the [docs](https:\/\/github.com\/pushshift\/reddit_sse_stream)\n&gt;I highly encourage you to connect using compression headers to conserve bandwidth if you use the full feed.\n\n&gt;`curl --verbose --compressed 'http:\/\/stream.pushshift.io\/?type=comments'`\n\nThen you can filter to just the parts of the stream you need like only comments and just the body and link http:\/\/stream.pushshift.io\/?type=comments&amp;filter=body,permalink\n\nOtherwise you could use something like PSAW https:\/\/github.com\/dmarx\/psaw or the pushshift api directly to offload most of the keyword searching\n\nFor comments something like https:\/\/api.pushshift.io\/reddit\/comment\/search?q=keyword&amp;size=100&amp;fields=permalink,body\n\nFor submissions something like\nhttps:\/\/api.pushshift.io\/reddit\/submission\/search?q=keyword&amp;size=100&amp;fields=permalink,selftext","created_utc":["2020-12-06","06:03:38"],"id":"gerylby","link_id":"t3_k7ak8m","parent_id":"t3_k7ak8m","permalink":"\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/gerylby\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggrne63":{"author":"EsssQueee","author_fullname":"t2_192397nx","author_premium":false,"banned_at_utc":null,"body":"thanks!  \nI am using the python sse client so I guess it is already using the compression headers for request. But filtering does seem to help. If I use the filter to include only selftext,url,body and permalink I am able to get it down to 30KBps.","created_utc":["2020-12-06","07:08:52"],"id":"ges6wuy","link_id":"t3_k7ak8m","parent_id":"t1_gerylby","permalink":"\/r\/pushshift\/comments\/k7ak8m\/httpstreampushshiftio_data_usage\/ges6wuy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggro4w4":{"author":"OkRepresentative3046","author_fullname":"t2_94k9zs5t","author_premium":false,"banned_at_utc":null,"body":"can I still request to have my data deleted?","created_utc":["2020-12-08","08:51:26"],"id":"gf15idu","link_id":"t3_jwj00o","parent_id":"t1_gcw3e4h","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf15idu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggro9ds":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"Seems like a waste of time to try to reconstruct reddit. Why not just create a doc with links to the relevant threads you want to review? The researcher can just click the link and review the comment thread on reddit? I'm not seeing the value in trying to do it on your own.\n\nAlso, youd need to create a detailed web app with a complex gui. It would be a lot of work.","created_utc":["2020-12-08","16:24:19"],"id":"gf1zmqd","link_id":"t3_k92svp","parent_id":"t3_k92svp","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf1zmqd\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggrookx":{"author":"Deertreetie","author_fullname":"t2_ha3r5c2","author_premium":false,"banned_at_utc":null,"body":"The aim is to import it into a qualitative data analysis program where the text can be coded. It can be in a PDF, excell file or text document of some sort.\n\nI could do this manually by doing screen captures of each thread and then importing it into the qualitative analysis program. I'm looking to save time (there will be 300+ reddit convos to qualitatively code) by using python in some way.\n\nI figure since I have the submission self-text, the comment text, the position of the comment in the comment tree, and the link ids, that there should be a way to code an output of a text file (either in one file or a separate file for each thread) where you can read the full discussion and see who replied to who. \n\nOnly the actual text is needed and maybe some metadata(usernames, dates, votes, etc). No graphical interface is needed. A comment that replies to another comment can be represented by an indent or even labeled by its position in the comment tree. \n\nWould this require something complex and time consuming like a webapp with a complex gui?","created_utc":["2020-12-08","16:36:41"],"id":"gf20wtw","link_id":"t3_k92svp","parent_id":"t1_gf1zmqd","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf20wtw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggqu82r":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"Oh got it. I've worked with that kind of software before. Each comment should have a parent id that will identify which comment it is replying to. Why not try writing the comments to a text file that maintains the parent\/child comment order?","created_utc":["2020-12-08","16:43:09"],"id":"gf21ln1","link_id":"t3_k92svp","parent_id":"t1_gf20wtw","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf21ln1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggtb5u4":{"author":"pc_4_life","author_fullname":"t2_25w85e","author_premium":false,"banned_at_utc":null,"body":"One other thing, I think the link id only points to the parent post and not parent comment.","created_utc":["2020-12-08","17:01:51"],"id":"gf23oc7","link_id":"t3_k92svp","parent_id":"t1_gf20wtw","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf23oc7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggtj1vc":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Sure! request via the [Data Deletion Request Megathread](https:\/\/www.reddit.com\/r\/pushshift\/comments\/hixijx\/data_deletion_request_megathread\/) or DM or Reddit-Chat abrownn","created_utc":["2020-12-08","17:41:46"],"id":"gf28e6d","link_id":"t3_jwj00o","parent_id":"t1_gf15idu","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf28e6d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggu50ho":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Yes, link\\_id will always be the top level submission of the comment tree.","created_utc":["2020-12-08","17:41:46"],"id":"gf28e6h","link_id":"t3_k92svp","parent_id":"t1_gf23oc7","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf28e6h\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggu50yb":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"pushshift doesn't have it's own privacy policy but requests are logged for diagnostics and abuse.","created_utc":["2020-12-08","18:10:38"],"id":"gf2byxe","link_id":"t3_k955jc","parent_id":"t3_k955jc","permalink":"\/r\/pushshift\/comments\/k955jc\/what_does_pushshift_log_by_way_of_api_users_im\/gf2byxe\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg7dhp0":{"author":"OkRepresentative3046","author_fullname":"t2_94k9zs5t","author_premium":false,"banned_at_utc":null,"body":"thank you!","created_utc":["2020-12-08","21:24:44"],"id":"gf31b5x","link_id":"t3_jwj00o","parent_id":"t1_gf28e6d","permalink":"\/r\/pushshift\/comments\/jwj00o\/data_deletionopt_out_thread\/gf31b5x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg91711":{"author":"Deertreetie","author_fullname":"t2_ha3r5c2","author_premium":false,"banned_at_utc":null,"body":"Sounds like a good idea. I'm new to python and have mostly been following tutorials. I think I might have something I can tweak to attempt this. Do you have any resources that might help with this as well? Thank you for your help by the way.","created_utc":["2020-12-08","23:25:45"],"id":"gf3gue5","link_id":"t3_k92svp","parent_id":"t1_gf21ln1","permalink":"\/r\/pushshift\/comments\/k92svp\/how_to_reconstruct_reddit_threads_with_submission\/gf3gue5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gge6x1b":{"author":"Xenc","author_fullname":"t2_4v98f","author_premium":true,"banned_at_utc":null,"body":"You could also put a couple of wallet addresses on there, say for Ethereum and Bitcoin.","created_utc":["2020-12-10","18:27:58"],"id":"gfananv","link_id":"t3_ka9j31","parent_id":"t3_ka9j31","permalink":"\/r\/pushshift\/comments\/ka9j31\/cryptocurrency_donations\/gfananv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gge74zz":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Note that this doesn't only occur between characters, e.g. `5\"9` or `wasn\"t` here, but also at the beginning and end of strings, e.g. `he said \"this\" and I` . Shouldn't these be escaped on the API side of things?","created_utc":["2020-12-10","22:36:55"],"id":"gfbjbwj","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbjbwj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfw2s3y":{"author":"TheElectricSlide2","author_fullname":"t2_1bwcu6ye","author_premium":false,"banned_at_utc":null,"body":"r","created_utc":["2020-12-10","23:14:32"],"id":"gfbnxqv","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbnxqv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfw6v1p":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"Strange, that looks like `5'9`  when viewed [via browser](http:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=2r2mx5).","created_utc":["2020-12-10","23:39:57"],"id":"gfbr1hx","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbr1hx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfz4mp6":{"author":"meager_narration","author_fullname":"t2_8yakpzil","author_premium":false,"banned_at_utc":null,"body":"How are you reading it? In general I use pandas in Python and I don't have issues with it.","created_utc":["2020-12-11","00:48:54"],"id":"gfbucqs","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfbucqs\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"go8omq8":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Yeah! I noticed that when searching by id too! It seems that adding `fields=selftext,id` to the query helped, though I'm not sure why","created_utc":["2020-12-11","02:09:00"],"id":"gfc5bvl","link_id":"t3_kanjiv","parent_id":"t1_gfbr1hx","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfc5bvl\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gga2tvu":{"author":"ToThePastMe","author_fullname":"t2_yi8qj","author_premium":false,"banned_at_utc":null,"body":"Just curious: Is it through the API or using Reddit Submission \/ Comments Dump file? Because I never ran into that issue after parsing over 10 full dumps with python+pandas (someone else mentioned that too), meaning millions of submission records. And how did you read that file \/ process the API result? Is it possible that the tool you used unescaped some characters or replaced ' by \"?","created_utc":["2020-12-11","02:40:45"],"id":"gfc9nya","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfc9nya\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfvzxom":{"author":"noroom","author_fullname":"t2_2tdxy","author_premium":false,"banned_at_utc":null,"body":"They are when they need to be.\n\nExample: https:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=k4owfz\n\nMeanwhile, you showed something that appears to have replaced the single quote with a double quote.\n\nhttps:\/\/api.pushshift.io\/reddit\/search\/submission\/?ids=2r2mx5\n\nThis is something on your end being weird.","created_utc":["2020-12-11","04:15:26"],"id":"gfckegt","link_id":"t3_kanjiv","parent_id":"t1_gfbjbwj","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfckegt\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffodn9":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Great questions, thanks - I'm realizing that this is probably an issue on my end. I'm using the API with the python requests and json libraries:\n\nresponse = requests.get(URL).json()\nfor post_obj in response['data']:\n   post_text = post_obj['selftext']\n\nIt appears to also be breaking on emoticons, e.g. :) :(","created_utc":["2020-12-11","14:35:53"],"id":"gfdwu1g","link_id":"t3_kanjiv","parent_id":"t1_gfc9nya","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfdwu1g\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfkp2u4":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"Flairing this as **spoiler**, the spoiler being that it's a code issue, not an API issue","created_utc":["2020-12-11","14:51:11"],"id":"gfdy05p","link_id":"t3_kanjiv","parent_id":"t3_kanjiv","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfdy05p\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfmxkhx":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"Yes, banned and quarantined data is available on Pushshift.\n\nThe easiest thing for you to do is get the data from  Google BigQuery. The data dumps on BigQuery stop at 2019, however. It\u2019s going to still require you to use SQL commands: there\u2019s no getting around at least basic coding. You will need to look up SQL queries for your specific use case.\n\nThe Pushshift data are free and public on BigQuery. You\u2019ll need to register and create a project in the Google developer console. Once you\u2019ve done that, you can query the public data dumps.\n\nI recommend searching Stack Overflow for how to do this.","created_utc":["2020-12-11","15:11:03"],"id":"gfdzlkb","link_id":"t3_kb217n","parent_id":"t3_kb217n","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfdzlkb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggcbmbw":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"As someone who isn't familiar at all with BigQuery (but is familiar with the Pushshift API and PSAW): what is the benefit to using BigQuery? Is accessing the Pushshift database easier if you use it?","created_utc":["2020-12-11","16:37:26"],"id":"gfe7wyq","link_id":"t3_kb217n","parent_id":"t1_gfdzlkb","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfe7wyq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ggcdhx1":{"author":"real_jabb0","author_fullname":"t2_4cyqprt4","author_premium":false,"banned_at_utc":null,"body":"I would guess that the query complexity one can achieve in BigQuery is much higher than the Pushshift API (which is backed by elasticsearch?). Also it might have a higher availability. I assume the downside is the data actuality as the most recent data is not in there(?).","created_utc":["2020-12-11","18:51:56"],"id":"gfenp64","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfenp64\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":true},"gffzuad":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"In my use cases, the chief benefit of the Google BigQuery dumps has been efficiently extracting full archives of subreddits without having to make expensive and slow API calls.\n\nOnce I have archived data from BigQuery, I augment it with \u201clive\u201d data from Pushshift\u2019s API (using PSAW).\n\nI also use BigQuery\u2019s computational power to create very large network edgelists, which is computationally expensive.","created_utc":["2020-12-11","19:06:08"],"id":"gfeptrc","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfeptrc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg202m":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"How does BigQuery get it's data? Do you fetch it from Reddit? Does it work with a downloaded Pushshift dump? Or is it somehow able to access the Pushshift API in a faster way?","created_utc":["2020-12-11","20:04:46"],"id":"gff11h3","link_id":"t3_kb217n","parent_id":"t1_gfeptrc","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gff11h3\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfvdby8":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I'm pretty sure Stuck_In_the_Matrix uploads it there himself. I'm not sure whether it's live or out of date right now though.","created_utc":["2020-12-11","21:25:09"],"id":"gffgd7l","link_id":"t3_kb217n","parent_id":"t1_gff11h3","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffgd7l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfvdoaj":{"author":"MFA_Nay","author_fullname":"t2_rmmen","author_premium":false,"banned_at_utc":null,"body":"For people without coding backgrounds the web GUI is nice, the SQL-like programming language is well documented and if you're dealing with large amounts of Reddit data using Google's cloud severs can work out quicker then using you local device(s).","created_utc":["2020-12-11","21:37:47"],"id":"gffirst","link_id":"t3_kb217n","parent_id":"t1_gfe7wyq","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffirst\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg756mh":{"author":"MFA_Nay","author_fullname":"t2_rmmen","author_premium":false,"banned_at_utc":null,"body":"Bit hazy on exact details but Felipe Hoffa (then working for Google) uploaded it with Baumgartner's consent in 2015. Around 2018 Pushshift's livestream was integrated into BigQuery's Pushshift project.","created_utc":["2020-12-11","21:40:03"],"id":"gffj7i2","link_id":"t3_kb217n","parent_id":"t1_gffgd7l","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gffj7i2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gg785y0":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I think I would need a lot more info about what the bot is doing to make a recommendation. Just starting a day ago rather than at a certain date is easy.\n\n    from datetime import datetime, timedelta\n\n    one_day_ago = datetime.utcnow() - timedelta(days=1)\n\nBut saving a whole bunch of data so it can be reused when the script restarts depends a lot more on what exactly you're doing.","created_utc":["2020-12-11","21:46:26"],"id":"gffkekw","link_id":"t3_kb9930","parent_id":"t3_kb9930","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffkekw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfdzlkb":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"I'll share my script in a PasteBin link if if would help.\n\nhttps:\/\/pastebin.com\/hcKDm2gM","created_utc":["2020-12-11","21:47:32"],"id":"gffkm7b","link_id":"t3_kb9930","parent_id":"t1_gffkekw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffkm7b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfe7wyq":{"author":"TheElectricSlide2","author_fullname":"t2_1bwcu6ye","author_premium":false,"banned_at_utc":null,"body":"New mods?","created_utc":["2020-12-11","22:07:05"],"id":"gffodn9","link_id":"t3_kb6lhm","parent_id":"t3_kb6lhm","permalink":"\/r\/pushshift\/comments\/kb6lhm\/question_about_data_consistency\/gffodn9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfenp64":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Unless I'm completely misremembering something, doing a `search_submissions` like that starts at the current date and goes backwards.\n\nAre you wanting to run that until it crashes, lets say it starts right now and gets back to a week ago, then when you start it again it starts at a week ago and keeps going? Or do you run it like once a day and want start at the current time and go back till yesterday when you hit posts you've already downloaded?\n\nAlso, what part is crashing? It might be simpler to figure out how to stop it from crashing than restarting it every time it does.","created_utc":["2020-12-11","23:04:55"],"id":"gffz2mj","link_id":"t3_kb9930","parent_id":"t1_gffkm7b","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffz2mj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfeptrc":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"I think when it crashes, it's something to do with requests. It varies, so I can't really narrow it down. The most common thing is running out of attempts to get a page. What I want to be able to do is to set it running, and it will trawl through the histories of the specified subreddits, downloading images until it hits the beginning. Once it has hit the beginning for all subs, start tracking any new submissions.","created_utc":["2020-12-11","23:08:05"],"id":"gffzojc","link_id":"t3_kb9930","parent_id":"t1_gffz2mj","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gffzojc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gff11h3":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"It's not you, it's just slow. Here's my query time graph for a really simple diagnostic query over the last 7 days in seconds the api took to respond\n\nhttps:\/\/i.imgur.com\/VvSAiql.png\n\nSome days it's faster but a lot of time it's just overwhelmed in general.","created_utc":["2020-12-11","23:08:58"],"id":"gffzuad","link_id":"t3_kbap4d","parent_id":"t3_kbap4d","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gffzuad\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffgd7l":{"author":"ToThePastMe","author_fullname":"t2_yi8qj","author_premium":false,"banned_at_utc":null,"body":"Hmmm I'm curious about it, I'll also check quick next week when back home. Never really had issues with the requests package but then I don't use it very frequently.","created_utc":["2020-12-11","23:14:42"],"id":"gfg0pvo","link_id":"t3_kanjiv","parent_id":"t1_gfdwu1g","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfg0pvo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffirst":{"author":"gardenfractals","author_fullname":"t2_u2zj9","author_premium":false,"banned_at_utc":null,"body":"I switched to using [psaw](https:\/\/github.com\/dmarx\/psaw) and haven't had any issues there. Thanks for your help!","created_utc":["2020-12-11","23:19:24"],"id":"gfg19zx","link_id":"t3_kanjiv","parent_id":"t1_gfg0pvo","permalink":"\/r\/pushshift\/comments\/kanjiv\/whats_the_best_way_to_handle_a_json_parse_when\/gfg19zx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffj7i2":{"author":"mattster42","author_fullname":"t2_7mdlw","author_premium":true,"banned_at_utc":null,"body":"Thanks, that's good to know. And a good reminder to donate.","created_utc":["2020-12-11","23:25:31"],"id":"gfg202m","link_id":"t3_kbap4d","parent_id":"t1_gffzuad","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfg202m\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfi11m7":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"But is it crashing trying to access the pushshift api, the reddit api or when it downloads the pictures? How many pictures on average does it download before crashing? And do you have any idea how many you're expecting if it did manage to download the entire history?\n\nIt's possible this is reddit telling you that you're downloading too many things too fast and you should slow down. In which case restarting it each time is just a stop gap and won't end up helping that much.\n\nCould you post the whole error message?","created_utc":["2020-12-11","23:25:40"],"id":"gfg20rk","link_id":"t3_kb9930","parent_id":"t1_gffzojc","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg20rk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbjbwj":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"If it actually manages to get the whole history, there will be several thousand images. Probably more than 20k. I have tried putting in a 10-second gap between each image download, but it still fails eventually. I think most of the time it gets around 2000 images but I did manage to get 15k once.","created_utc":["2020-12-11","23:29:04"],"id":"gfg2f4d","link_id":"t3_kb9930","parent_id":"t1_gfg20rk","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg2f4d\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbnxqv":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Could you try it again and post the full error message when it crashes?","created_utc":["2020-12-11","23:52:44"],"id":"gfg57iw","link_id":"t3_kb9930","parent_id":"t1_gfg2f4d","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfg57iw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbr1hx":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-12-12","09:45:49"],"id":"gfi11m7","link_id":"t3_kb217n","parent_id":"t3_kb217n","permalink":"\/r\/pushshift\/comments\/kb217n\/too_bad_at_it_to_understand_any_of_this_please\/gfi11m7\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfbucqs":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"Here is the full error message:  \n[https:\/\/pastebin.com\/n3Xg5kvr](https:\/\/pastebin.com\/n3Xg5kvr)","created_utc":["2020-12-12","17:17:49"],"id":"gfjc0kq","link_id":"t3_kb9930","parent_id":"t1_gfg57iw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfjc0kq\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfc5bvl":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"https:\/\/github.com\/Watchful1\/Sketchpad\/blob\/master\/postDownloader.py","created_utc":["2020-12-12","21:47:59"],"id":"gfkp2u4","link_id":"t3_kbmezv","parent_id":"t3_kbmezv","permalink":"\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/gfkp2u4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfc9nya":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"The first one is an error downloading the image. You can fix that by putting a try\/except around the `r = requests.get(url)` line. The second is a bug in PSAW that was fixed about 6 months ago, so you need to update PSAW to the latest version.","created_utc":["2020-12-13","02:19:50"],"id":"gfm4vko","link_id":"t3_kb9930","parent_id":"t1_gfjc0kq","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm4vko\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfckegt":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"The latest version appears to be 0.0.12, and that's the version I have. I've not had PSAW for more than 6 months.","created_utc":["2020-12-13","02:25:36"],"id":"gfm62ce","link_id":"t3_kb9930","parent_id":"t1_gfm4vko","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm62ce\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfdwu1g":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Huh, you're right. It looks like this fix was never released as a new version. Which is unfortunately since it's literally in the part of the code that catches pushshift errors and retries instead of just crashing.\n\nYou could edit your PSAW file manually, [here's the commit](https:\/\/github.com\/dmarx\/psaw\/commit\/b61ec6f73e5566525a9b32e466843fd11817556d) with the fix. So you would need to find where that code is stored and just make that change yourself.\n\nOr you could uninstall PSAW and install it directly from github, like this `pip install git+https:\/\/github.com\/dmarx\/psaw`.\n\nYou might also be able to just try again. Pushshift has been having lots of stability issues the last week, but for some reason it's much, much more stable today. If you fix that first issue by putting the try\/except around the image download it might just work.","created_utc":["2020-12-13","02:33:04"],"id":"gfm7j3o","link_id":"t3_kb9930","parent_id":"t1_gfm62ce","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfm7j3o\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfdy05p":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"Ah, OK. I've made those changes and re-run the code, so I'll see if anything messes up or whether it's all good now. Thanks for the help.","created_utc":["2020-12-13","02:49:59"],"id":"gfmal04","link_id":"t3_kb9930","parent_id":"t1_gfm7j3o","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfmal04\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg0pvo":{"author":"RyanRowley007","author_fullname":"t2_7jxjdlzq","author_premium":false,"banned_at_utc":null,"body":"you legend, thank you","created_utc":["2020-12-13","05:35:49"],"id":"gfmxkhx","link_id":"t3_kbmezv","parent_id":"t1_gfkp2u4","permalink":"\/r\/pushshift\/comments\/kbmezv\/data_download_for_user\/gfmxkhx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg19zx":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"If I put the `r = requests.get(url)` line inside the `try\/except` block, it tells me that `r` might have been referenced before assignment. Wouldn't I have to put a bunch of other stuff inside the `try\/except` block as well?","created_utc":["2020-12-13","16:09:13"],"id":"gfp3ibw","link_id":"t3_kb9930","parent_id":"t1_gfm7j3o","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfp3ibw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ghtlrqv":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"You need to `return` in the `except` part. If it can't download the image, you need to skip it rather than keep going in the function.","created_utc":["2020-12-13","21:13:03"],"id":"gfq92lc","link_id":"t3_kb9930","parent_id":"t1_gfp3ibw","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfq92lc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfananv":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"And if I do that, it will work properly?","created_utc":["2020-12-13","21:18:52"],"id":"gfqa4zg","link_id":"t3_kb9930","parent_id":"t1_gfq92lc","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfqa4zg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf2byxe":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I don't know, try it and see","created_utc":["2020-12-13","21:20:12"],"id":"gfqae0f","link_id":"t3_kb9930","parent_id":"t1_gfqa4zg","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfqae0f\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffkekw":{"author":"Arag0ld","author_fullname":"t2_2z32g6fu","author_premium":false,"banned_at_utc":null,"body":"It kind of works, but the files seem to stop downloading at around 100-200 and I'm not sure why","created_utc":["2020-12-14","13:34:37"],"id":"gfswjit","link_id":"t3_kb9930","parent_id":"t1_gfqae0f","permalink":"\/r\/pushshift\/comments\/kb9930\/caching_the_last_download\/gfswjit\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffkm7b":{"author":"transfo47","author_fullname":"t2_obf65","author_premium":false,"banned_at_utc":null,"body":"&gt;but a lot of time it's just overwhelmed in general.\n\nI feel that.","created_utc":["2020-12-15","02:46:08"],"id":"gfvdby8","link_id":"t3_kbap4d","parent_id":"t1_gffzuad","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfvdby8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffz2mj":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Oddly it's actually been better the last few days\n\nhttps:\/\/i.imgur.com\/K1zMmvF.png\n\nso maybe you just have to struggle along until life gets better :)","created_utc":["2020-12-15","02:49:20"],"id":"gfvdoaj","link_id":"t3_kbap4d","parent_id":"t1_gfvdby8","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gfvdoaj\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gffzojc":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"This does happen, but only very occasionally.","created_utc":["2020-12-15","06:08:34"],"id":"gfvzxom","link_id":"t3_kdd06y","parent_id":"t3_kdd06y","permalink":"\/r\/pushshift\/comments\/kdd06y\/does_pushshiftpsaw_ever_add_things_out_of_order\/gfvzxom\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg20rk":{"author":"KKingler","author_fullname":"t2_ngpoa","author_premium":true,"banned_at_utc":null,"body":"Try removeddit","created_utc":["2020-12-15","06:34:33"],"id":"gfw2s3y","link_id":"t3_kddn9l","parent_id":"t3_kddn9l","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfw2s3y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg2f4d":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Looks like the site is down: https:\/\/snew.notabug.io\/","created_utc":["2020-12-15","07:16:22"],"id":"gfw6v1p","link_id":"t3_kddn9l","parent_id":"t3_kddn9l","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfw6v1p\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfg57iw":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2020-12-16","01:28:59"],"id":"gfz4mp6","link_id":"t3_kddn9l","parent_id":"t1_gfw2s3y","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/gfz4mp6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfjc0kq":{"author":"mattster42","author_fullname":"t2_7mdlw","author_premium":true,"banned_at_utc":null,"body":"I noticed it seemed to speed up those days, too. But it's been slow again the past day or so. Does your graph reflect this?","created_utc":["2020-12-18","02:15:08"],"id":"gg756mh","link_id":"t3_kbap4d","parent_id":"t1_gfvdoaj","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gg756mh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm4vko":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Yeah, it's not as bad as it was a week ago, but it's definitely worse\n\nhttps:\/\/i.imgur.com\/NZbdyy3.png","created_utc":["2020-12-18","02:40:38"],"id":"gg785y0","link_id":"t3_kbap4d","parent_id":"t1_gg756mh","permalink":"\/r\/pushshift\/comments\/kbap4d\/is_this_query_inefficientresource_taxing\/gg785y0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm62ce":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Maybe it's just that recent months have seen bigger relative increases, then the relative increases of recent years.","created_utc":["2020-12-18","03:31:02"],"id":"gg7dhp0","link_id":"t3_kfa0gq","parent_id":"t3_kfa0gq","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gg7dhp0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfm7j3o":{"author":"meyerovb","author_fullname":"t2_r4y42","author_premium":true,"banned_at_utc":null,"body":"What\u2019s ur filter? If u pull 2019 they year, then you pull 1\/2019-12\/2019 separately and add them up, you get different post count?","created_utc":["2020-12-18","15:56:29"],"id":"gg91711","link_id":"t3_kfa0gq","parent_id":"t3_kfa0gq","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gg91711\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfmal04":{"author":"Anti-politik","author_fullname":"t2_4ycleuno","author_premium":false,"banned_at_utc":null,"body":"The ingest often gets behind. Try again in a few hours, or consider the beta API: https:\/\/beta.pushshift.io\/redoc","created_utc":["2020-12-18","20:49:11"],"id":"gga2tvu","link_id":"t3_kfp2fl","parent_id":"t3_kfp2fl","permalink":"\/r\/pushshift\/comments\/kfp2fl\/having_trouble_getting_data_today\/gga2tvu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfp3ibw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Are you talking about searching link posts to your domain or just the domain included anywhere in a post? Pretty sure both of those are already possible.","created_utc":["2020-12-19","09:55:18"],"id":"ggcbmbw","link_id":"t3_kg3cg5","parent_id":"t3_kg3cg5","permalink":"\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/ggcbmbw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfq92lc":{"author":"CaptainMelon","author_fullname":"t2_51smd","author_premium":false,"banned_at_utc":null,"body":"Both, searching for a link included in the comments, for now the api search on the text only. For example if I search for \"nosdeputes\" in the comments api, it doesn't return this comment linking to the website \"nosdeputes.fr\": [https:\/\/www.reddit.com\/r\/france\/comments\/keb25k\/fil\\_pour\\_demander\\_aux\\_modos\\_demp%C3%AAcher\\_la\/gg1movd\/](https:\/\/www.reddit.com\/r\/france\/comments\/keb25k\/fil_pour_demander_aux_modos_demp%C3%AAcher_la\/gg1movd\/?utm_source=reddit&amp;utm_medium=web2x&amp;context=3)","created_utc":["2020-12-19","10:26:50"],"id":"ggcdhx1","link_id":"t3_kg3cg5","parent_id":"t1_ggcbmbw","permalink":"\/r\/pushshift\/comments\/kg3cg5\/feature_request_allow_to_search_on_the_links\/ggcdhx1\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfqa4zg":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"yes exactly!","created_utc":["2020-12-19","22:00:58"],"id":"gge6x1b","link_id":"t3_kfa0gq","parent_id":"t1_gg91711","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gge6x1b\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfqae0f":{"author":"RedditApiUser","author_fullname":"t2_7gr55rnn","author_premium":false,"banned_at_utc":null,"body":"Ive been doing research for a data analytics company and have seen this happen in several groups and topics","created_utc":["2020-12-19","22:02:54"],"id":"gge74zz","link_id":"t3_kfa0gq","parent_id":"t1_gg7dhp0","permalink":"\/r\/pushshift\/comments\/kfa0gq\/data_pulling\/gge74zz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gfswjit":{"author":"git-commit-die","author_fullname":"t2_3rv8esy0","author_premium":true,"banned_at_utc":null,"body":"The t1_ and t3_ prefixes are [type prefixes](https:\/\/www.reddit.com\/dev\/api\/#fullnames), if a fullname *(e.g. the parent ID)* starts with `t1_` it's a comment and if it starts with `t3_` it's a link\/submission.\n\nSo if you have the parent ID t1_d5z2b0c, you could get the parent body by making a follow up request for only that ID *(d5z2b0c)*. You can also use the `filter` parameter to have it only return the body.\n\n`...\/reddit\/search\/comment\/?filter=body&amp;ids=d5z2b0c`\n\nTop-level comments will have a parent ID that corelates to a submission, but the process would pretty much be the same, just use the submission endpoint instead. Note that submission text bodies use `selftext` instead of `body`, and not all submissions have text bodies.\n\n`...\/reddit\/search\/submission\/?filter=selftext&amp;ids=4vlbv9`","created_utc":["2020-12-23","02:00:30"],"id":"ggqu82r","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggqu82r\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf1zmqd":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Aggs is disabled because usage of it was overwhelming the service.","created_utc":["2020-12-23","06:36:06"],"id":"ggrne63","link_id":"t3_kilscl","parent_id":"t3_kilscl","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggrne63\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf20wtw":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"Gotcha. \n\nIf you or anyone knows, will it be something that's offered in the new api? For money of course.","created_utc":["2020-12-23","06:44:06"],"id":"ggro4w4","link_id":"t3_kilscl","parent_id":"t1_ggrne63","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggro4w4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf21ln1":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"No real telling what will actually happen. The new api has been planned for literally two years now, but there's only progress when Stuck_In_the_Matrix has time.","created_utc":["2020-12-23","06:45:29"],"id":"ggro9ds","link_id":"t3_kilscl","parent_id":"t1_ggro4w4","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggro9ds\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf23oc7":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"Hey u\/Stuck_in_the_Matrix, can I give you some money to use aggs for like just one hour?\n\nEdit: So I just read through the post from November and descided to rescind this request and give you some money anyway. Cheers, looking forward to seeing aggs back soon hopefully.","created_utc":["2020-12-23","06:50:02"],"id":"ggrookx","link_id":"t3_kilscl","parent_id":"t1_ggro9ds","permalink":"\/r\/pushshift\/comments\/kilscl\/is_aggs_not_working_for_everyone\/ggrookx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf28e6h":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"The t prefixes are 'thing' types from the Reddit API. See the table titled 'type prefixes' here: https:\/\/www.reddit.com\/dev\/api\/\n\nYou can use and `ids` argument to specify the parent's id and `fields` to collect only body. You'll need to use the right endpoints for comments vs submissions, and adjust the `fields` for submissions (likely title + selftext)","created_utc":["2020-12-23","19:24:52"],"id":"ggtb5u4","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggtb5u4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gf3gue5":{"author":"PM_ME_UR_INSIGHTS","author_fullname":"t2_pk2aa","author_premium":false,"banned_at_utc":null,"body":"If you want a Dad Bod, just snack a lot and reduce your activity levels.","created_utc":["2020-12-23","20:33:01"],"id":"ggtj1vc","link_id":"t3_kigfrx","parent_id":"t3_kigfrx","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggtj1vc\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gerylby":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"banned_at_utc":null,"body":"Hilarious!","created_utc":["2020-12-23","23:47:57"],"id":"ggu50ho","link_id":"t3_kigfrx","parent_id":"t1_ggtj1vc","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggu50ho\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ges6wuy":{"author":"Nimtrix1849","author_fullname":"t2_9iwz5","author_premium":false,"banned_at_utc":null,"body":"Thanks!","created_utc":["2020-12-23","23:48:04"],"id":"ggu50yb","link_id":"t3_kigfrx","parent_id":"t1_ggtb5u4","permalink":"\/r\/pushshift\/comments\/kigfrx\/noob_question_getting_parent_body\/ggu50yb\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gepgz71":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"There's literally a thread pinned to the top of the subreddit explaining how to do this","created_utc":["2020-12-26","01:56:11"],"id":"gh127pp","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh127pp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gephklz":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"But it really works?","created_utc":["2020-12-26","01:56:32"],"id":"gh128tk","link_id":"t3_kk8ge4","parent_id":"t1_gh127pp","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh128tk\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gembo0m":{"author":"Vivid_Walrus","author_fullname":"t2_7cutv22v","author_premium":false,"banned_at_utc":null,"body":"Yes, though in fairness I've had a few drinks and didn't comprehend the part about it being a deleted account (even though it's literally right in the title, doh), so I may be an ass.  Not sure how they handle that.","created_utc":["2020-12-26","01:58:24"],"id":"gh12ezh","link_id":"t3_kk8ge4","parent_id":"t1_gh128tk","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh12ezh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gend64y":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"The deleted account shows up in pushshift but it can\u2019t be viewed as if it were active. Doesn\u2019t show up in regular Reddit either","created_utc":["2020-12-26","01:59:28"],"id":"gh12ikx","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh12ikx\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geli0ug":{"author":"_Xeet_","author_fullname":"t2_2yb5l98z","author_premium":false,"banned_at_utc":null,"body":"You need to send proof you owned the account but it does get removed if you send a request, read the pinned thread","created_utc":["2020-12-26","03:09:02"],"id":"gh18v5x","link_id":"t3_kk8ge4","parent_id":"t3_kk8ge4","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh18v5x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gem561g":{"author":"Plenty_Ad_2363","author_fullname":"t2_9gtbw16p","author_premium":false,"banned_at_utc":null,"body":"Sweet I didn\u2019t know this was possible to do.","created_utc":["2020-12-26","03:19:34"],"id":"gh19reh","link_id":"t3_kk8ge4","parent_id":"t1_gh18v5x","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/gh19reh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gem7v95":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"Kinda sorta? 100 results is an API limitation, it used to be higher but its been reduced to 100 for performance reasons.\nThe way around this is to paginate through the results.\n\nThere's a third party front end at https:\/\/camas.github.io\/reddit-search\/ that gives a more button that will request the next 100 results. It's not automatic but it's still a large step up from doing the pagination manually.","created_utc":["2020-12-26","08:45:34"],"id":"gh1yyt8","link_id":"t3_kkdlr5","parent_id":"t3_kkdlr5","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh1yyt8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gefn89o":{"author":"TheMedernShairluck","author_fullname":"t2_5kp5wq5q","author_premium":false,"banned_at_utc":null,"body":"Thx! I'm having a look at it now.\n\nIt's kinda odd: it gives the top 100 results, but when I press 'more', gives 99% of the same top 100 results. I'll play around with it a bit and see if I can do better.","created_utc":["2020-12-26","10:12:20"],"id":"gh23nyy","link_id":"t3_kkdlr5","parent_id":"t1_gh1yyt8","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh23nyy\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geg2vmr":{"author":"cyrilio","author_fullname":"t2_62dmi","author_premium":true,"banned_at_utc":null,"body":"This is a good question. Looks like that other comment has great link for it","created_utc":["2020-12-26","20:36:30"],"id":"gh3b4ey","link_id":"t3_kkdlr5","parent_id":"t3_kkdlr5","permalink":"\/r\/pushshift\/comments\/kkdlr5\/increase_the_number_of_search_results_on\/gh3b4ey\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelig67":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Why do you think that? It works on all subs other than private ones and maybe some quarantined ones.","created_utc":["2020-12-27","04:00:48"],"id":"gh4knfr","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/gh4knfr\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"genge1t":{"author":"ATownHoldItDown","author_fullname":"t2_8v1x6","author_premium":false,"banned_at_utc":null,"body":"I have a feeling you're using redditsearch.io, and your query is limited to 'today'.\n\nIf you search a sub that was banned a year ago, it won't show anything from today. Likewise, if you search for posts from two years ago for a sub that is 6 months old, it won't show anything.","created_utc":["2020-12-27","05:11:46"],"id":"gh4ro1x","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/gh4ro1x\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gzzbmik":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I'm planning to put up a torrent of the files. Stuck_In_the_Matrix is recompressing all the older ones which is useful for a couple reasons, so I'm waiting for that to be done rather than put together something that will be out of date in a few months.\n\nThe data is already in a database, that's what the API is backed by. The main limitation is the huge amount of traffic the API gets, so unless you can either provide a lots of bandwidth and processing power, it probably won't make a big difference.","created_utc":["2020-12-29","05:40:34"],"id":"ghcx2yg","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghcx2yg\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gzzfylo":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"My idea was to develop a pipeline to allow people to download the submissions and comments via bittorrent, and move that data into a local DB for analysis (vs the pushshift API) . This way people can do their work without any burden on the pushshift infra. I wouldn't host anything but the torrents.","created_utc":["2020-12-29","06:07:05"],"id":"ghczwu4","link_id":"t3_km63g6","parent_id":"t1_ghcx2yg","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghczwu4\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gedx32g":{"author":"i_like_the_idea","author_fullname":"t2_8umm4glt","author_premium":false,"banned_at_utc":null,"body":"How would you split it up?","created_utc":["2020-12-29","16:51:35"],"id":"ghe9f4t","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghe9f4t\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdro9x6":{"author":"FixShitUp","author_fullname":"t2_rqkfpt4","author_premium":false,"banned_at_utc":null,"body":"For reference: https:\/\/archive.org\/details\/reddit-comments-7z","created_utc":["2020-12-29","17:54:00"],"id":"gheg2ud","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/gheg2ud\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gee7j11":{"author":"RoflStomper","author_fullname":"t2_461ls","author_premium":false,"banned_at_utc":null,"body":"Absolutely! While pushshift is great, it is evident that it's a part-time venture. Some sort of distributed mirror taking a slightly different approach would be amazing for reliability and would definitely take load off the pushshift servers as heavy users (the ones likely to be causing issues) are more capable of working locally.","created_utc":["2020-12-29","20:02:49"],"id":"ghevdi5","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghevdi5\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geehc2f":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"Yup, similar idea. That was last updated in 2017.","created_utc":["2020-12-29","20:55:26"],"id":"ghf1tq2","link_id":"t3_km63g6","parent_id":"t1_gheg2ud","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghf1tq2\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelvoj2":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"I figure I would put it all in one torrent. Most clients let the user pick their files.","created_utc":["2020-12-29","20:56:33"],"id":"ghf1yo6","link_id":"t3_km63g6","parent_id":"t1_ghe9f4t","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghf1yo6\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelvq16":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"FYI the PushShift dataset is available on BigQuery: [https:\/\/pushshift.io\/using-bigquery-with-reddit-data\/](https:\/\/pushshift.io\/using-bigquery-with-reddit-data\/)","created_utc":["2020-12-29","22:13:26"],"id":"ghfbc40","link_id":"t3_km63g6","parent_id":"t3_km63g6","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghfbc40\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gelw4zx":{"author":"Ok-Resident-5794","author_fullname":"t2_99i909oi","author_premium":false,"banned_at_utc":null,"body":"I will point out that this is only possible for it being searchable via the pushshift api. The comments are continually put into files which are then available for download to other people, who then share them with others. So by way of the comments actually being deleted? It's impossible and they will exist forever. But by way of making sure they can't be searched via this 1 API? Yea.\n\nExample: All the comments\/posts that have been saved are available via torrent. https:\/\/www.reddit.com\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/","created_utc":["2020-12-29","23:41:30"],"id":"ghflwhu","link_id":"t3_kk8ge4","parent_id":"t1_gh19reh","permalink":"\/r\/pushshift\/comments\/kk8ge4\/is_it_possible_for_pushshift_to_delete_old_posts\/ghflwhu\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdqxlsx":{"author":"Paul-E0","author_fullname":"t2_9jqpn0d0","author_premium":false,"banned_at_utc":null,"body":"Good point, that is sufficient for a lot of people. Based on some posts I've seen here, it appears the BigQuery data goes from 2015 through 2019, so it might not be comprehensive enough for some purposes.\n\nFor me, I prefer having it stored locally so that queries don't go over the network and I don't need to think about a quota, bandwidth, or network latency.","created_utc":["2020-12-30","01:27:15"],"id":"ghfyb08","link_id":"t3_km63g6","parent_id":"t1_ghfbc40","permalink":"\/r\/pushshift\/comments\/km63g6\/is_there_interest_in_a_torrent\/ghfyb08\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdqy2hj":{"author":"AIArtisan","author_fullname":"t2_1230r07v","author_premium":false,"banned_at_utc":null,"body":"ive had no issues finding data from all subs I have tried. whats your query?","created_utc":["2020-12-31","07:29:06"],"id":"ghkwf05","link_id":"t3_kku7y9","parent_id":"t3_kku7y9","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/ghkwf05\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdrvq77":{"author":"PickleRickFanning","author_fullname":"t2_9a3bixh3","author_premium":false,"banned_at_utc":null,"body":"I am using an app (removedit) that pulls from push shift but on most of the bigger subs, it can't recover removed or deleted comments\/posts. On smaller subs this isn't usually a problem. Granted, I am using an app that may not be querying the api in the right way but it does work sometimes and others not. \n\nIs this because of settings possibly set by mods in the default subs (for example), the app not working correctly or something else entirely?","created_utc":["2020-12-31","14:36:51"],"id":"ghlnao8","link_id":"t3_kku7y9","parent_id":"t1_ghkwf05","permalink":"\/r\/pushshift\/comments\/kku7y9\/why_does_push_shift_only_work_on_some_subs\/ghlnao8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdsh6js":{"author":"BoomerFelonOwl","author_fullname":"t2_4wd1sjyg","author_premium":false,"banned_at_utc":null,"body":"- Get current UTC timestamp and initialize it for a loop\n- Make requests to different urls in chunks of 100 posts with the \"before\" http paramater getting the 100 most recent posts before that time \n- Save the timestamp of the post for the last post in the chunk to use in your next request \n- Break when the postTimestamp &lt; currentTimestamp - (24 * 60 * 60)","created_utc":["2021-01-02","14:34:06"],"id":"ghtlrqv","link_id":"t3_k9b72d","parent_id":"t3_k9b72d","permalink":"\/r\/pushshift\/comments\/k9b72d\/how_can_i_extract_new_posts_and_comments_added_to\/ghtlrqv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdsnx53":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Beta search for comments now just returns an internal server error.","created_utc":["2021-01-07","02:26:18"],"id":"gid847y","link_id":"t3_jc2cc8","parent_id":"t3_jc2cc8","permalink":"\/r\/pushshift\/comments\/jc2cc8\/pushshift_beta_ingest_now_available\/gid847y\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"ge4ymvd":{"author":"QishengLi","author_fullname":"t2_117l3k","author_premium":false,"banned_at_utc":null,"body":"May I ask, what is the `limit` parameter? I didn't find it in the documentation...","created_utc":["2021-01-10","22:06:43"],"id":"giskxwm","link_id":"t3_ih66b8","parent_id":"t3_ih66b8","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/giskxwm\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geean9x":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"As watchful1 stated `limit` appears to be an alias for `size` so both function exactly the same.","created_utc":["2021-01-11","17:42:07"],"id":"giw4137","link_id":"t3_ih66b8","parent_id":"t1_giskxwm","permalink":"\/r\/pushshift\/comments\/ih66b8\/difference_between_size_and_limit_and_are_they\/giw4137\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"geh3ddt":{"author":"lanfranchi","author_fullname":"t2_9x03s","author_premium":false,"banned_at_utc":null,"body":"anyone know why the files ended in april 2020?","created_utc":["2021-02-01","12:57:57"],"id":"gll3ve9","link_id":"t3_js69o5","parent_id":"t3_js69o5","permalink":"\/r\/pushshift\/comments\/js69o5\/how_to_get_all_submissions_posted_ever_on_reddit\/gll3ve9\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdmfy5k":{"author":"osiworx","author_fullname":"t2_84gi8ey8","author_premium":false,"banned_at_utc":null,"body":"They seem to be down since a few days now. I hope they come back anytime soon.","created_utc":["2021-02-02","08:27:22"],"id":"glpawn0","link_id":"t3_lambkn","parent_id":"t3_lambkn","permalink":"\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/glpawn0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdmreak":{"author":"IsilZha","author_fullname":"t2_66rue","author_premium":false,"banned_at_utc":null,"body":"Disappointing - over the last 6-8 months, the reddit side of pushshift is feeling more and more abandoned.  Got all these promises of a new ingest, and the beta went up, but then it kind of went no where after that - except disabling aggregations, really crippling a lot of use for the tool - and without migrating to the new ingest it's becoming less and less reliable with big lag time and huge data gaps now.","created_utc":["2021-02-02","21:49:57"],"id":"glrterp","link_id":"t3_lambkn","parent_id":"t3_lambkn","permalink":"\/r\/pushshift\/comments\/lambkn\/submissioncomment_ids_returns_an_empty_list\/glrterp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gdkeekw":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"I don't think it's related. Though hopefully the new infrastructure will reduce the amount of downtime once it's online.\n\nI've given up reporting the outages to him.","created_utc":["2021-02-02","21:54:27"],"id":"glru39s","link_id":"t3_lb4n9h","parent_id":"t3_lb4n9h","permalink":"\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/glru39s\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvitc0":{"author":"ShiningConcepts","author_fullname":"t2_orcvx","author_premium":false,"banned_at_utc":null,"body":"Yeah I love SITM and his work but he goes AWOL for frequent periods of time. I don't mean to judge since I don't know what's going on with him, but he can be hard to reach\/get updates from.","created_utc":["2021-02-03","07:18:42"],"id":"gltvbuv","link_id":"t3_lb4n9h","parent_id":"t1_glru39s","permalink":"\/r\/pushshift\/comments\/lb4n9h\/pushshift_down_due_to_revamping_of_infrastructure\/gltvbuv\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvrqhh":{"author":"MakeYourMarks","author_fullname":"t2_gje3d","author_premium":false,"banned_at_utc":null,"body":"I know this is unhelpful and you don't want to hear this, but nothing comes close. Depending on what you're doing, you might be able to get by with torrenting some of the static comments\/submissions files and drawing your data from there.","created_utc":["2021-02-03","07:28:49"],"id":"gltwb38","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/gltwb38\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvtz60":{"author":"Silver__Bug","author_fullname":"t2_53v0ksem","author_premium":false,"banned_at_utc":null,"body":"You can try the dumps available [Here](http:\/\/files.pushshift.io\/) or the bigquery api, though they are not up to date.","created_utc":["2021-02-03","07:53:10"],"id":"gltyjvp","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/gltyjvp\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvw0op":{"author":"spicyboi97","author_fullname":"t2_1325a8","author_premium":false,"banned_at_utc":null,"body":"If you\u2019re using python, use PRAW\n\nif you\u2019re using Node, use Snoowrap\n\nI use both actively and they\u2019re both great.","created_utc":["2021-02-03","10:07:55"],"id":"glu905l","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/glu905l\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvzwx8":{"author":"immibis","author_fullname":"t2_dj2ua","author_premium":true,"banned_at_utc":null,"body":"Reddit API...?","created_utc":["2021-02-03","13:50:32"],"id":"glunzgz","link_id":"t3_lbflbm","parent_id":"t3_lbflbm","permalink":"\/r\/pushshift\/comments\/lbflbm\/anyone_got_an_alternative_of_push_shift_to_use\/glunzgz\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glw1xkw":{"author":"AutomaticManager888","author_fullname":"t2_2xhuiavu","author_premium":false,"banned_at_utc":null,"body":"The thing I want to see most is an up to date search for the newest posts in a subreddit. Reddit's own api seems to no longer support this, and I found pushshift as an alternative for that until a few minutes ago that I realized there is a serious problem with pushshift. So, after finding a post about that on this sub, I go back to this sub and see this post 8 minutes ago as of the time I'm writing this, and I could not ask for a better timing of events (pure sarcasm), however I wish reddit would have kept this in their own api and I see NO reason they should have removed it..... My app relies entirely on getting hte newest content available, and this task seems to be impossible so far. \n\n&amp;#x200B;\n\nps. This is my first reddit app and first experience with their api... this system is garbage so far","created_utc":["2021-02-03","18:14:20"],"id":"glvitc0","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvitc0\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glvtxww":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"PushShift is [being transitioned](https:\/\/twitter.com\/jasonbaumgartne\/status\/1352611879642927107) from a bunch of servers in a basement to the AWS cloud. I'm not sure most people realize the scale and storage requirements of this endeavour. As of last June, the platform was ingesting half a petabyte of uncompressed data each month and serving 50-100 TB of data via the APIs and [data.pushshift.io](https:\/\/data.pushshift.io). The projected costs for the new infrastructure are [$15k-20k per month](https:\/\/twitter.com\/jasonbaumgartne\/status\/1355040356543377414). The reality is the existing hardware can no longer keep up with the current rate of content generation on Reddit (huge spam problems), so we're going to have to wait until the transition is complete.","created_utc":["2021-02-03","19:14:34"],"id":"glvrqhh","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvrqhh\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gltwb38":{"author":"Ricsta76","author_fullname":"t2_q59yz","author_premium":false,"banned_at_utc":null,"body":"Unfortunately, the search feature on the official Reddit API is limited. There is no way to search with date parameters (like \"posts from today only\") and the max amount of results is 100 per API call. I also don't think there is a way to generally search for comments without specifying a specific submission. So I guess it depends what data you are looking for, but Pushshift exists because of these limitations.\n\nI think there might be a way to set up a comment stream from the Reddit API, but I don't know much about that.","created_utc":["2021-02-03","19:29:14"],"id":"glvtxww","link_id":"t3_lbimf0","parent_id":"t3_lbimf0","permalink":"\/r\/pushshift\/comments\/lbimf0\/pushshift_alternatives\/glvtxww\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gltyjvp":{"author":"VBGBeveryday","author_fullname":"t2_7jtsh","author_premium":false,"banned_at_utc":null,"body":"Wow! What a project. Definitely have nothing but love for PushShift. I'm excited for the new infrastructure, I'm sure that will open up many more possibilities for app developers and end-users.\n\nIn the meantime, I'm hoping to provider a decent alternative to [redditsearch.io](https:\/\/redditsearch.io) for those that use the tool and are not able to do so at this time.","created_utc":["2021-02-03","19:29:27"],"id":"glvtz60","link_id":"t3_lbqrly","parent_id":"t1_glvrqhh","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvtz60\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glu905l":{"author":"shiruken","author_fullname":"t2_4amlb","author_premium":true,"banned_at_utc":null,"body":"If you need a starting point, here's an alternate someone made: [https:\/\/camas.github.io\/reddit-search\/](https:\/\/camas.github.io\/reddit-search\/)\n\nObviously it's not working right now due to PushShift being offline.","created_utc":["2021-02-03","19:43:08"],"id":"glvw0op","link_id":"t3_lbqrly","parent_id":"t1_glvtz60","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvw0op\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glunzgz":{"author":"Watchful1","author_fullname":"t2_d0z23","author_premium":true,"banned_at_utc":null,"body":"Have you heard anything from Jason about it since then? I don't want to complain about a free project, but he's missed lots of his own deadlines in the past, so I'm not really hopeful of this transition happening anytime soon.","created_utc":["2021-02-03","20:07:57"],"id":"glvzwx8","link_id":"t3_lbqrly","parent_id":"t1_glvrqhh","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glvzwx8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glru39s":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[removed]","created_utc":["2021-02-03","20:21:08"],"id":"glw1xkw","link_id":"t3_lbqrly","parent_id":"t3_lbqrly","permalink":"\/r\/pushshift\/comments\/lbqrly\/making_alternative_to_redditsearchio_while\/glw1xkw\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"gltvbuv":{"author":"AwesomeTheKid","author_fullname":"t2_kt1du","author_premium":false,"banned_at_utc":null,"body":"Works. Thanks.","created_utc":["2021-02-21","18:52:02"],"id":"go8omq8","link_id":"t3_kddn9l","parent_id":"t1_gfw2s3y","permalink":"\/r\/pushshift\/comments\/kddn9l\/is_there_something_wrong_with_ceddit\/go8omq8\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glpawn0":{"author":"[deleted]","author_fullname":null,"author_premium":null,"banned_at_utc":null,"body":"[deleted]","created_utc":["2021-05-30","18:07:44"],"id":"gzzbmik","link_id":"t3_k5m2ko","parent_id":"t1_geg2vmr","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gzzbmik\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null},"glrterp":{"author":"s_i_m_s","author_fullname":"t2_zsx6r","author_premium":false,"banned_at_utc":null,"body":"He's on twitter most often https:\/\/twitter.com\/jasonbaumgartne\n\nBut I don't know of any reliable method to contact him.","created_utc":["2021-05-30","18:45:46"],"id":"gzzfylo","link_id":"t3_k5m2ko","parent_id":"t1_gzzbmik","permalink":"\/r\/pushshift\/comments\/k5m2ko\/any_idea_why_the_gofundme_is_paused\/gzzfylo\/","subreddit":"pushshift","subreddit_id":"t5_37z6f","author_cakeday":null}}