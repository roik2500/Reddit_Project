{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GCzaC-7ics2"
   },
   "source": [
    "# Installation and Import\n",
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic\n",
    "!pip install --upgrade pandas==1.3.4\n",
    "!pip install ipywidgets\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# #!pip install scikit-learn==0.22.1\n",
    "# #!pip install matplotlib\n",
    "# !pip install torchvision \n",
    "# !pip install bertopic\n",
    "# !pip torch\n",
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm\n",
    "from itertools import product\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for converting csv to pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_pkl(subreddit_csv):\n",
    "    path_to_pkl='/home/roikreme/BertTopic/{}/{}_main_data.pkl'.format(subreddit,subreddit)\n",
    "    print(\"start reading csv file\")\n",
    "    data=pd.read_csv(subreddit_csv)\n",
    "    print('finish to read csv file and start to convert to pkl')\n",
    "    data.to_pickle(path_to_pkl,protocol=4)\n",
    "\n",
    "# subreddit='wallstreetbets'\n",
    "# path='/home/roikreme/BertTopic/{}/{}_main_data.csv'.format(subreddit,subreddit)\n",
    "# convert_csv_to_pkl(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_topic_model(documents, n_neighbors, min_topic_size,calculate_probabilities = True):\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=10, min_dist=0.0, metric='cosine')\n",
    "\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=20)\n",
    "\n",
    "    topic_model = BERTopic(umap_model=umap_model,vectorizer_model=vectorizer_model, calculate_probabilities=calculate_probabilities, verbose=True,min_topic_size=min_topic_size, nr_topics=\"auto\")\n",
    "    topic_model.fit(documents)\n",
    "\n",
    "    return topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "\n",
    "def get_coherence(df, topic_model):\n",
    "    documents_per_topic = df.groupby(['Topic'], as_index=False).agg({'title_selftext': ' '.join})\n",
    "    cleaned_docs = topic_model._preprocess_text(documents_per_topic.title_selftext.values)\n",
    "\n",
    "    # Extract vectorizer and analyzer from BERTopic\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    words = vectorizer.get_feature_names()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words=[]\n",
    "    for t in range(len(set(topics))-2):\n",
    "        t_w=[]\n",
    "        topic=topic_model.get_topic(t)\n",
    "        if not isinstance(topic,bool):\n",
    "            for words in topic:\n",
    "                if words[0] not in tokens[0]:continue\n",
    "                t_w.append(words[0])\n",
    "            topic_words.append(t_w)\n",
    "    \n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(topics=list(topic_words), \n",
    "                                     texts=tokens, \n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "\n",
    "    return coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit=\"antiwork\"\n",
    "with open('/home/roikreme/BertTopic/{}/model/{}_clean_df.pickle'.format(subreddit,subreddit), \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "df.dropna(subset = [\"title_selftext\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "content=df['title_selftext'].to_list()\n",
    "docs_for_transform = random.sample(content,141130 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the specific subreddit contains only title\n",
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "docs_for_train = random.sample(content,141130)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random 80000 docs 5 times \n",
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_list=[]\n",
    "for i in tqdm(range(1,6)):\n",
    "    random.seed(1)\n",
    "    docs = random.sample(document,80000)\n",
    "    model=get_topic_model(docs, 20, 50)\n",
    "    topics, probas = model.transform(list(df['title_selftext']))\n",
    "    df[\"Topic\"] = topics\n",
    "    tmp = df[['post_id',\"Topic\",'title_selftext']]\n",
    "    tmp[\"ID\"] =  range(len(df))\n",
    "    insert_topic_word(tmp)\n",
    "    coh = get_coherence(tmp, model)\n",
    "    random_list.append({\"random_num\":i,\"coherenc\": coh})\n",
    "    pd.DataFrame(random_list).to_csv(\"/home/roikreme/BertTopic/random/random{}_coh.csv\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_list).to_csv(\"/home/roikreme/BertTopic/random/final_random.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimization of the model\n",
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_topics, new_probs = topic_model.reduce_topics(docs, topics, probabilities=probas)\n",
    "n_neighbors = [15,20,25, 30, 35,40]\n",
    "min_topic_sizes = [50, 100 ,150, 200, 250,300]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert to each record in the dataframe the topic words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "d = {'Number Of Negihbor':[],'Min Topic Size':[],'Num of Topic':[],'coherence':[],'Quantity of topic -1':[],'topic -1 %':[],'Total Amount':[]}\n",
    "res_tabel = pd.DataFrame(data=d,index=[])\n",
    "res_tabel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the optimization tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/roikreme/BertTopic/{}/random optimization/all/new_data2/final_tabel_df.csv\".format(subreddit), \"rb\") as f:\n",
    "    res_tabel = pd.read_csv(f)\n",
    "\n",
    "res_tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2=[50,100,150,200,250,300]\n",
    "temp1=[15,20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model = get_topic_model(docs_for_train, 20, 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimization of bert topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGQ6qzcoics-",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "from collections import Counter\n",
    "index_save=0\n",
    "for n_neighbor, min_topic_size  in tqdm(product(n_neighbors, min_topic_sizes), total=36):\n",
    "    #if n_neighbors==20:continue\n",
    "   \n",
    "    # save the model of this iteration\n",
    "    model = get_topic_model(docs_for_train, n_neighbor, min_topic_size)\n",
    "    model.save(\"/home/roikreme/BertTopic/{}/random optimization/all/models/my-model_{}_{}_{}\".format(subreddit,n_neighbor,min_topic_size,subreddit))\n",
    "    print(\"finish model\")\n",
    "\n",
    "    topics, probas = model.transform(list(df['title_selftext']))\n",
    "    df['Topic']=topics\n",
    "    print(\"finish transform\")\n",
    "\n",
    "    #Saving a data for this divition of topics\n",
    "    path_to_save='/home/roikreme/BertTopic/{}/random optimization/{}/new_data2/df_80k_trainer/{}_{}.pickle'.format(subreddit,status,n_neighbor,min_topic_size)\n",
    "    df.to_pickle(path_to_save,protocol=4)\n",
    "\n",
    "    #Calaulate the amount of each topic\n",
    "    get_topic=model.get_topic_info()\n",
    "    c = Counter(topics)\n",
    "    soret_c=sorted(c.items(),key=lambda x:x[0])\n",
    "    count=[i[1] for i in soret_c]\n",
    "\n",
    "    #update the dataframe with the ammount of each topic after transform\n",
    "    get_topic['Count']=count\n",
    "\n",
    "    sum_ = sum(count)\n",
    "    get_topic['percentage']=get_topic['Count'].apply(lambda x: str(round((x/sum_)*100,2))+'%')\n",
    "\n",
    "    print(\"start coh\")\n",
    "    coh = get_coherence(df, model)\n",
    "\n",
    "    #Adding to the dataframe\n",
    "    res_tabel.loc[len(res_tabel.index)]=[str(n_neighbor),str(min_topic_size),str(len(get_topic)),coh,str(count[0]),get_topic['percentage'][0],str(sum_)]\n",
    "\n",
    "\n",
    "    #saveing in any 2 iteration\n",
    "    if index_save % 2==0:\n",
    "        print(\"save\")\n",
    "        res_tabel.to_csv('/home/roikreme/BertTopic/{}/random optimization/{}/new_data2/df_80k_trainer/{}_{}.pickle'.format(subreddit,status,n_neighbor,min_topic_size))\n",
    "    index_save+=1\n",
    "   \n",
    "    \n",
    "    print(\"coh is:{}, min_topic_size:{}, n_neighbor:{} \".format(coh,min_topic_size,n_neighbor))\n",
    "    res.append({\"coherenc\": coh, \"min_topic_size\": min_topic_size, \"n_neighbor\":n_neighbor})\n",
    "    #get_topic.to_csv(\"/home/roikreme/BertTopic/{}/random optimization/new_data/{}_{}_{}.csv\".format(subreddit,n_neighbor,min_topic_size,coh),index=False)\n",
    "   # pd.DataFrame(res).to_csv(\"/home/roikreme/BertTopic/random/{}/new_data/final_coh.csv\".format(subreddit))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last version of table - sorted\n",
    "res_tabel.sort_values(by='Num of Topic',inplace=True)\n",
    "res_tabel.to_csv(\"/home/roikreme/BertTopic/{}/random optimization/new_data/final_tabel_df.csv\".format(subreddit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the results of coherence in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res).to_csv(\"/home/roikreme/BertTopic/random/{}/final_coh.csv\".format(subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a plot for optimization - coherence\n",
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=res_tabel['Num of Topic'].to_list()\n",
    "y=res_tabel['coherence'].to_list()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Num of Topic')\n",
    "plt.ylabel('coherence')\n",
    "plt.title('coherence graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# n=[15,20,25]\n",
    "# m_topic=[50,100,150,200,250,300]\n",
    "# neg_topic=[(neg,topic) for neg,topic in product(n, m_topic)]\n",
    "\n",
    "# neg_topic=[str(r) for r in neg_topic]\n",
    "# score=[round(r['coherenc'],3) for r in res]\n",
    "\n",
    "# fig, axs = plt.subplots(1, figsize=(25, 10), sharey=True)\n",
    "\n",
    "# axs.set_xlabel(\"(n_neighbors, min_topic_sizes)\")\n",
    "# axs.set_ylabel(\"coherence\")\n",
    "\n",
    "# ymax=max(score)\n",
    "# xpos=score.index(ymax)\n",
    "# xmax=neg_topic[xpos]\n",
    "# axs.annotate(\"Max = {}\".format(ymax),xy=(xmax,ymax),xytext=(xmax,ymax),arrowprops=dict(facecolor='black'))\n",
    "# axs.bar(neg_topic, score,width=0.5,align='center')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the maximum coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S35cGDBYics_",
    "outputId": "8700afcb-5f1e-4b77-dcfe-77a9d6cd7739"
   },
   "outputs": [],
   "source": [
    "coh = pd.DataFrame(res)\n",
    "coh.loc[coh[\"coherenc\"].argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the optimize model\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of the optimizing model\n",
    "n_neighbor=15\n",
    "min_topic_size=150 \n",
    "\n",
    "# create the model\n",
    "model = get_topic_model(docs_for_train, n_neighbor, min_topic_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=BERTopic.load(\"/home/roikreme/BertTopic/{}/model/all/my-model_{}_{}_{}\".format(subreddit,n_neighbor,min_topic_size,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/home/roikreme/BertTopic/{}/model/all/my-model_{}_{}_{}\".format(subreddit,n_neighbor,min_topic_size,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics, probas = model.transform(df['title_selftext'].to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a columns of probability and topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Topic\"] = topics\n",
    "prob=[round(p[np.argmax(p)],4) for p in probas]\n",
    "df[\"Topic\"] =topics\n",
    "df['probas']=prob\n",
    "df\n",
    "# tmp = df[['post_id','status',\"title\",\"Topic\",'title_selftext']]\n",
    "# tmp[\"ID\"] =  range(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mergine between the output of berttopic (get_topic_info) to main df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettopic=model.get_topic_info()\n",
    "df=df.merge(gettopic,left_on='Topic',right_on='Topic')\n",
    "df['ID']=len(df)\n",
    "#df['cont/len']=df['Count'].div(len(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "insert to each record in the dataframe the topic words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_topic_word(dff):\n",
    "    topics=set(dff['Topic'].to_list())\n",
    "    size=len(set(topics))\n",
    "    for t in tqdm(range(-1,size-1)):\n",
    "        t_w=set()\n",
    "        topic=model.get_topic(t)\n",
    "        if not isinstance(topic,bool) and str(topic)!='NaN':\n",
    "            for words in topic:\n",
    "                t_w.add(words[0])\n",
    "            dff.loc[dff.Topic == t, \"topic_words\"] = ', '.join(t_w)\n",
    "            \n",
    "# insert_topic_word(df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "insert_topic_word(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the data as a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save='/home/roikreme/BertTopic/{}/model/all/{}_model_data.pickle'.format(subreddit,subreddit)\n",
    "df.to_pickle(path_to_save,protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "subreddit=\"politics\"\n",
    "model = BERTopic.load(\"/home/roikreme/BertTopic/{}/model/all/my-model_{}_15_300\".format(subreddit,subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.iloc[19459].title\n",
    "#model.get_topic(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh = get_coherence(new_df, model)\n",
    "coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-yOP6YIics_"
   },
   "outputs": [],
   "source": [
    "m=model.get_topic_info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.merge(df,left_on='Topic',right_on='Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BertTopic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
